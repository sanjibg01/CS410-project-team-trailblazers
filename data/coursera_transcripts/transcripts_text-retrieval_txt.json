{
 "01_course-welcome-video.en.txt": "[SOUND]\nHello, welcome to the course in Text and\nRetrieval and Search Engines. I'm ChengXiang Zhia. I have a nickname Cheng. I'm a Professor of the Department of\nComputer Science at the University of Illinois at Urbana-Champaign. This first lecture is basically\nan introduction to the course, a brief introduction to what\nwe will cover in the course. We're going to first talk about the data\nmining specialization seeing this course is part of that specialization. And then we will cover motivation\nobjectives of the course, this will be followed by prerequisites and\ncourse format and reference books. And then finally,\nwe will talk about the course schedule, which has a number of topics to be\ncovered in the rest of this course. So the data mining specialization\noffered by the University of Illinois at Urbana-Champaign is really to address\nthe need for their mining techniques to handle a lot of big data,\nto turn the big data into knowledge. There are five lecture-based courses, as you see on the slide,\nplus one capstone project in the end. I'm teaching two of them,\nwhich is this course, Text Retrieval and Search Engines, and this one. The two courses that I cover\nhere are all about text data. In contrast,\nthe other courses are covering more general techniques that can be\napplied to all kinds of data. So pattern discovery,\ntaught by Professor Jiawei Han, and cluster analysis, again taught by him,\nabout the general data mining techniques to handle structured and\nunstructured text data. And data visualization covered by\nProfessor John Hart, is about the general visualization techniques, again,\napplicable to all kinds of data. So the motivation for\nthis course, in fact, also for the other courses that I'm teaching,\nis that we have a lot of text data and the data is everywhere,\nis growing rapidly. So you must have been\nexperiencing this growth. Just think about how much text data\nyou're dealing with every day, I list some date types here. For example, on the Internet we see\na lot of web pages, news articles, etc., and then we have blog articles, emails, scientific literature, and tweets. As we are speaking, maybe a lot\nof tweets are being written, and a lot of emails are being sent. So, the amount of text data is beyond\nour capacity to understand, then. Also, the amount of data makes it\npossible to actually analyze the data to discover interesting knowledge. And that's what we meant by\nharnessing big text data. [MUSIC]",
 "02_course-introduction-video.en.txt": "[MUSIC] Text data is very special. In contrast to the data captured\nby machines such as sensors, text data is produced by humans. And they also are meant\nto be consumed by humans. And this has some\ninteresting consequences. Because it is produced by humans, it tends\nto have a lot of useful knowledge about people's' preferences,\npeople's' opinions about everything. And that makes it possible to mine text data to discover those\nlatent prefaces of people, which could be very useful to build\nan intelligent system to help people. You can think about\nscientific literature or so and it's a way to encode\nour knowledge about the world. So it's very high quality content, yet we\nhave difficulty digesting all the content. Now as a result of the fact that\ntext is consumed by we humans, we also need intelligent software tools\nto help people digest the content, or otherwise we'd miss\na lot of useful content. This slide shows that the human really\nplays important role in test data mining. We have to consider human in the loop, and we have to consider the fact that\nthe text is generated by human. So, here are some examples of\nuseful text information systems. This is by no means a complete\nlist of all applications. I categorize them into\ndifferent categories. But you can probably imagine\nother kinds of applications. So let's take a look at some of them. Search for example,\nwe all know search engines is special. Web search engines, iPad,\nall of you are using Google, or Bing, or another web search engine all the time. And we also have live research assistants. And in fact, wherever you have a lot of\ntext data, you would have a search engine. So for example, you might have\na search box on your laptop. All right,\nto search content on your computer. So that's one kind of application systems,\nbut we also have filtering systems or\nrecommended systems. Those systems can push\ninformation to users. They can recommend useful\ninformation to users. So again, use filters, spam filters. Literature the movie recommenders. Now not of them are necessary\nrecommending the information to you. For example email filter,\nspam email filter, this is actually to filter out\nthe spams from your inbox, all right. But in nature these are similar systems in\nthat they have to make a binary decision regarding whether to retain\na particular document or discard it. Another kind of systems\nare categorization systems. So for example, in handling emails, you might prefer automatic,\nsorter that would automatically sort incoming emails into a proper\nfolders that you created. Or we might want to categorize product\nreviews into positive or negative. News agencies might be interested in categorizing news articles into\nall kinds of subject categories. Those are all categorization systems. Finally there are also systems\nthat might do more analysis. And oh, you can say mine text data. And these can be text mining systems or information extraction systems,\nand they can be used to analyze text data in more detail\nto discover potentially useful knowledge. For example companies might\nbe interested in discovering major complaints from their customers\nbased on the email messages that the, they have received from the customers. All right, so\nhaving a system to support that would really help improve their productivity and\nthe customer relations. Also in business, intelligence companies\nare often interested in analyzing product reviews to understand the relative\nstrengths of their own products in comparison with competitors. And, and so these are all examples\nof these test mining systems. [INAUDIBLE] we have a lot of data\nin particular literature data. So, there's also great opportunity\nof using computer systems to analyze the data to\nautomatically read literature, and to gain knowledge, and\nto help biologists make discoveries. And you can imagine many others. So the point is that with so\nmuch text data, we can build very useful systems to\nhelp people in many different ways. Now how do we build this systems? Well these actually are the main\ntechnologies that we'll be talking about in this course and the other course\nthat I'm teaching for this specialization. The main techniques for\nbuilding these systems and also for harnessing the text data are text\nretrieval and text data mining. So I use this picture to show\nthe relation between these two some of the different techniques. We started with big text data, right? But for any applications, we don't\nnecessarily need to use all the data. Often we only need the small subset of the\nmost relevant data, and that's shown here. So text retrieval is to convert big,\nraw text data into that small subset of most relevant data that are most\nuseful for a particular application. And this is usually\ndone by search engines. And so\nthis will be covered in this course. After we have got a small\namount of relevant data, we also need to further analyze the data\nto help people digest the data, or to turn the data into\nactionable knowledge. And this step is called text mining,\nwhere we use a number of techniques to mine the data to get useful knowledge or\npairings. And the knowledge can then be used\nin many different applications. And this part, text mining, will be\ncovered in the other course that I'm teaching called Text Mining and Analytics. The emphasis of this course\nis on basic concepts and practical techniques in text retrieval. More specifically we will\ncover how search engines work. How to implement a search engine. How to evaluate a search engine, so\nthat you know one search engine is better than another or\none method is better than another. How to improve and\noptimize a search engine system. And how to build a recommender system. We also hope to provide a hands on\nexperience on multiple aspects. One is to create a test collection for\nevaluating search engines. This is very important for knowing\nwhich technique actually worked well. And whether your search engine system\nis really good for your application. The other aspect is to experiment\nwith search engine algorithms. In practice, you will have to face\nchoices of different algorithms. So it's important to know\nhow to compare them and to figure out how they work or\nmaybe potentially, how to improve them. And finally, we'll provide a platform for\nyou to do search engine competition. Where you can compare your different\nideas to see which idea works better on some data set. The prerequisites for\nthis course are minimum. Basically we hope you have some basic\nconcepts of computer science, for example data structures. And we hope you will be comfortable\nwith programming, especially in C++. because that's the language that we'll use\nfor some of the programming assignments. The format is lectures plus quizzes,\nas often happens in MOOCs. And we also will provide\na program assignments for those of you that have\nthe resources to do that. The main refrence book for this course is a recent book that Sean Massung and I have co-authored. the title is Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining. However, this reference book is not required In the sense that if you follow all of the lecture videos closely Then you should have no problem with working on the quiz\nproblems and the programing assignment. To pass the course However, the book would be useful to give you a higher\nlevel and systematic desctiption in this course of all the topics covered in this course, plus some others <i><b><u><font color=#00000000></font></u></b></i> It will also help you understand subtopics in more depth, So if you have a problem with following some videos, the book might be useful to you. The book is also the reference  book for another mooc text mining and analytics. If you are interested in buying the book  there is a link here, And there should be a substantial discount for the students of this course. \u00c2\u00a0\u00c2 There are also quite a few other useful reference books and readings\nand they are available through the link at the bottom of this slide Finally, and this is the course schedule. That's just the top of the map for\nthe rest of the course, and it shows the topics that we will\ncover in the remaining lectures. This picture also shows basic flow of\ninformation in a text information system. So starting from the big text data, the\nfirst step is to do some natural language content analysis, because text data is\nin the form of natural language text. So we need to understand\nthe text to some extent in order to do something useful for\nthe users. So this is the first\ntopic that we will cover. And then on top of that as you\ncan see there are two boxes here. Those are two types of systems\nthat can be used to help people get access to the most relevant data. Or in other words, those are the two\nkinds of systems that will convert big text data into small\nrelevant text data. Search engines are helping\nusers to search or to query the data to get\nthe most relevant documents out. Recommender systems are to\nrecommend information to users, to push information to users. So those are two, complementary was of getting users connected to the most\nrelevant data at the right time. So this part is called text access,\nand this will be the next topic. And after we cover that we are going\nto cover a number of topics, all about the search engines. Now the text access\ntopic is a brief topic, a brief coverage of\nthe two kinds of systems. In the remaining topics, we'll cover\nsearch engines in much more detail. That includes text retrieval problem,\ntext retrieval methods, how to evaluate these methods, implementation of\nthe system, and web search applications. And after these, we're going to\ngo cover the recommender system. So this is what you expect\nin the rest of this course. Thanks. [MUSIC]",
 "01_lesson-1-1-natural-language-content-analysis.en.txt": "[SOUND] >> This lecture is about Natural Language of Content Analysis. As you see from this picture, this is really the first step\nto process any text data. Text data are in natural languages. So computers have to understand\nnatural languages to some extent, in order to make use of the data. So that's the topic of this lecture. We're going to cover three things. First, what is natural\nlanguage processing, which is the main technique for processing\nnatural language to obtain understanding. The second is the state of\nthe art of NLP which stands for natural language processing. Finally we're going to cover the relation\nbetween natural language processing and text retrieval. First, what is NLP? Well the best way to explain it\nis to think about if you see a text in a foreign language\nthat you can understand. Now what do you have to do in\norder to understand that text? This is basically what\ncomputers are facing. So looking at the simple sentence like\na dog is chasing a boy on the playground. We don't have any problems\nunderstanding this sentence. But imagine what the computer would\nhave to do in order to understand it. Well in general,\nit would have to do the following. First, it would have to know dog\nis a noun, chasing's a verb, etc. So this is called lexical analysis,\nor part-of-speech tagging, and we need to figure out the syntactic\ncategories of those words. So that's the first step. After that, we're going to figure\nout the structure of the sentence. So for example, here it shows that A and the dog would go together\nto form a noun phrase. And we won't have dog and is to go first. And there are some structures\nthat are not just right. But this structure shows what we might\nget if we look at the sentence and try to interpret the sentence. Some words would go together first, and then they will go together\nwith other words. So here we show we have noun phrases\nas intermediate components, and then verbal phrases. Finally we have a sentence. And you get this structure. We need to do something called\na semantic analysis, or parsing. And we may have a parser\naccompanying the program, and that would automatically\ncreated this structure. At this point you would know\nthe structure of this sentence, but still you don't know\nthe meaning of the sentence. So we have to go further\nto semantic analysis. In our mind we usually can map such a sentence to what we already\nknow in our knowledge base. For example, you might imagine\na dog that looks like that. There's a boy and\nthere's some activity here. But for a computer would have\nto use symbols to denote that. We'd use a symbol (d1) to denote a dog. And (b)1 can denote a boy and\nthen (p)1 can denote a playground. Now there is also a chasing\nactivity that's happening here so we have a relationship chasing\nthat connects all these symbols. So this is how a computer would obtain\nsome understanding of this sentence. Now from this representation we could\nalso further infer some other things, and we might indeed naturally think of\nsomething else when we read a text and this is called inference. So for example, if you believe\nthat if someone's being chased and this person might be scared,\nbut with this rule, you can see computers could also\ninfer that this boy maybe scared. So this is some extra knowledge\nthat you'd infer based on some understanding of the text. You can even go further to understand\nwhy the person say at this sentence. So this has to do as a use of language. This is called pragmatic analysis. In order to understand the speak\nactor of a sentence, right? We say something to\nbasically achieve some goal. There's some purpose there. And this has to do with\nthe use of language. In this case the person who said this sentence might be reminding\nanother person to bring back the dog. That could be one possible intent. To reach this level of\nunderstanding would require all of these steps and\na computer would have to go through all these steps in order to completely\nunderstand this sentence. Yet we humans have no trouble\nwith understanding that, we instantly would get everything. There is a reason for that. That's because we have a large\nknowledge base in our brain and we can use common sense knowledge\nto help interpret the sentence. Computers unfortunately are hard\nto obtain such understanding. They don't have such a knowledge base. They are still incapable of doing\nreasoning and uncertainties, so that makes natural language\nprocessing difficult for computers. But the fundamental reason why natural\nlanguage processing is difficult for computers is simply because natural\nlanguage has not been designed for computers. Natural languages are designed for\nus to communicate. There are other languages designed for\ncomputers. For example, programming languages. Those are harder for us, right? So natural languages is designed to\nmake our communication efficient. As a result,\nwe omit a lot of common sense knowledge because we assume everyone\nknows about that. We also keep a lot of ambiguities because\nwe assume the receiver or the hearer could know how to decipher an ambiguous word\nbased on the knowledge or the context. There's no need to demand different\nwords for different meanings. We could overload the same word with\ndifferent meanings without the problem. Because of these reasons this makes every\nstep in natural language of processing difficult for computers,\nambiguity is the main difficulty. And common sense and reasoning is\noften required, that's also hard. So let me give you some\nexamples of challenges here. Consider the word level ambiguity. The same word can have\ndifferent syntactic categories. For example design can be a noun or\na verb. The word of root may\nhave multiple meanings. So square root in math sense or\nthe root of a plant. You might be able to think\nabout it's meanings. There are also syntactical ambiguities. For example, the main topic of this\nlecture, natural language processing, can actually be interpreted in two\nways in terms of the structure. Think for a moment and\nsee if you can figure that out. We usually think of this as\nprocessing of natural language, but you could also think of this as do\nsay, language processing is natural. So this is an example\nof synaptic ambiguity. What we have different is\nstructures that can be applied to the same sequence of words. Another common example of an ambiguous\nsentence is the following. A man saw a boy with a telescope. Now in this case the question is,\nwho had a telescope. This is called a prepositional\nphrase attachment ambiguity or PP attachment ambiguity. Now we generally don't have a problem with\nthese ambiguities because we have a lot of background knowledge to help\nus disambiguate the ambiguity. Another example of difficulty\nis anaphora resolution. So think about the sentence John\npersuaded Bill to buy a TV for himself. The question here is does\nhimself refer to John or Bill? So again this is something that\nyou have to use some background or the context to figure out. Finally, presupposition\nis another problem. Consider the sentence,\nhe has quit smoking. Now this obviously implies\nthat he smoked before. So imagine a computer wants to understand\nall these subtle differences and meanings. It would have to use a lot of\nknowledge to figure that out. It also would have to maintain a large\nknowledge base of all the meanings of words and how they are connected to our\ncommon sense knowledge of the world. So this is why it's very difficult. So as a result, we are steep not perfect, in fact far from perfect in understanding\nnatural language using computers. So this slide sort of gains a simplified\nview of state of the art technologies. We can do part of speech\ntagging pretty well, so I showed 97% accuracy here. Now this number is obviously\nbased on a certain dataset, so don't take this literally. This just shows that we\ncan do it pretty well. But it's still not perfect. In terms of parsing,\nwe can do partial parsing pretty well. That means we can get noun phrase\nstructures, or verb phrase structure, or some segment of the sentence, and this dude correct them in\nterms of the structure. And in some evaluation results,\nwe have seen above 90% accuracy in terms of partial\nparsing of sentences. Again, I have to say these numbers\nare relative to the dataset. In some other datasets,\nthe numbers might be lower. Most of the existing work has been\nevaluated using news dataset. And so a lot of these numbers are more or\nless biased toward news data. Think about social media data,\nthe accuracy likely is lower. In terms of a semantical analysis, we are far from being able to do\na complete understanding of a sentence. But we have some techniques\nthat would allow us to do partial understanding of the sentence. So I could mention some of them. For example, we have techniques that can\nallow us to extract the entities and relations mentioned in text articles. For example,\nrecognizing dimensions of people, locations, organizations, etc in text. So this is called entity extraction. We may be able to recognize the relations. For example,\nthis person visited that place or this person met that person or\nthis company acquired another company. Such relations can be extracted by using the computer current\nNatural Language Processing techniques. They're not perfect but\nthey can do well for some entities. Some entities are harder than others. We can also do word sense\ndisintegration to some extend. We have to figure out whether this word in\nthis sentence would have certain meaning in another context the computer could\nfigure out, it has a different meaning. Again, it's not perfect, but\nyou can do something in that direction. We can also do sentiment analysis, meaning, to figure out whether\na sentence is positive or negative. This is especially useful for\nreview analysis, for example. So these are examples\nof semantic analysis. And they help us to obtain partial\nunderstanding of the sentences. It's not giving us a complete\nunderstanding, as I showed it before, for this sentence. But it would still help us gain\nunderstanding of the content. And these can be useful. In terms of inference,\nwe are not there yet, probably because of the general difficulty\nof inference and uncertainties. This is a general challenge\nin artificial intelligence. Now that's probably also because\nwe don't have complete semantical representation for\nnatural [INAUDIBLE] text. So this is hard. Yet in some domains perhaps,\nin limited domains when you have a lot of restrictions on the word uses, you may be\nable to perform inference to some extent. But in general we can not\nreally do that reliably. Speech act analysis is also\nfar from being done and we can only do that analysis for\nvery special cases. So this roughly gives you some\nidea about the state of the art. And then we also talk a little\nbit about what we can't do, and so we can't even do 100%\npart of speech tagging. Now this looks like a simple task, but think about the example here,\nthe two uses of off may have different syntactic categories if you\ntry to make a fine grained distinctions. It's not that easy to figure\nout such differences. It's also hard to do\ngeneral complete parsing. And again, the same sentence\nthat you saw before is example. This ambiguity can be very hard to\ndisambiguate and you can imagine example where you have to use a lot of knowledge\nin the context of the sentence or from the background, in order to figure\nout who actually had the telescope. So although the sentence looks very\nsimple, it actually is pretty hard. And in cases when the sentence is\nvery long, imagine it has four or five prepositional phrases, and there\nare even more possibilities to figure out. It's also harder to do precise\ndeep semantic analysis. So here's an example. In the sentence \"John owns a restaurant.\"\nHow do we define owns exactly? The word own,\nit is something that we can understand but it's very hard to precisely describe\nthe meaning of own for computers. So as a result we have a robust and\na general Natural Language Processing techniques\nthat can process a lot of text data. In a shallow way,\nmeaning we only do superficial analysis. For example, parts of speech tagging or a\npartial parsing or recognizing sentiment. And those are not deep understanding, because we're not really understanding\nthe exact meaning of the sentence. On the other hand of the deep\nunderstanding techniques tend not to scale up well, meaning that they would\nfill only some restricted text. And if you don't restrict\nthe text domain or the use of words, then these\ntechniques tend not to work well. They may work well based on machine\nlearning techniques on the data that are similar to the training data\nthat the program has been trained on. But they generally wouldn't work well on\nthe data that are very different from the training data. So this pretty much summarizes the state\nof the art of Natural Language Processing. Of course, within such a short amount\nof time we can't really give you a complete view of NLP,\nwhich is a big field. And I'd expect to see multiple courses on\nNatural Language Processing topic itself. But because of its relevance to the topic\nthat we talk about, it's useful for you to know the background in case\nyou happen to be exposed to that. So what does that mean for Text Retrieval? Well, in Text Retrieval we\nare dealing with all kinds of text. It's very hard to restrict\ntext to a certain domain. And we also are often dealing\nwith a lot of text data. So that means The NLP techniques must\nbe general, robust, and efficient. And that just implies today we can only\nuse fairly shallow NLP techniques for text retrieval. In fact, most search engines today use something\ncalled a bag of words representation. Now, this is probably the simplest\nrepresentation you can possibly think of. That is to turn text data\ninto simply a bag of words. Meaning we'll keep individual words, but\nwe'll ignore all the orders of words. And we'll keep duplicated\noccurrences of words. So this is called a bag\nof words representation. When you represent text in this way,\nyou ignore a lot of valid information. That just makes it harder to understand\nthe exact meaning of a sentence because we've lost the order. But yet this representation tends\nto actually work pretty well for most search tasks. And this was partly because the search\ntask is not all that difficult. If you see matching of some of\nthe query words in a text document, chances are that that document is about\nthe topic, although there are exceptions. So in comparison of some other tasks, for example, machine translation would require\nyou to understand the language accurately. Otherwise the translation would be wrong. So in comparison such tasks\nare all relatively easy. Such a representation is often sufficient\nand that's also the representation that the major search engines today,\nlike a Google or Bing are using. Of course, I put in parentheses but\nnot all, of course there are many queries that are not answered well by\nthe current search engines, and they do require the replantation that\nwould go beyond bag of words replantation. That would require more natural\nlanguage processing to be done. There was another reason why we\nhave not used the sophisticated NLP techniques in modern search engines. And that's because some\nretrieval techniques actually, naturally solved the problem of NLP. So one example is word\nsense disintegration. Think about a word like Java. It could mean coffee or\nit could mean program language. If you look at the word anome,\nit would be ambiguous, but when the user uses the word in the query,\nusually there are other words. For example, I'm looking for\nusage of Java applet. When I have applet there,\nthat implies Java means program language. And that contest can help us\nnaturally prefer documents which Java is referring\nto program languages. Because those documents would\nprobably match applet as well. If Java occurs in that\ndocuments where it means coffee then you would never match applet or\nwith very small probability. So this is the case when\nsome retrieval techniques naturally achieve the goal of word. Another example is some technique called feedback which we will talk about\nlater in some of the lectures. This technique would allow us to add\nadditional words to the query and those additional words could\nbe related to the query words. And these words can help matching\ndocuments where the original query words have not occurred. So this achieves, to some extent,\nsemantic matching of terms. So those techniques also helped us bypass some of the difficulties\nin natural language processing. However, in the long run we still need\na deeper natural language processing techniques in order to improve the\naccuracy of the current search engines. And it's particularly needed for\ncomplex search tasks. Or for question and answering. Google has recently launched a knowledge\ngraph, and this is one step toward that goal, because knowledge graph would\ncontain entities and their relations. And this goes beyond the simple\nbag of words replantation. And such technique should help us\nimprove the search engine utility significantly, although this is the open\ntopic for research and exploration. In sum, in this lecture we\ntalked about what is NLP and we've talked about the state\nof that techniques. What we can do, what we cannot do. And finally, we also explain why\nthe bag of words replantation remains the dominant replantation\nused in modern search engines, even though deeper NLP would be needed for\nfuture search engines. If you want to know more, you can take\na look at some additional readings. I only cited one here and\nthat's a good starting point. Thanks. [MUSIC]",
 "02_lesson-1-2-text-access.en.txt": "[SOUND] In this lecture,\nwe're going to talk about the text access. In the previous lecture, we talked about\nthe natural language content, analysis. We explained that the state of the are\nnatural language processing techniques are still not good enough to process\na lot of unrestricted text data in a robust manner. As a result, bag of words remains very popular in\napplications like a search engine. In this lecture, we're going to talk\nabout some high-level strategies to help users get access to the text data. This is also important step to convert\nraw big text data into small random data. That are actually needed\nin a specific application. So the main question we'll address here,\nis how can a text information system, help users\nget access to the relevant text data? We're going to cover two complimentary\nstrategies, push versus pull. And then we're going to talk about\ntwo ways to implement the pull mode, querying versus browsing. So first push versus pull. These are two different ways connect\nthe users with the right information at the right time. The difference is which\ntakes the initiative, which party takes the initiative. In the pull mode, the users take the initiative to\nstart the information access process. And in this case, a user typically would\nuse a search engine to fulfill the goal. For example,\nthe user may type in the query and then browse the results to\nfind the relevant information. So this is usually appropriate for satisfying a user's ad\nhoc information need. An ad hoc information need is\na temporary information need. For example, you want to buy a product so you suddenly have a need to read\nreviews about related product. But after you have cracked information,\nyou have purchased in your product. You generally no longer\nneed such information, so it's a temporary information need. In such a case, it's very hard for\na system to predict your need, and it's more proper for\nthe users to take the initiative, and that's why search engines are very useful. Today because many people have many\ninformation needs all the time. So as we're speaking Google is probably\nprocessing many queries from this. And those are all, or mostly adequate. Information needs. So this is a pull mode. In contrast in the push mode in\nthe system would take the initiative to push the information to the user or\nto recommend information to the user. So in this case this is usually\nsupported by a recommender system. Now this would be appropriate if. The user has a stable information. For example you may have a research\ninterest in some topic and that interest tends to stay for a while. So, it's rather stable. Your hobby is another example of. A stable information need is such a case\nthe system can interact with you and can learn your interest, and\nthen to monitor the information stream. If the system hasn't seen any\nrelevant items to your interest, the system could then take the initiative\nto recommend the information to you. So, for example, a news filter or news recommended system could\nmonitor the news stream and identify interesting news to you and\nsimply push the news articles to you. This mode of information access may be\nalso a property that when this system has good knowledge about the users need\nand this happens in the search context. So for example, when you search for\ninformation on the web a search engine might infer you might be\nalso interested in something related. Formation. And they would recommend the information\nto you, so that just reminds you, for example, of an advertisement\nplaced on the search page. So this is about the two high level\nstrategies or two modes of text access. Now let's look at the pull\nmode in more detail. In the pull mode, we can further\ndistinguish it two ways to help users. Querying versus browsing. In querying,\na user would just enter a query. Typical the keyword query, and the search engine system would\nreturn relevant documents to use. And this works well when the user knows\nwhat exactly are the keywords to be used. So if you know exactly\nwhat you are looking for, you tend to know the right keywords. And then query works very well,\nand we do that all of the time. But we also know that sometimes\nit doesn't work so well. When you don't know the right\nkeywords to use in the query, or you want to browse information\nin some topic area. You use because browsing\nwould be more useful. So in this case, in the case of browsing,\nthe users would simply navigate it, into the relevant information\nby following the paths supported by the structures of documents. So the system would maintain\nsome kind of structures and then the user could follow\nthese structures to navigate. So this really works well when the user\nwants to explore the information space or the user doesn't know what\nare the keywords to using the query. Or simply because the user finds it\ninconvenient to type in a query. So even if a user knows what query to\ntype in if the user is using a cellphone to search for information. It's still harder to enter the query. In such a case, again,\nbrowsing tends to be more convenient. The relationship between browsing and\nquerying is best understood by making and imagine you're site seeing. Imagine if you're touring a city. Now if you know the exact\naddress of attraction. Taking a taxi there is\nperhaps the fastest way. You can go directly to the site. But if you don't know the exact address,\nyou may need to walk around. Or you can take a taxi to a nearby\nplace and then walk around. It turns out that we do exactly\nthe same in the information studies. If you know exactly what you\nare looking for, then you can use the right keywords in your query\nto find the information you're after. That's usually the fastest way to do,\nfind information. But what if you don't know\nthe exact keywords to use? Well, you clearly probably won't so well. You will not related pages. And then, you need to also walk\naround in the information space, meaning by following the links or\nby browsing. You can then finally get\ninto the relevant page. If you want to learn about again. You will likely do a lot of browsing so just like you are looking around in\nsome area and you want to see some interesting attractions\nrelated in the same. [INAUDIBLE]. So this analogy also tells us that\ntoday we have very good support for query, but we don't really have\ngood support for browsing. And this is because in order\nto browse effectively, we need a map to guide us,\njust like you need a map to. Of Chicago, through the city of Chicago, you need a\ntopical map to tour the information space. So how to construct such a topical\nmap is in fact a very interesting research question that might bring us more interesting browsing experience\non the web or in applications. So, to summarize this lecture, we've talked about the two high level\nstrategies for text access; push and pull. Push tends to be supported by\nthe Recommender System, and Pull tends to be supported\nby the Search Engine. Of course, in the sophisticated\n[INAUDIBLE] information system, we should combine the two. In the pull mode, we can further this\n[INAUDIBLE] Querying and Browsing. Again we generally want to combine\nthe two ways to help you assist, so that you can support\nthe both querying nad browsing. If you want to know more about\nthe relationship between pull and push, you can read this article. This give excellent discussion of the\nrelationship between machine filtering and information retrieval. Here informational filtering is similar\nto information recommendation or the push mode of information access. [MUSIC]",
 "03_lesson-1-3-text-retrieval-problem.en.txt": "[MUSIC] This lecture is about\nthe text retrieval problem. This picture shows our overall plan for\nlectures. In the last lecture, we talked about\nthe high level strategies for text access. We talked about push versus pull. Such engines are the main tools for\nsupporting the pull mode. Starting from this lecture, we're going to talk about the how\nsearch engines work in detail. So first it's about\nthe text retrieval problem. We're going to talk about\nthe three things in this lecture. First, we define Text Retrieval. Second we're going to make a comparison\nbetween Text Retrieval and the related task Database Retrieval. Finally, we're going to talk about\nthe Document Selection versus Document Ranking as two strategies for\nresponding to a user's query. So what is Text Retrieval? It should be a task that's familiar for the most of us because we're using\nweb search engines all the time. So text retrieval is basically a task where the system would respond to\na user's query With relevant documents. Basically, it's for supporting a query as one way to implement the poll\nmode of information access. So the situation is the following. You have a collection of\ntext retrieval documents. These documents could be all\nthe webpages on the web, or all the literature articles\nin the digital library. Or maybe all the text\nfiles in your computer. A user will typically give a query to\nthe system to express information need. And then, the system would return\nrelevant documents to users. Relevant documents refer to those\ndocuments that are useful to the user who typed in the query. All this task is a phone call\nthat information retrieval. But literally information retrieval would\nbroadly include the retrieval of other non-textual information as well,\nfor example audio, video, etc. It's worth noting that\nText Retrieval is at the core of information retrieval in\nthe sense that other medias such as video can be retrieved by\nexploiting the companion text data. So for example,\ncurrent the image search engines actually match a user's query was\nthe companion text data of the image. This problem is also\ncalled search problem. And the technology is often called\nthe search technology industry. If you ever take a course in databases it will be useful to pause\nthe lecture at this point and think about the differences between\ntext retrieval and database retrieval. Now these two tasks\nare similar in many ways. But, there are some important differences. So, spend a moment to think about\nthe differences between the two. Think about the data, and the information\nmanaged by a search engine versus those that are managed\nby a database system. Think about the different between\nthe queries that you typically specify for database system versus queries that\nare typed in by users in a search engine. And then finally think about the answers. What's the difference between the two? Okay, so if we think about the information\nor data managed by the two systems, we will see that in text retrieval. The data is unstructured, it's free text. But in databases, they are structured data\nwhere there is a clear defined schema to tell you this column is the names\nof people and that column is ages, etc. The unstructured text is not obvious what are the names of people\nmentioned in the text. Because of this difference, we also see\nthat text information tends to be more ambiguous and we talk about that in the\nprocessing chapter, whereas in databases. But they don't tend to have\nwhere to find the semantics. The results important\ndifference in the queries, and this is partly due to the difference\nin the information or data. So test queries tend to be ambiguous. Whereas in their research,\nthe queries are typically well-defined. Think about a SQL query that would clearly\nspecify what records to be returned. So it has very well-defined semantics. Keyword queries or electronic queries tend to be incomplete,\nalso in that it doesn't really specify what documents\nshould be retrieved. Whereas complete specification for\nwhat should be returned. And because of these differences,\nthe answers would be also different. Being the case of text retrieval, we're\nlooking for it rather than the documents. In the database search,\nwe are retrieving records or match records with the sequel\nquery more precisely. Now in the case of text retrieval,\nwhat should be the right answers to the query is not very well specified,\nas we just discussed. So it's unclear what should be\nthe right answers to a query. And this has very important consequences,\nand that is, textual retrieval is\nan empirically defined problem. So this is a problem because\nif it's empirically defined, then we can not mathematically prove one\nmethod is better than another method. That also means we must rely\non empirical evaluation involving users to know\nwhich method works better. And that's why we have. You need more than one lectures\nto cover the issue of evaluation. Because this is very important topic for\nSir Jennings. Without knowing how to evaluate heroism\nproperly, there's no way to tell whether we have got the better or\nwhether one system is better than another. So now let's look at\nthe problem in a formal way. So, this slide shows a formal formulation\nof the text retrieval problem. First, we have our vocabulary set, which\nis just a set of words in a language. Now here,\nwe are considering only one language, but in reality, on the web,\nthere might be multiple natural languages. We have texts that are in\nall kinds of languages. But here for simplicity, we just\nassume that is one kind of language. As the techniques used for retrieving\ndata from multiple languages Are more or less similar to the techniques used for\nretrieving documents in one end, which although there is important difference,\nthe principle methods are very similar. Next, we have the query,\nwhich is a sequence of words. And so here, you can see the query is defined as\na sequence of words. Each q sub i is a word in the vocabulary. A document is defined in the same way,\nso it's also a sequence of words. And here,\nd sub ij is also a word in the vocabulary. Now typically, the documents\nare much longer than queries. But there are also cases where\nthe documents may be very short. So you can think about what\nmight be a example of that case. I hope you can think of Twitter search. Tweets are very short. But in general,\ndocuments are longer than the queries. Now, then we have\na collection of documents, and this collection can be very large. So think about the web. It could be very large. And then the goal of text retrieval\nis you'll find the set of relevant in the documents, which we denote by R'(q),\nbecause it depends on the query. And this in general, a subset of all\nthe documents in the collection. Unfortunately, this set of relevant\ndocuments is generally unknown, and user-dependent in the sense that,\nfor the same query typed in by different users, they expect\nthe relevant documents may be different. The query given to us by\nthe user is only a hint on which document should be in this set. And indeed, the user is generally\nunable to specify what exactly should be in this set, especially in the case\nof web search, where the connection's so large, the user doesn't have complete\nknowledge about the whole production. So the best search system\ncan do is to compute an approximation of this\nrelevant document set. So we denote it by R'(q). So formerly,\nwe can see the task is to compute this R'(q) approximation of\nthe relevant documents. So how can we do that? Now imagine if you are now asked\nto write a program to do this. What would you do? Now think for a moment. Right, so these are your input. The query, the documents. And then you are to compute\nthe answers to this query, which is a set of documents that\nwould be useful to the user. So, how would you solve the problem? Now in general,\nthere are two strategies that we can use. The first strategy is we do a document\nselection, and that is, we're going to have a binary classification\nfunction, or binary classifier. That's a function that\nwould take a document and query as input, and then give a zero or one as output to indicate whether this\ndocument is relevant to the query or not. So in this case, you can see the document. The relevant document is set,\nis defined as follows. It basically, all the documents that\nhave a value of 1 by this function. So in this case, you can see the system must have decide\nif the document is relevant or not. Basically, it has to say\nwhether it's one or zero. And this is called absolute relevance. Basically, it needs to know\nexactly whether it's going to be useful to the user. Alternatively, there's another\nstrategy called document ranking. Now in this case, the system is not going to make a call\nwhether a document is random or not. But rather the system is going to\nuse a real value function, f here. That would simply give us a value that would indicate which\ndocument is more likely relevant. So it's not going to make a call whether\nthis document is relevant or not. But rather it would say which\ndocument is more likely relevant. So this function then can be\nused to random documents, and then we're going to let\nthe user decide where to stop, when the user looks at the document. So we have a threshold theta\nhere to determine what documents should be in\nthis approximation set. And we're going to assume\nthat all the documents that are ranked above the threshold\nare in this set, because in effect, these are the documents that\nwe deliver to the user. And theta is a cutoff\ndetermined by the user. So here we've got some collaboration\nfrom the user in some sense, because we don't really make a cutoff. And the user kind of helped\nthe system make a cutoff. So in this case,\nthe system only needs to decide if one document is more\nlikely relevant than another. And that is, it only needs to\ndetermine relative relevance, as opposed to absolute relevance. Now you can probably already sense that relative relevance would be easier to\ndetermine than absolute relevance. Because in the first case, we have to say exactly whether\na document is relevant or not. And it turns out that ranking is indeed\ngenerally preferred to document selection. So let's look at these two\nstrategies in more detail. So this picture shows how it works. So on the left side,\nwe see these documents, and we use the pluses to indicate\nthe relevant documents. So we can see the true relevant\ndocuments here consists this set of true relevant documents, consists\nof these process, these documents. And with the document selection function, we're going to basically\nclassify them into two groups, relevant documents, and non-relevant ones. Of course, the classified will not\nbe perfect so it will make mistakes. So here we can see, in the approximation\nof the relevant documents, we have got some number in the documents. And similarly, there is a relevant document that's\nmisclassified as non-relevant. In the case of document ranking,\nwe can see the system seems like, simply ranks all the documents in\nthe descending order of the scores. And then, we're going to let the user\nstop wherever the user wants to stop. If the user wants to\nexamine more documents, then the user will scroll down some\nmore and then stop [INAUDIBLE]. But if the user only wants to\nread a few random documents, the user might stop at the top position. So in this case, the user stops at d4. So in fact, we have delivered\nthese four documents to our user. So as I said ranking is generally\npreferred, and one of the reasons is because the classifier in the case of\ndocument selection is unlikely accurate. Why? Because the only clue\nis usually the query. But the query may not be accurate in the\nsense that it could be overly constrained. For example, you might expect relevant\ndocuments to talk about all these topics by using specific vocabulary. And as a result,\nyou might match no relevant documents. Because in the collection, no others have discussed the topic\nusing these vocabularies, right? So in this case,\nwe'll see there is this problem of no relevant documents to return in\nthe case of over-constrained query. On the other hand,\nif the query is under-constrained, for example, if the query does not have sufficient descriptive\nwords to find the random documents. You may actually end up having of\nover delivery, and this when you thought these words my be sufficient\nto help you find the right documents. But, it turns out they\nare not sufficient and there are many distractions,\ndocuments using similar words. And so, this is a case of over delivery. Unfortunately, it's very hard to find the\nright position between these two extremes. Why? Because whether users looking for\nthe information in general the user does not have a good knowledge about\nthe information to be found. And in that case, the user does not\nhave a good knowledge about what vocabularies will be used in\nthose relevent documents. So it's very hard for a user to pre-specify the right\nlevel of constraints. Even if the classifier is accurate,\nwe also still want to rend these relevant documents, because they\nare generally not equally relevant. Relevance is often a matter of degree. So we must prioritize these documents for\na user to examine. And note that this\nprioritization is very important because a user cannot\ndigest all the content the user generally would have to\nlook at each document sequentially. And therefore, it would make sense to\nusers with the most relevant documents. And that's what ranking is doing. So for these reasons,\nranking is generally preferred. Now this preference also has\na theoretical justification and this is given by the probability\nranking principle. In the end of this lecture,\nthere is reference for this. This principle says, returning a ranked\nlist of documents in descending order of probability that a document\nis relevant to the query is the optimal strategy under\nthe following two assumptions. First, the utility of\na document (to a user) Is independent of the utility\nof any other document. Second, a user would be assumed to\nbrowse the results sequentially. Now it's easy to understand why these\nassumptions are needed in order to justify Site for the ranking strategy. Because if the documents are independent, then we can evaluate the utility\nof each document that's separate. And this would allow the computer\nscore for each document independently. And then, we are going to rank these\ndocuments based on the scrolls. The second assumption is to say that the\nuser would indeed follow the rank list. If the user is not going to follow\nthe ranked list, is not going to examine the documents sequentially, then obviously\nthe ordering would not be optimal. So under these two assumptions, we can\ntheoretically justify the ranking strategy is, in fact, the best that you could do. Now, I've put one question here. Do these two assumptions hold? I suggest you to pause the lecture,\nfor a moment, to think about this. Now, can you think of\nsome examples that would suggest these assumptions\naren't necessarily true. Now, if you think for a moment, you may realize none of\nthe assumptions Is actually true. For example, in the case of\nindependence assumption we might have documents that have similar or\nexactly the same content. If we look at each of them alone,\neach is relevant. But if the user has already seen\none of them, we can assume it's generally not very useful for the user to\nsee another similar or duplicated one. So clearly the utility\non the document that is dependent on other documents\nthat the user has seen. In some other cases you might see\na scenario where one document that may not be useful to the user, but when three\nparticular documents are put together. They provide answers to\nthe user's question. So this is a collective relevance and\nthat also suggests that the value of the document might\ndepend on other documents. Sequential browsing generally would make\nsense if you have a ranked list there. But even if you have a rank list,\nthere is evidence showing that users don't always just go strictly\nsequentially through the entire list. They sometimes will look at the bottom for\nexample, or skip some. And if you think about the more\ncomplicated interfaces that we could possibly use like\ntwo dimensional in the phase. Where you can put that additional\ninformation on the screen then sequential browsing is a very\nrestricted assumption. So the point here is that none of these assumptions is\nreally true but less than that. But probability ranking principle\nestablishes some solid foundation for ranking as a primary pattern for\nsearch engines. And this has actually been the basis for a lot of research work in\ninformation retrieval. And many hours have been designed\nbased on this assumption, despite that the assumptions\naren't necessarily true. And we can address this problem\nby doing post processing Of a ranked list, for example,\nto remove redundancy. So to summarize this lecture, the main points that you can\ntake away are the following. First, text retrieval is\nan empirically defined Problem. And that means which algorithm is\nbetter must be judged by the users. Second, document ranking\nis generally preferred. And this will help users prioritize\nexamination of search results. And this is also to bypass the difficulty\nin determining absolute relevance Because we can get some help from users\nin determining where to make the cut off, it's more flexible. So, this further suggests that the main\ntechnical challenge in designing a search engine is the design\neffective ranking function. In other words, we need to define\nwhat is the value of this function F on the query and document pair. How we design such a function is the main\ntopic in the following lectures. There are two suggested\nadditional readings. The first is the classical paper on\nthe probability ranking principle. The second one is a must-read for anyone\ndoing research on information retrieval. It's a classic IR book, which has\nexcellent coverage of the main research and results in early days up to\nthe time when the book was written. Chapter six of this book has\nan in-depth discussion of the Probability Ranking Principle and\nProbably for retrieval models in general. [MUSIC]",
 "04_lesson-1-4-overview-of-text-retrieval-methods.en.txt": "[SOUND] This lecture is a overview of\ntext retrieval methods. In the previous lecture, we introduced\nthe problem of text retrieval. We explained that the main problem is the design of ranking function\nto rank documents for a query. In this lecture, we will give an overview of different\nways of designing this ranking function. So the problem is the following. We have a query that has\na sequence of words and the document that's also\na sequence of words. And we hope to define a function f that can compute a score based\non the query and document. So the main challenge you hear is with\ndesign a good ranking function that can rank all the relevant documents\non top of all the non-relevant ones. Clearly, this means our function\nmust be able to measure the likelihood that a document\nd is relevant to a query q. That also means we have to have\nsome way to define relevance. In particular, in order to\nimplement the program to do that, we have to have a computational\ndefinition of relevance. And we achieve this goal by\ndesigning a retrieval model, which gives us\na formalization of relevance. Now, over many decades, researchers have designed many\ndifferent kinds of retrieval models. And they fall into different categories. First, one family of the models\nare based on the similarity idea. Basically, we assume that if\na document is more similar to the query than another document is, then we will say the first document\nis more relevant than the second one. So in this case,\nthe ranking function is defined as the similarity between the query and\nthe document. One well known example in this\ncase is vector space model, which we will cover more in\ndetail later in the lecture. A second kind of models\nare called probabilistic models. In this family of models, we follow a very\ndifferent strategy, where we assume that queries and documents are all\nobservations from random variables. And we assume there is a binary\nrandom variable called R here to indicate whether a document\nis relevant to a query. We then define the score of document with\nrespect to a query as a probability that this random variable R is equal to 1,\ngiven a particular document query. There are different cases\nof such a general idea. One is classic probabilistic model,\nanother is language model, yet another is divergence\nfrom randomness model. In a later lecture, we will talk more\nabout one case, which is language model. A third kind of model are based\non probabilistic inference. So here the idea is to associate\nuncertainty to inference rules, and we can then quantify\nthe probability that we can show that the query\nfollows from the document. Finally, there is also a family of models that are using axiomatic thinking. Here, an idea is to define\na set of constraints that we hope a good retrieval function to satisfy. So in this case, the problem is\nto seek a good ranking function that can satisfy all\nthe desired constraints. Interestingly, although these different\nmodels are based on different thinking, in the end, the retrieval function\ntends to be very similar. And these functions tend to\nalso involve similar variables. So now let's take a look at the common\nform of a state of the art retrieval model and to examine some of the common\nideas used in all these models. First, these models are all\nbased on the assumption of using bag of words to represent text,\nand we explained this in the natural\nlanguage processing lecture. Bag of words representation remains\nthe main representation used in all the search engines. So with this assumption,\nthe score of a query, like a presidential campaign news\nwith respect to a document of d here, would be based on scores computed\nbased on each individual word. And that means the score would\ndepend on the score of each word, such as presidential, campaign, and news. Here, we can see there\nare three different components, each corresponding to how well the\ndocument matches each of the query words. Inside of these functions,\nwe see a number of heuristics used. So for example, one factor that\naffects the function d here is how many times does the word\npresidential occur in the document? This is called a term frequency, or TF. We might also denote as\nc of presidential and d. In general, if the word occurs\nmore frequently in the document, then the value of this\nfunction would be larger. Another factor is,\nhow long is the document? And this is to use the document length for\nscoring. In general, if a term occurs in a long document many times,\nit's not as significant as if it occurred the same number\nof times in a short document. Because in a long document, any term\nis expected to occur more frequently. Finally, there is this factor\ncalled document frequency. That is, we also want to look at how\noften presidential occurs in the entire collection, and we call this document\nfrequency, or df of presidential. And in some other models,\nwe might also use a probability to characterize this information. So here, I show the probability of\npresidential in the collection. So all these are trying to characterize\nthe popularity of the term in the collection. In general, matching a rare term in\nthe collection is contributing more to the overall score than\nmatching up common term. So this captures some of the main ideas\nused in pretty much older state of the art original models. So now, a natural question is,\nwhich model works the best? Now it turns out that many\nmodels work equally well. So here are a list of\nthe four major models that are generally regarded as\na state of the art original models, pivoted length normalization,\nBM25, query likelihood, PL2. When optimized,\nthese models tend to perform similarly. And this was discussed in detail in this\nreference at the end of this lecture. Among all these,\nBM25 is probably the most popular. It's most likely that this has been used\nin virtually all the search engines, and you will also often see this\nmethod discussed in research papers. And we'll talk more about this\nmethod later in some other lectures. So, to summarize, the main points made\nin this lecture are first the design of a good ranking function pre-requires a\ncomputational definition of relevance, and we achieve this goal by designing\nappropriate retrieval model. Second, many models are equally effective,\nbut we don't have a single winner yet. Researchers are still active and\nworking on this problem, trying to find a truly\noptimal retrieval model. Finally, the state of the art\nranking functions tend to rely on the following ideas. First, bag of words representation. Second, TF and\ndocument frequency of words. Such information is used in\nthe weighting function to determine the overall contribution of matching\na word and document length. These are often combined in interesting\nways, and we'll discuss how exactly they are combined to rank\ndocuments in the lectures later. There are two suggested additional\nreadings if you have time. The first is a paper where you can\nfind the detailed discussion and comparison of multiple\nstate of the art models. The second is a book with\na chapter that gives a broad review of different retrieval models. [MUSIC]",
 "05_lesson-1-5-vector-space-model-basic-idea.en.txt": "[SOUND] This lecture is about the\nvector space retrieval model. We're going to give\nan introduction to its basic idea. In the last lecture, we talked about\nthe different ways of designing a retrieval model, which would give\nus a different arranging function. In this lecture, we're going to\ntalk about a specific way of designing a ramping function called\na vector space retrieval model. And we're going to give a brief\nintroduction to the basic idea. Vector space model is a special case of similarity based models\nas we discussed before. Which means we assume relevance\nis roughly similarity, between the document and the query. Now whether is this assumption\nis true is actually a question. But in order to solve the search problem, we have to convert the vague notion\nof relevance into a more precise definition that can be implemented\nwith the program analogy. So in this process,\nwe have to make a number of assumptions. This is the first assumption\nthat we make here. Basically, we assume that if a document\nis more similar to a query than another document. Then the first document will be assumed it\nwill be more relevant than the second one. And this is the basis for\nranking documents in this approach. Again, it's questionable whether this is\nreally the best definition for randoms. As we will see later there\nare other ways to model randoms. The basic idea of vectors for base retrieval model is actually\nvery easy to understand. Imagine a high dimensional space where\neach dimension corresponds to a term. So here I issue a three dimensional\nspace with three words, programming, library and presidential. So each term here defines one dimension. Now we can consider vectors in this,\nthree dimensional space. And we're going to assume\nthat all our documents and the query will be placed\nin this vector space. So for example, on document might\nbe represented by this vector, d1. Now this means this document\nprobably covers library and presidential, but\nit doesn't really talk about programming. What does this mean in terms\nof representation of document? That just means we're going to look at\nour document from the perspective of this vector. We're going to ignore everything else. Basically, what we see here is only\nthe vector root condition of the document. Of course,\nthe document has all information. For example, the orders of\nwords are [INAUDIBLE] model and that's because we assume that\nthe [INAUDIBLE] of words will [INAUDIBLE]. So with this presentation\nyou can really see d1 simply suggests a [INAUDIBLE] library. Now this is different from another\ndocument which might be recommended as a different vector, d2 here. Now in this case, the document that\ncovers programming and library, but it doesn't talk about presidential. So what does this remind you? Well you can probably guess the topic\nis likely about program language and the library is software lab library. So this shows that by using\nthis vector space reproduction, we can actually capture the differences\nbetween topics of documents. Now you can also imagine\nthere are other vectors. For example,\nd3 is pointing into that direction, that might be a presidential program. And in fact we can place all\nthe documents in this vector space. And they will be pointing\nto all kinds of directions. And similarly, we're going to place our query also\nin this space, as another vector. And then we're going to measure the\nsimilarity between the query vector and every document vector. So in this case for example, we can easily see d2 seems to be\nthe closest to this query vector. And therefore,\nd2 will be rendered above others. So this is basically the main\nidea of the vector space model. So to be more precise, vector space model is a framework. In this framework,\nwe make the following assumptions. First, we represent a document and\nquery by a term vector. So here a term can be any basic concept. For example, a word or a phrase or\neven n gram of characters. Those are just sequence of\ncharacters inside a word. Each term is assumed that will\nbe defined by one dimension. Therefore n terms in our vocabulary,\nwe define N-dimensional space. A query vector would consist\nof a number of elements corresponding to the weights\non different terms. Each document vector is also similar. It has a number of elements and\neach value of each element is indicating the weight of\nthe corresponding term. Here, you can see,\nwe assume there are N dimensions. Therefore, they are N elements each corresponding to the weight\non the particular term. So the relevance in this case will be assumed to be the similarity\nbetween the two vectors. Therefore, our ranking function\nis also defined as the similarity between the query vector and\ndocument vector. Now if I ask you to write a program\nto implement this approach in a search engine. You would realize that\nthis was far from clear. We haven't said a lot of things in detail, therefore it's impossible to actually\nwrite the program to implement this. That's why I said, this is a framework. And this has to be refined\nin order to actually suggest a particular ranking function\nthat you can implement on a computer. So what does this framework not say? Well, it actually hasn't said many things that would be required in order\nto implement this function. First, it did not say how we should define\nor select the basic concepts exactly. We clearly assume\nthe concepts are orthogonal. Otherwise, there will be redundancy. For example, if two synonyms or somehow\ndistinguish it as two different concepts. Then they would be defining\ntwo different dimensions and that would clearly cause redundancy here. Or all the emphasizing of\nmatching this concept, because it would be as if\nyou match the two dimensions when you actually matched\none semantic concept. Secondly, it did not say how we\nexactly should place documents and the query in this space. Basically that show you some examples\nof query and document vectors. But where exactly should the vector for\na particular document point to? So this is equivalent to how\nto define the term weights? How do you compute the lose\nelement values in those vectors? This is a very important question, because term weight in the query vector\nindicates the importance of term. So depending on how you assign the weight, you might prefer some terms\nto be matched over others. Similarly, the total word in\nthe document is also very meaningful. It indicates how well the term\ncharacterizes the document. If you got it wrong then you clearly\ndon't represent this document accurately. Finally, how to define the similarity\nmeasure is also not given. So these questions must be addressed\nbefore we can have a operational function that we can actually\nimplement using a program language. So how do we solve these problems is the main topic of the next lecture. [MUSIC]",
 "06_lesson-1-6-vector-space-retrieval-model-simplest-instantiation.en.txt": "In this lecture we're going to talk about how to instantiate\nvector space model so that we can get very\nspecific ranking function. So this is to continue the discussion\nof the vector space model, which is one particular approach\nto design a ranking function. And we're going to talk about how\nwe use the general framework of the the vector space\nmodel as a guidance to instantiate the framework to derive\na specific ranking function. And we're going to cover the symbolist\ninstantiation of the framework. So as we discussed in\nthe previous lecture, the vector space model\nis really a framework. And this didn't say. As we discussed in the previous lecture,\nvector space model is really a framework. It does not say many things. So, for example, here it shows that it did not say\nhow we should define the dimension. It also did not say how we place\na document vector in this space. It did not say how we place a query\nvector in this vector space. And, finally, it did not say how we\nshould measure the similarity between the query vector and the document vector. So you can imagine,\nin order to implement this model, we have to say specifically\nhow we compute these vectors. What is exactly xi? And what is exactly yi? This will determine where\nwe place a document vector, where we place a query vector. And, of course, we also need to say exactly what\nshould be the similarity function. So if we can provide a definition\nof the concepts that would define the dimensions and these xi's or\nyi's and namely weights of terms for queries and document, then we will be\nable to place document vectors and query vectors in this well defined space. And then,\nif we also specify similarity function, then we'll have a well\ndefined ranking function. So let's see how we can do that and\nthink about the instantiation. Actually, I would suggest you to\npause the lecture at this point, spend a couple minutes to think about. Suppose you are asked\nto implement this idea. You have come up with the idea of vector\nspace model, but you still haven't figured out how to compute these vectors exactly,\nhow to define the similarity function. What would you do? So, think for a couple of minutes,\nand then proceed. So, let's think about some simplest ways\nof instantiating this vector space model. First, how do we define the dimension? Well, the obvious choice is to use each word in our vocabulary\nto define the dimension. And show that there are N\nwords in our vocabulary. Therefore, there are N dimensions. Each word defines one dimension. And this is basically\nthe bag of words with Now let's look at how we\nplace vectors in this space. Again here, the simplest strategy is to use a Bit Vector to represent\nboth the query and a document. And that means each element, xi and yi will be taking a value\nof either zero or 1. When it's 1, it means the corresponding word is\npresent in the document or in the query. When it's 0,\nit's going to mean that it's absent. So you can imagine if the user\ntypes in a few words in the query, then the query vector will only\nhave a few 1's, many, many zeros. The document vector,\ngenerally we have more 1's, of course. But it will also have many zeros since\nthe vocabulary is generally very large. Many words don't really\noccur in any document. Many words will only occasionally\noccur in a document. A lot of words will be absent\nin a particular document. So now we have placed the documents and\nthe query in the vector space. Let's look at how we\nmeasure the similarity. So, a commonly used similarity\nmeasure here is Dot Product. The Dot Product of two\nvectors is simply defined as the sum of the products of the\ncorresponding elements of the two vectors. So, here we see that it's\nthe product of x1 and y1. So, here. And then, x2 multiplied by y2. And then, finally, xn multiplied by yn. And then, we take a sum here. So that's a Dot Product. Now, we can represent this in a more\ngeneral way using a sum here. So this is only one of the many different\nways of measuring the similarity. So, now we see that we have\ndefined the dimensions, we have defined the vectors, and we have\nalso defined the similarity function. So now we finally have the simplest\nvector space model, which is based on the bit vector [INAUDIBLE] dot product\nsimilarity and bag of words [INAUDIBLE]. And the formula looks like this. So this is our formula. And that's actually a particular retrieval\nfunction, a ranking function right? Now we can finally implement this\nfunction using a program language, and then rank the documents for query. Now, at this point you should\nagain pause the lecture to think about how we can\ninterpreted this score. So, we have gone through the process\nof modeling the retrieval problem using a vector space model. And then,\nwe make assumptions about how we place vectors in the vector space, and\nhow do we define the similarity. So in the end, we've got a specific\nretrieval function shown here. Now, the next step is to think about\nwhether this retrieval function actually makes sense, right? Can we expect this function\nto actually perform well when we used it to rank documents for\nuser's queries? So it's worth thinking about what is\nthis value that we are calculating. So, in the end, we'll get a number. But what does this number mean? Is it meaningful? So, spend a couple minutes\nto sort of think about that. And, of course, the general question here is do you\nbelieve this is a good ranking function? Would it actually work well? So, again,\nthink about how to interpret this value. Is it actually meaningful? Does it mean something? This is related to how well\nthe document matched the query. So, in order to assess\nwhether this simplest vector space model actually works well,\nlet's look at the example. So, here I show some sample documents and\na sample query. The query is news about\nthe presidential campaign. And we have five documents here. They cover different terms in the query. And if you look at these documents for\na moment, you may realize that some documents are probably relevant, and\nsome others are probably not relevant. Now, if I asked you to rank these\ndocuments, how would you rank them? This is basically our ideal ranking. When humans can examine the documents,\nand then try to rank them. Now, so think for a moment,\nand take a look at this slide. And perhaps by pausing the lecture. So I think most of you would\nagree that d4 and d3 are probably better than others because they\nreally cover the query well. They match news,\npresidential and campaign. So, it looks like these documents\nare probably better than the others. They should be ranked on top. And the other three d2, d1, and\nd5 are really not relevant. So we can also say d4 and\nd3 are relevant documents, and d1, d2 and d5 are non-relevant. So now let's see if our simplest\nvector space model could do the same, or could do something closer. So, let's first think about\nhow we actually use this model to score documents. All right. Here I show two documents, d1 and d3. And we have the query also here. In the vector space model, of course we\nwant to first compute the vectors for these documents and the query. Now, I showed the vocabulary here as well. So these are the end dimensions\nthat we'll be thinking about. So what do you think is the vector for\nthe query? Note that we're assuming\nthat we only use zero and 1 to indicate whether a term is absent or\npresent in the query or in the document. So these are zero,1 bit vectors. So what do you think is the query vector? Well, the query has four words here. So for these four words,\nthere will be a 1. And for the rest, there will be zeros. Now, what about the documents? It's the same. So d1 has two rows, news and about. So, there are two 1's here,\nand the rest are zeroes. Similarly, so now that we have the two vectors,\nlet's compute the similarity. And we're going to use Do Product. So you can see when we use Dot Product, we just multiply the corresponding\nelements, right? So these two will be formal product, and these two will\ngenerate another product, and these two will generate yet\nanother product and so on, so forth. Now you can easily see if we do that,\nwe actually don't have to care about these zeroes because whenever we have\na zero the product will be zero. So when we take a sum\nover all these pairs, then the zero entries will be gone. As long as you have one zero,\nthen the product would be zero. So, in the fact, we're just\ncounting how many pairs of 1 and 1. In this case, we have seen two,\nso the result will be 2. So what does that mean? Well, that means this number, or\nthe value of this scoring function, is simply the count of how many unique\nquery terms are matched in the document. Because if a term is matched in the\ndocument, then there will be two one's. If it's not, then there will\nbe zero on the document side. Similarly, if the document has a term but\nthe term is not in the query, there will be a zero in the query vector. So those don't count. So, as a result,\nthis scoring function basically measures how many unique query\nterms are matched in a document. This is how we interpret this score. Now, we can also take a look at d3. In this case, you can see the result\nis 3 because d3 matched to the three distinctive query words news, presidential\ncampaign, whereas d1 only matched the two. Now in this case, this seems\nreasonable to rank d3 on top of d1. And this simplest vector\nspace model indeed does that. So that looks pretty good. However, if we examine this model in\ndetail, we likely will find some problems. So, here I'm going to show all\nthe scores for these five documents. And you can easily verify they're\ncorrect because we're basically counting the number of unique query\nterms matched in each document. Now note that this measure\nactually makes sense, right? It basically means if a document\nmatches more unique query terms, then the document will be\nassumed to be more relevant. And that seems to make sense. The only problem is here we can note that\nthere are three documents, d2, d3 and d4. And they tied with a 3 as a score. So, that's a problem because if you look\nat them carefully, it seems that the d4 should be ranked above d3 because d3 only mentions the presidential once,\nbut d4 mentioned it multiple times. In the case of d3,\npresidential could be an dimension. But d4 is clearly above\nthe presidential campaign. Another problem is that d2 and\nd3 also have the same score. But if you look at the three words\nthat are matched, in the case of d2, it matched the news, about and campaign. But in the case of d3, it matched news,\npresidential and campaign. So intuitively this reads better\nbecause matching presidential is more important than matching about, even though about and\nthe presidential are both in the query. So intuitively,\nwe would like d3 to be ranked above d2. But this model doesn't do that. So that means this model\nis still not good enough. We have to solve these problems. To summarize, in this lecture we talked about how\nto instantiate a vector space model. We mainly need to do three things. One is to define the dimension. The second is to decide how to place\ndocuments as vectors in the vector space, and to also place a query in\nthe vector space as a vector. And third is to define\nthe similarity between two vectors, particularly the query vector and\nthe document vector. We also talked about various simple way\nto instantiate the vector space model. Indeed, that's probably the simplest\nvector space model that we can derive. In this case,\nwe use each word to define the dimension. We use a zero, 1 bit vector to\nrepresent a document or a query. In this case, we basically only care\nabout word presence or absence. We ignore the frequency. And we use the Dot Product\nas the similarity function. And with such a instantiation, we showed that the scoring\nfunction is basically to score a document based on the number of distinct\nquery words matched in the document. We also showed that such a simple vector\nspace model still doesn't work well, and we need to improve it. And this is a topic that we're\ngoing to cover in the next lecture. [MUSIC]",
 "01_lesson-2-1-vector-space-model-improved-instantiation.en.txt": "[SOUND] In this lecture, we are going to talk about how\nto improve the instantiation of the vector space model. This is a continued discussion\nof the vector space model. We're going to focus on how to improve\nthe instantiation of this model. In the previous lecture, you have seen that with simple\ninstantiations of the vector space model, we can come up with a simple scoring\nfunction that would give us basically an account of how many unique query\nterms are matched in the document. We also have seen that this function\nhas a problem, as shown on this slide. In particular,\nif you look at these three documents, they will all get the same score because\nthey match the three unique query words. But intuitively we would like\nd4 to be ranked above d3, and d2 is really not relevant. So the problem here is that this function\ncouldn't capture the following heuristics. First, we would like to give\nmore credit to d4 because it matched presidential more times than d3. Second, intuitively, matching presidential\nshould be more important than matching about, because about is a very\ncommon word that occurs everywhere. It doesn't really carry that much content. So in this lecture, let's see how we can improve the model\nto solve these two problems. It's worth thinking at this point\nabout why do we have these problems? If we look back at assumptions we have\nmade while instantiating the vector space model,\nwe'll realize that the problem is really coming from\nsome of the assumptions. In particular, it has to do with how we\nplaced the vectors in the vector space. So then naturally,\nin order to fix these problems, we have to revisit those assumptions. Perhaps we will have to use different ways\nto instantiate the vector space model. In particular, we have to place\nthe vectors in a different way. So let's see how we can improve this. One natural thought is in order to\nconsider multiple times of a term in the document, we should consider the term frequency\ninstead of just the absence or presence. In order to consider the difference\nbetween a document where a query term occurred multiple times and one\nwhere the query term occurred just once, we have to consider the term frequency,\nthe count of a term in the document. In the simplest model, we only modeled\nthe presence and absence of a term. We ignored the actual number of times\nthat a term occurs in a document. So let's add this back. So we're going to then\nrepresent a document by a vector with term frequency as element. So that is to say, now the elements\nof both the query vector and the document vector will not be 0 or\n1s, but instead they will be the counts of\na word in the query or the document. So this would bring in additional\ninformation about the document, so this can be seen as more accurate\nrepresentation of our documents. So now let's see what the formula\nwould look like if we change this representation. So as you'll see on this slide,\nwe still use dot product. And so the formula looks\nvery similar in the form. In fact, it looks identical. But inside the sum, of course,\nx i and y i are now different. They are now the counts of word i in the query and in the document. Now at this point I also suggest you\nto pause the lecture for a moment and just to think about how we can interpret\nthe score of this new function. It's doing something very similar\nto what the simplest VSM is doing. But because of the change of the vector, now the new score has\na different interpretation. Can you see the difference? And it has to do with the consideration\nof multiple occurrences of the same term in a document. More importantly, we would like to know\nwhether this would fix the problems of the simplest vector space model. So let's look at this example again. So suppose we change the vector\nrepresentation to term frequency vectors. Now let's look at these\nthree documents again. The query vector is the same\nbecause all these words occurred exactly once in the query. So the vector is still a 01 vector. And in fact, d2 is also essentially\nrepresenting the same way because none of these words\nhas been repeated many times. As a result,\nthe score is also the same, still 3. The same is true for d3,\nand we still have a 3. But d4 would be different, because\nnow presidential occurred twice here. So the ending for presidential in the\ndocument vector would be 2 instead of 1. As a result, now the score for\nd4 is higher. It's a 4 now. So this means by using term frequency, we can now rank d4 above d2 and\nd3, as we hoped to. So this solved the problem with d4. But we can also see that d2 and\nd3 are still filtering the same way. They still have identical scores,\nso it did not fix the problem here. So how can we fix this problem? Intuitively, we would like\nto give more credit for matching presidential than matching about. But how can we solve\nthe problem in a general way? Is there any way to determine\nwhich word should be treated more importantly and\nwhich word can be basically ignored? About is such a word which does not\nreally carry that much content. We can essentially ignore that. We sometimes call such\na word a stock word. Those are generally very frequent and\nthey occur everywhere. Matching it doesn't really mean anything. But computationally how\ncan we capture that? So again, I encourage you to\nthink a little bit about this. Can you came up with any statistical\napproaches to somehow distinguish presidential from about? Now if you think about it for a moment, you'll realize that one difference is\nthat a word like above occurs everywhere. So if you count the occurrence of\nthe word in the whole collection, then we will see that about has much\nhigher frequency than presidential, which tends to occur\nonly in some documents. So this idea suggests\nthat we could somehow use the global statistics of terms or some other information\nto trying to down-weight the element of about in\na vector representation of d2. At the same time,\nwe hope to somehow increase the weight of presidential\nin the vector of d3. If we can do that, then we can\nexpect that d2 will get the overall score to be less than 3 while\nd3 will get the score above 3. Then we would be able to\nrank d3 on top of d2. So how can we do this systematically? Again, we can rely on\nsome statistical count. And in this case, the particular idea\nis called inverse document frequency. Now we have seen document\nfrequency as one signal used in the modern retrieval functions. We discussed this in a previous lecture. So here is the specific way of using it. Document frequency is the count of\ndocuments that contain a particular term. Here we say inverse document frequency\nbecause we actually want to reward a word that doesn't occur in many documents. And so the way to incorporate this\ninto our vector representation is then to modify the frequency\ncount by multiplying it by the IDF of the corresponding word,\nas shown here. If we can do that,\nthen we can penalize common words, which generally have a lower IDF, and reward rare words,\nwhich will have a higher IDF. So more specifically, the IDF can be defined as\nthe logarithm of M+1 divided by k, where M is the total number of documents\nin the collection, k is the DF or document frequency, the total number\nof documents containing the word W. Now if you plot this\nfunction by varying k, then you would see the curve\nwould look like this. In general, you can see it\nwould give a higher value for a low DF word, a rare word. You can also see the maximum value\nof this function is log of M+1. It would be interesting for you to think\nabout what's the minimum value for this function. This could be an interesting exercise. Now the specific function\nmay not be as important as the heuristic to simply\npenalize popular terms. But it turns out that this particular\nfunction form has also worked very well. Now whether there's a better\nform of function here is the open research question. But it's also clear that if\nwe use a linear penalization, like what's shown here with this line, then it may not be as\nreasonable as the standard IDF. In particular, you can see\nthe difference in the standard IDF, and we somehow have\na turning point of here. After this point, we're going to say these\nterms are essentially not very useful. They can be essentially ignored. And this makes sense when\nthe term occurs so frequently and let's say a term occurs in more\nthan 50% of the documents, then the term is unlikely very important\nand it's basically a common term. It's not very important\nto match this word. So with the standard IDF you can\nsee it's basically assumed that they all have low weights. There's no difference. But if you look at\nthe linear penalization, at this point that there\nis still some difference. So intuitively we'd want to\nfocus more on the discrimination of low DF words rather\nthan these common words. Well, of course,\nwhich one works better still has to be validated by using the empirically\ncorrelated dataset. And we have to use users to\njudge which results are better. So now let's see how\nthis can solve problem 2. So now let's look at\nthe two documents again. Now without the IDF weighting before,\nwe just have term frequency vectors. But with IDF weighting we\nnow can adjust the TF weight by multiplying with the IDF value. For example,\nhere we can see is adjustment and in particular for about there's adjustment\nby using the IDF value of about, which is smaller than the IDF\nvalue of presidential. So if you look at these,\nthe IDF will distinguish these two words. As a result, adjustment here would be\nlarger, would make this weight larger. So if we score with these new vectors,\nthen what would happen is that, of course,\nthey share the same weights for news and campaign, but the matching of\nabout will discriminate them. So now as a result of IDF weighting,\nwe will have d3 to be ranked above d2 because it matched a rare word,\nwhereas d2 matched a common word. So this shows that the IDF\nweighting can solve problem 2. So how effective is this model in\ngeneral when we used TF-IDF weighting? Well, let's look at all these\ndocuments that we have seen before. These are the new scores\nof the new documents. But how effective is this new weighting\nmethod and new scoring function point? So now let's see overall how effective\nis this new ranking function with TF-IDF weighting. Here we show all the five documents\nthat we have seen before, and these are their scores. Now we can see the scores for the first four documents here\nseem to be quite reasonable. They are as we expected. However, we also see a new\nproblem because now d5 here, which did not have a very high score\nwith our simplest vector space model, now actually has a very high score. In fact, it has the highest score here. So this creates a new problem. This is actually a common phenomenon\nin designing retrieval functions. Basically, when you try\nto fix one problem, you tend to introduce other problems. And that's why it's very tricky how\nto design effective ranking function. And what's the best ranking function\nis their open research question. Researchers are still working on that. But in the next few lectures we're going\nto also talk about some additional ideas to further improve this model and\ntry to fix this problem. So to summarize this lecture, we've talked\nabout how to improve the vector space model, and\nwe've got to improve the instantiation of the vector space model\nbased on TD-IDF weighting. So the improvement is mostly on\nthe placement of the vector where we give high weight to a term that\noccurred many times in a document but infrequently in the whole collection. And we have seen that this\nimproved model indeed looks better than the simplest\nvector space model. But it also still has some problems. In the next lecture we're going to look at\nhow to address these additional problems. [MUSIC]",
 "02_lesson-2-2-tf-transformation.en.txt": "[MUSIC] In this lecture, we continue\nthe discussion of vector space model. In particular, we're going to\ntalk about the TF transformation. In the previous lecture, we have derived a TF idea of weighting\nformula using the vector space model. And we have assumed that this model\nactually works pretty well for these examples as shown on this slide,\nexcept for d5, which has received a very high score. Indeed, it has received the highest\nscore among all these documents. But this document is intuitive and\nnon-relevant, so this is not desirable. In this lecture,\nwe're going to talk about, how we're going to use TF\ntransformation to solve this problem. Before we discuss the details,\nlet's take a look at the formula for this simple TF-IDF\nweighting ranking function. And see why this document has\nreceived such a high score. So this is the formula, and\nif you look at the formula carefully, then you will see it involves a sum\nover all the matched query terms. And inside the sum, each matched\nquery term has a particular weight. And this weight is TF-IDF weighting. So it has an idea of component,\nwhere we see two variables. One is the total number of documents\nin the collection, and that is M. The other is the document of frequency. This is the number of\ndocuments that are contained. This word w. The other variables involved in the formula include\nthe count of the query term. W in the query, and\nthe count of the word in the document. If you look at this document again,\nnow it's not hard to realize that the reason why it hasn't\nreceived a high score is because it has a very high count of campaign. So the count of campaign in this document\nis a 4, which is much higher than the other documents, and has contributed\nto the high score of this document. So in treating the amount\nto lower the score for this document, we need to somehow\nrestrict the contribution of the matching of this\nterm in the document. And if you think about the matching\nof terms in the document carefully, you actually would realize, we probably shouldn't reward\nmultiple occurrences so generously. And by that I mean,\nthe first occurrence of a term says a lot about\nthe matching of this term, because it goes from zero\ncount to a count of one. And that increase means a lot. Once we see a word in the document, it's very likely that the document\nis talking about this word. If we see a extra occurrence on\ntop of the first occurrence, that is to go from one to two,\nthen we also can say that, well the second occurrence kind of confirmed that it's\nnot a accidental managing of the word. Now we are more sure that this\ndocument is talking about this word. But imagine we have seen, let's say,\n50 times of the word in the document. Now, adding one extra occurrence is not\ngoing to test more about the evidence, because we're already sure that\nthis document is about this word. So if you're thinking this way, it seems\nthat we should restrict the contribution of a high count of a term, and\nthat is the idea of TF Transformation. So this transformation function is\ngoing to turn the real count of word into a term frequency weight for\nthe word in the document. So here I show in x axis that we'll count,\nand y axis I show the term frequency weight. So in the previous breaking functions, we actually have imprison rate\nuse some kind of transformation. So for example,\nin the 0/1 bit vector recantation, we actually use such a transformation\nfunction, as shown here. Basically if the count is 0,\nthen it has 0 weight, otherwise it would have a weight of 1. It's flat. Now, what about using\nterm count as TF weight? Well, that's a linear function, so it has\njust exactly the same weight as the count. Now we have just seen that\nthis is not desirable. So what we want is something like this. So for example,\nwith an algorithm function, we can't have a sublinear\ntransformation that looks like this. And this will control the influence\nof really high weight, because it's going to lower its inference. Yet, it will retain\nthe inference of small counts. Or we might want to even bend the curve\nmore by applying logarithm twice. Now people have tried all these methods. And they are indeed working better than\nthe linear form of the transformation. But so far, what works the best seems\nto be this special transformation, called a BM25 transformation. BM stands for best matching. Now in this transformation,\nyou can see there's a parameter k here. And this k controls the upper\nbound of this function. It's easy to see this\nfunction has a upper bound, because if you look at the x divided by\nx + k, where k is a non-active number, then the numerator will never be able\nto exceed the denominator, right? So it's upper bounded by k+1. Now, this is also difference between\nthis transformation function and a logarithm transformation. Which it doesn't have upper bound. Furthermore, one interesting property\nof this function is that, as we vary k, we can actually simulate different\ntransformation functions. Including the two extremes\nthat are shown here. That is, the 0/1 bit transformation and\nthe linear transformation. So for example, if we set k to 0,\nnow you can see the function value will be 1. So we precisely recover\nthe 0/1 bit transformation. If you set k to very large\nnumber on the other hand, it's going to look more like\nthe linear transformation function. So in this sense,\nthis transformation is very flexible. It allows us to control\nthe shape of the transformation. It also has a nice property\nof the upper bound. And this upper bound is useful to control\nthe inference of a particular term. And so that we can prevent a spammer\nfrom just increasing the count of one term to spam all queries\nthat might match this term. In other words, this upper bound\nmight also ensure that all terms would be counted when we aggregate\nthe weights to compute the score. As I said, this transformation\nfunction has worked well so far. So to summarize this lecture,\nthe main point is that we need to do Sublinear TF Transformation,\nand this is needed to capture the intuition of diminishing\nreturn from higher term counts. It's also to avoid the dominance by\none single term over all others. This BM25 transformation that we\ntalked about is very interesting. It's so far one of the best-performing\nTF Transformation formulas. It has upper bound, and so\nit's also robust and effective. Now if we're plugging this function into\nour TF-IDF weighting vector space model. Then we'd end up having\nthe following ranking function, which has a BM25 TF component. Now, this is already\nvery close to a state of the odd ranking function called BM25. And we'll discuss how we can further\nimprove this formula in the next lecture. [MUSIC]",
 "03_lesson-2-3-doc-length-normalization.en.txt": "[SOUND] This lecture is about Document Length Normalization\nin the Vector Space Model. In this lecture, we will continue\nthe discussion of the vector space model. In particular, we're going to discuss the\nissue of document length normalization. So far in the lectures about the vector\nspace model, we have used the various signals from the document to assess\nthe matching of the document with a query. In particular,\nwe have considered the tone frequency. The count of a tone in a document. We have also considered it's\nglobal statistics such as, IDF, Inverse Document Frequency. But we have not considered\ndocument lengths. So here I show two example documents,\nd4 is much shorter with only 100 words. D6 on the other hand, has a 5000 words. If you look at the matching\nof these query words, we see that in d6, there are more\nmatchings of the query words. But one might reason that,\nd6 may have matched these query words in a scattered manner. So maybe the topic of d6, is not\nreally about the topic of the query. So, the discussion of the campaign\nat the beginning of the document, may have nothing to do with the managing\nof presidential at the end. In general,\nif you think about the long documents, they would have a higher chance for\nmatching any query. In fact, if you generate a long document\nrandomly by assembling words from a distribution of words, then eventually\nyou probably will match an inquiry. So in this sense, we should penalize on\ndocuments because they just naturally have better chance matching to any query, and\nthis is idea of document normalization. We also need to be careful in avoiding\nto over penalize long documents. On the one hand,\nwe want to penalize the long document. But on the other hand,\nwe also don't want to over-penalize them. Now, the reasoning is because\na document that may be long because of different reasons. In one case, the document may be\nlong because it uses more words. So for example, think about the vortex\narticle on the research paper. It would use more words than\nthe corresponding abstract. So, this is a case where we probably\nshould penalize the matching of long documents such as a full paper. When we compare the matching\nof words in such a long document with matching of\nthe words in the shop abstract. Then long papers in general, have a higher chance of matching clearer\nwords, therefore, we should penalize them. However, there is another case\nwhen the document is long, and that is when the document\nsimply has more content. Now consider another\ncase of long document, where we simply concatenate a lot\nof abstracts of different papers. In such a case, obviously, we don't want\nto over-penalize such a long document. Indeed, we probably don't want to penalize\nsuch a document because it's long. So that's why, we need to be careful about\nusing the right degree of penalization. A method of that has been working well,\nbased on recent results, is called a pivoted length normalization. And in this case, the idea is to use the average document\nlength as a pivot, as a reference point. That means we'll assume that for\nthe average length documents, the score is about right so\nthe normalizer would be 1. But if the document is longer\nthan the average document length, then there will be some penalization. Whereas if it's a shorter,\nthen there is even some reward. So this is illustrated at\nusing this slide, on the axis, x-axis you can see the length of document. On the y-axis, we show the normalizer. In this case, the Pivoted Length\nNormalization formula for the normalizer, is seeing to be interpolation of 1 and the normalize the document in length\ncontrolled by a parameter B here. So you can see here,\nwhen we first divide the length of the document by the average documents,\nthis not only gives us some sense about how this document is\ncompared with average documents, but also gives us a benefit of not\nworrying about the unit of length. We can measure the length by words or\nby characters. Anyway, this normalizer\nhas interesting property. First we see that, if we set the parameter\nb to 0 then the value would be 1. So, there's no lens normalization at all. So, b, in this sense,\ncontrols the lens normalization. Whereas, if we set b to a nonzero value,\nthen the normalizer would look like this. All right, so\nthe value would be higher for documents that are longer than\nthe average document lens. Whereas, the value of\nthe normalizer would be shorter, would be smaller for shorter documents. So in this sense,\nwe see there is a penalization for long documents, and\nthere's a reward for short documents. The degree of penalization\nis controlled by b, because if we set b to a larger value,\nthen the normalizer would look like this. There's even more penalization for\nlong documents and more reward for the short documents. By adjusting b, which varies from 0 to 1, we can control the degree\nof length normalization. So, if we plug in this length\nnormalization fact that into the vector space model, ranking functions\nis that we have already examined them. Then we will end up having\nthe following formulas. And these are in fact the state of\nthe vector space model formulas. Let's take a look at each of them. The first one is called a pivoted length\nnormalization vector space model, and a reference in [INAUDIBLE]\nduration of this model. And here we see that, it's basically\na TFI model that we have discussed, the idea of component should\nbe very familiar to you. There is also a query term\nfrequency component here. And then, in the middle, there is\nthe normalizer tf and in this case, we see we use the double logarithm\nas we discussed before and this is to achieve\na sublinear transformation. But we also put a document\nthe length normalizer in the bottom. Right, so this would cause\npenalization for long document, because the larger the denominator is,\nthen the smaller the is. And this is of course controlled\nby the parameter b here. And you can see again, if b is set to 0\nthen there is no length normalization. Okay, so this is one of the two most\neffective at these base model formulas. The next one called a BM25 or Okapi, is also similar in that it\nalso has a IDF component here, and query IDF component here. But in the middle,\nthe normal issue's a little bit different. As we explained,\nthere is our copy tf transformation here, and that does sublinear\ntransformation with the upper bound. In this case we have put the length\nnormalization factor here. We're adjusting k but\nit achieves a similar factor, because we put a normalizer\nin the denominator. Therefore, again, if a document is longer\nthen the term weight will be smaller. So you can see after we have gone through\nall the n answers that we talked about, and we have in the end reached\nthe basically the state of god functions. So, So far, we have talked about mainly how to place the document\nvector in the vector space. And, this has played an important role\nin determining the effectiveness of the simple function. But there are also other dimensions,\nwhere we did not really examine details. For example, can we further\nimprove the instantiation of the dimension of the Vector Space Model? Now, we've just assumed that the bag\nof words representation should issue dimension as a word but obviously,\nwe can see there are many other choices. For example, a stemmed word, those\nare the words that haven't transformed into the same root form, so that computation and computing were all\nbecome the same and they can be match. We get those stop word removal. This is to remove some very common words\nthat don't carry any content like the off. We get use of phrases\nto define dimensions. We can even use later in\nthe semantical analysis, it will find some clusters of words that represent the\na late in the concept as one by an engine. We can also use smaller unit,\nlike a character end grams those are sequences of and\nthe characters for dimensions. However, in practice, people have found\nthat the bag-of-words representation with phrases is still the most effective\none and it's also efficient. So, this is still so far the most\npopular dimension instantiation method. And it's used in all major search engines. I should also mention, that sometimes\nwe need to do language specific and domain specific tokenization. And this is actually very important, as we\nmight have variations of terms that might prevent us from matching them with each\nother, even when they mean the same thing. In some languages like Chinese,\nthere is also the challenge in segmenting text to obtain word band rates because\nit's just a sequence of characters. A word might correspond to one\ncharacter or two characters or even three characters. So, it's easier in English when we\nhave a space to separate the words. In some other languages, we may need\nto do some Americanize processing to figure a way out of what\nare the boundaries for words. There is also the possibility to\nimprove the similarity of the function. And so\nfar we have used as a top product, but one can imagine there are other measures. For example, we can measure the cosine\nof the angle between two vectors. Or we can use Euclidean distance measure. And these are all possible, but dot product seems still the best and\none reason is because it's very general. In fact that it's sufficiently general, if you consider the possibilities\nof doing waiting in different ways. So, for example, cosine measure can be thought of as the\nthought product of two normalized factors. That means, we first normalize each factor\nand then we take the thought product. That would be critical\nto the cosine measure. I just mentioned that the BM25, seems to\nbe one of the most effective formulas. But there has been also further\ndevelopments in improving BM25. Although, none of these words have\nchanged the BM25 fundamental. So in one line work,\npeople have divide the BM25 F. Here, F stands for field, and this is\nuse BM25 for documents with structures. So for example, you might consider\na title field, the abstract, or body of the research article. Or even anchor text on the web page,\nthose are the text fields that describe links to other pages and\nthese can all be combined with a proper way of different fields to help\nimprove scoring for different documents. When we use BM25 for such a document and the obvious choice is to apply BM25 for\neach field and then combine the scores. Basically, the idea of BM25F is\nto first combine the frequency counts of terms in all the fields,\nand then apply BM25. Now, this has advantage of avoiding over\ncounting the first occurrence of the term. Remember in the sublinear\ntransformation of TF, the first occurrence is very important and\nit contributes a large weight. And if we do that for all the fields, then the same term might have gained\na lot of advantage in every field. But when we combine these\nword frequencies together, we just do the transformation one time. At that time, then the extra occurrences will not be\ncounted as fresh first recurrences. And this method has been working very well\nfor scoring structure with documents. The other line of extension\nis called a BM25+. In this line,\nrisk is to have to address the problem of over penalization of\nlong documents by BM25. So to address this problem,\nthe fix is actually quite simple. We can simply add a small constant\nto the TF normalization formula. But what's interesting is that,\nwe can analytically prove that by doing such a small modification,\nwe will fix the problem of over penalization of\nlaw documents by the original BM25. So the new formula called BM25+, is empirically and\nanalytically shown to be better than BM25. So to summarize all what we have\nsaid about vector space model, here are the major take away points. First, in such a model,\nwe use the similarity of relevance. Assuming that relevance of a document\nwith respect to a query, is basically proportional to the similarity\nbetween the query and the document. So naturally,\nthat implies that the query and document must have been\nrepresented in the same way. And in this case, we will present them as\nvectors in high-dimensional vector space. Where the dimensions are defined by words,\nor concepts, or terms, in general. And we generally, need to use a lot of\nheuristics to design the ranking function. We use some examples, which show\nthe needs for several heuristics, including Tf weighting and transformation. And IDF weighting, and\ndocument length normalization. These major heuristics are the most\nimportant of heuristics, to ensure such a general ranking function\nto work well for all kinds of test. And finally, BM25 and\npivoted normalization seem to be the most effective formulas\nout of the vector space model. Now I have to say that, I put BM25 in\nthe category of vector space model, but in fact, the BM25 has been derived\nusing probabilistic model. So the reason why I've put it in\nthe vector space model is first, the ranking function actually has a nice\ninterpretation in the vector space model. We can easily see, it looks very much like a vector space\nmodel, with a special waiting function. The second reason is because the original\nBM25, has somewhat different form of IDF. And that form of IDF after\nthe [INAUDIBLE] doesn't work so well as the standard IDF\nthat you have seen here. So as effective retrieval function, BM25 should probably use a heuristic\nmodification of the IDF. To make them even more look\nlike a vector space model There are some additional readings. The first is, a paper about\nthe pivoted length normalization. It's an excellent example\nof using empirical data analysis to suggest the need for\nlength normalization and then further derive the length\nnormalization formula. The second, is the original paper\nwhere the BM25 was proposed. The third paper,\nhas a thorough discussion of BM25 and its extensions, particularly BM25 F. And finally, in the last paper\nhas a discussion of improving BM25 to correct the over\npenalization of long documents. [MUSIC]",
 "04_lesson-2-4-implementation-of-tr-systems.en.txt": "[MUSIC] This lecture is about the implementation\nof text retrieval systems. In this lecture we will discuss\nhow we can implement a text retrieval method to build a search engine. The main challenge is to\nmanage a lot of text data and to enable a query to be answered very\nquickly and to respond to many queries. This is a typical text\nretrieval system architecture. We can see the documents are first\nprocessed by a tokenizer to get tokenized units, for example, words. And then, these words, or\ntokens, will be processed by a indexer that will create a index,\nwhich is a data structure for the search engine to use\nto quickly answer a query. And the query would be going\nthrough a similar processing step. So the Tokenizer would be\napprised of the query as well, so that the text can be\nprocessed in the same way. The same units would be\nmatched with each other. The query's representation would\nthen be given to the Scorer, which would use the index to quickly\nanswer user's query by scoring the documents and then ranking them. The results will be given to the user. And then the user can look at the results\nand provided us some feedback that can be explicit judgements of both\nwhich documents are good, which documents are bad. Or implicit feedback such as so that\nuser didn't have to do anything extra. End user will just look at the results,\nand skip some, and\nclick on some result to view. So these interacting signals can be used\nby the system to improve the ranking accuracy by assuming that viewed documents\nare better than the skipped ones. So a search engine system then\ncan be divided into three parts. The first part is the indexer, and\nthe second part is a Scorer that responds to the users query, and\nthe third part is a Feedback mechanism. Now typically, the Indexer is\ndone in the offline manner, so you can pre-process the correct data and to build the inventory index,\nwhich we will introduce in moment. And this data structure can then be used\nby the online module which is a scorer to process a user's query dynamically and\nquickly generate search results. The feedback mechanism can be done online\nor offline, depending on the method. The implementation of the indexer and\nthe scorer is very standard, and this is the main topic of this\nlecture and the next few lectures. The feedback mechanism,\non the other hand, has variations, it depends on which method is used. So that is usually done in\nalgorithms specific way. Let's first talk about the tokenizer. Tokernization is a normalized lexical\nunits in through the same form, so that semantically similar words\ncan be matched with each other. Now, in the language like English,\nstemming is often used and this will map all the inflectional\nforms of words into the same root form. So for example, computer, computation, and computing can all be matched\nto the root form compute. This way all these different forms of\ncomputing can be matched with each other. Now normally, this is a good idea, to increase the coverage of documents\nthat are matched up with this query. But it's also not always beneficial, because sometimes the subtlest\ndifference between computer and computation might still suggest the\ndifference in the coverage of the content. But in most cases,\nstemming seems to be beneficial. When we tokenize the text in some other\nlanguages, for example Chinese, we might face some special challenges in segmenting\nthe text to find the word boundaries. Because it's not obvious\nwhere the boundary is as there's no space to separate them. So here of course, we have to use some\nlanguage specific processing techniques. Once we do tokenization, then we would\nindex the text documents and than it'll convert the documents and do some data\nstructure that can enable faster search. The basic idea is to precompute\nas much as we can basically. So the most commonly used index\nis call an Inverted index. And this has been used\nin many search engines to support basic search algorithms. Sometimes the other indices, for example, document index might be needed in order\nto support feedback, like I said. And these kind of techniques\nare not really standard in that they vary a lot according\nto the feedback methods. To understand why we want to use\ninverted index it will be useful for you to think about how you would\nrespond to a single term query quickly. So if you want to use more time to\nthink about that, pause the video. So think about how you can\npre process the text data so that you can quickly respond\nto a query with just one word. Where if you have thought\nabout that question, you might realize that where\nthe best is to simply create the list of documents that match\nevery term in the vocabulary. In this way, you can basically\npre-construct the answers. So when you see a term you can simply just\nto fetch the random list of documents for that term and return the list to the user. So that's the fastest way to\nrespond to a single term here. Now the idea of the invert index\nis actually, basically, like that. We're going to do pre-constructed\nsearch an index, that will allows us to quickly find all the documents\nthat match a particular term. So let's take a look at this example. We have three documents here, and these are the documents that you\nhave seen in some previous lectures. Suppose that we want to create\nan inverted index for these documents. Then we want to maintain a dictionary, in\nthe dictionary we will have one entry for each term and we're going to store\nsome basic statistics about the term. For example, the number of\ndocuments that match the term, or the total number of code or\nfrequency of the term, which means we would kind of duplicate\nthe occurrences of the term. And so, for example, news, this term occur in all\nthe three documents, so the count of documents is three. And you might also realize we needed this\ncount of documents, or document frequency, for computing some statistics to\nbe used in the vector space model. Can you think of that? So what weighting heuristic\nwould need this count. Well, that's the idea, right,\ninverse document frequency. So, IDF is the property of a term,\nand we can compute it right here. So, with the document that count here,\nit's easy to compute the idea of, either at this time, or\nwith the old index, or. At random time when we see a query. Now in addition to these basic statistics, we'll also store all the documents\nthat matched the news, and these entries are stored\nin the file called Postings. So in this case it matched\nthree documents and we store information about\nthese three documents here. This is the document id,\ndocument 1 and the frequency is 1. The tf is one for news, in the second\ndocument it's also 1, et cetera. So from this list, we can get all\nthe documents that match the term news and we can also know the frequency\nof news in these documents. So, if the query has just one word,\nnews, and we have easily look up to this\ntable to find the entry and go quicker into the postings to fetch\nall the documents that matching yours. So, let's take a look at another term. This time, let's take a look\nat the word presidential. This would occur in only one document,\ndocument 3. So the document frequency is 1 but\nit occurred twice in this document. So the frequency count is two, and\nthe frequency count is used for some other reachable method where\nwe might use the frequency to assess the popularity of\na term in the collection. Similarly we'll have a pointer\nto the postings here, and in this case,\nthere is only one entry here because the term occurred in just one document and\nthat's here. The document id is 3 and\nit occurred twice. So this is the basic\nidea of inverted index. It's actually pretty simple, right? With this structure we can easily fetch\nall the documents that match a term. And this will be the basis for\nscoring documents for a query. Now sometimes we also want to store\nthe positions of these terms. So in many of these cases the term\noccurred just once in the document. So there's only one position for\nexample in this case. But in this case, the term occurred\ntwice so there's two positions. Now the position information is very\nuseful for the checking whether the matching of query terms is\nactually within a small window of, let's say, five words or ten words. Or, whether the matching of the two query\nterms is, in fact, a phrase of two words. That this can all be checked quickly\nby using the position from each. So, why is inverted index good for\nfast search? Well, we just talked about the possibility\nof using the two answer single-term query. And that's very easy. What about the multiple term queries? Well let's first look at the some\nspecial cases of the Boolean query. A Boolean query is basically\na Boolean expression like this. So I want the value in the document\nto match both term A and term B. So that's one conjunctive query. Or I want the web documents\nto match term A or term B. That's a disjunctive query. But how can we answer such\na query by using inverted index? Well if you think a bit about it, it would be obvious because\nwe have simply fetch all the documents that match term A and also\nfetch all the documents that match term B. And then just take the intersection\nto answer a query like A and B. Or to take the union to\nanswer the query A or B. So this is all very easy to answer. It's going to be very quick. Now what about the multi-term\nkeyword query? We talked about the vector space model for\nexample and we will do a match such query with\ndocument and generate the score. And the score is based on\naggregated term weights. So in this case it's not\nthe Boolean query but the scoring can be actually\ndone in similar way. Basically it's similar to\ndisjunctive Boolean query. Basically, it's like A or B. We take the union of all the documents\nthat match at least one query term and then we would aggregate the term weights. So this is a basic idea of using inverted\nindex for scoring documents in general. And we're going to talk about\nthis in more detail later. But for now, let's just look at the question\nwhy is in both index, a good idea? Basically why is more efficient than\nsequentially just scanning documents. This is the obvious approach. You can just compute a score for each\ndocument and then you can then sort them. And this is a straightforward method but this is going to be very slow imagine\nthe wealth, there's a lot of documents. If you do this then it will take\na long time to answer your query. So the question now is why would\nthe invert index be much faster? Well it has to do is the word\ndistribution in text. So, here's some common phenomena\nof word distribution in the text. There are some languages independent\nof patterns that seem to be stable. And these patterns are basically\ncharacterized by the following pattern. A few words like the common\nwords like the, a, or we occur very, very frequently in text. So they account for\na large percent of occurrences of words. But most words would occur just rarely. There are many words that occur just once, let's say, in a document or\nonce in the collection. And there are many such. It's also true that the most\nfrequent the words in one corpus they have to be rare in another. That means although the general\nphenomenon is applicable, was observed in many cases that\nexact words that are common may vary from context to context. So this phenomena is characterized\nby what's called a Zipf's Law. This law says that the rank of a word multiplied by the frequency of\nthe word is roughly constant. So formally if we use F(w)\nto denote the frequency, r(w) to denote the rank of a word. Then this is the formula. It basically says the same thing,\njust mathematical term. Where C is basically a constant and\nso, and there is also a parameter, alpha, that might be adjusted to\nbetter fit any empirical observations. So if I plot the word\nfrequencies in sorted order, then you can see this more easily. The x axis is basically the word rank. This is r(w) and\nthe y axis is word frequency or F(w). Now this curve shows that the product\nof the two is roughly the constant. Now if you look at these words, we can see\nThey can be separated into three groups. In the middle,\nit's the intermediary frequency words. These words tend to occur\nquite in a few documents, but they are not like those\nmost frequent words. And they are also not very rare. So they tend to be often used in queries and they also tend\nto have high TF-IDF weights. These intermediate frequency words. But if you look at the left\npart of the curve, these are the highest frequency words. They are covered very frequently. They are usually words,\nlike the, we, of Etc. Those words are very, very frequent and\nthey are in fact the two frequent to be discriminated, and they are generally\nnot very useful for retrieval. So they are often removed and\nthis is called the stop words removal. So you can use pretty much just the kind\nof words in the collection to kind of infer what words might be stop words. Those are basically\nthe highest frequency words. And they also occupy a lot of\nspace in the inverted index. You can imagine the posting entries for\nsuch a word would be very long. And then therefore, if you can remove such words you can save\na lot of space in the inverted index. We also show the tail part,\nwhich has a lot of rare words. Those words don't occur very frequently,\nand there are many such words. Those words are actually very useful for search also, if a user happens to\nbe interested in such a topic. But because they're rare,\nit's often true that users aren't necessarily\ninterested in those words. But retain them would allow us to\nmatch such a document accurately. They generally have very high IDF. So what kind of data structures should\nwe use to store inverted index? Well, it has two parts, right. If you recall, we have a dictionary and\nwe also have postings. The dictionary has modest size, although\nfor the web it's still going to be very large but compare it with\npostings it's more distinct. And we also need to have fast\nrandom access to the entries because we're going to look up\non the query term very quickly. So therefore, we'd prefer to keep such\na dictionary in memory if it's possible. If the collection is not very large,\nthis is feasible, but if the collection is very large\nthen it's in general not possible. If the vocabulary size is very large,\nobviously we can't do that. So, in general that's how it goes. So the data structures\nthat we often use for storing dictionary,\nit would be direct access. There are structures like hash table, or b-tree if we can't store\neverything in memory or use disk. And then try to build a structure that\nwould allow it to quickly look up entries. For postings they are huge. And in general, we don't have to have\ndirect access to a specific entry. We generally would just look up\na sequence of document IDs and frequencies for all the documents\nthat matches the query term. So would read those entries sequentially. And therefore because it's large and\nwe generally have store postings on disc, they have to stay on disc and they would\ncontain information such as document IDs, term frequency or\nterm positions, etcetera. Now because they are very large,\ncompression is often desirable. Now this is not only to save disc space,\nand this is of course one benefit of compression, it It's\nnot going to occupy that much space. But it's also to help improving speed. Can you see why? Well, we know that input and\noutput would cost a lot of time. In comparison with the time taken by CPU. So, CPU is much faster but\nIO takes time and so by compressing the inverter index,\nopposing files will become smaller, and the entries, that we have the readings,\nand memory to process a query term, would be smaller, and\nthen, so we can reduce the amount of tracking IO and\nthat can save a lot of time. Of course, we have to then do more\nprocessing of the data when we uncompress the data in the memory. But as I said CPU is fast. So over all we can still save time. So compression here is both\nto save disc space and to speed up the loading of the index. [MUSIC]",
 "05_lesson-2-5-system-implementation-inverted-index-construction.en.txt": "[SOUND] This lecture is about the inverted index construction. In this lecture, we will continue\nthe discussion of system implementation. In particular, we're going to discuss\nhow to construct the inverted index. The construction of the inverted index\nis actually very easy if the dataset is very small. It's very easy to construct a dictionary\nand then store the postings in a file. The problem is that when our data\nis not able to fit to the memory then we have to use some\nspecial method to deal with it. And unfortunately, in most retrieval\napplications the dataset will be large. And they generally cannot be\nloaded into memory at once. And there are many approaches to\nsolve that problem, and sorting-based method is quite common and\nworks in four steps as shown here. First, you collect the local termID,\ndocumentID and frequency tuples. Basically you will locate the terms\nin a small set of documents. And then once you collect those accounts\nyou can sort those count based on terms. So that you will be able to local\na partial inverted index and these are called rounds. And then you write them into\na temporary file on the disk and then you merge in step 3. Do pairwise merging of these runs, until\nyou eventually merge all the runs and generate a single inverted index. So this is an illustration of this method. On the left you see some documents and on the right we have a term lexicon and\na document ID lexicon. These lexicons are to map string-based\nrepresentations of document IDs or terms into integer representations or map back from integers to\nthe stream representation. The reason why we want our interest\nusing integers to present these IDs is because integers\nare often easier to handle. For example,\nintegers can be used as index for array, and they are also easy to compress. So this is one reason why we tend\nto map these strings into integers, so that we don't have to\ncarry these strings around. So how does this approach work? Well, it's very simple. We're going to scan these\ndocuments sequentially and then parse the documents and\ncount the frequencies of terms. And in this stage we generally sort\nthe frequencies by document IDs, because we process each\ndocument sequentially. So we'll first encounter all\nthe terms in the first document. Therefore the document IDs\nare all ones in this case. And this will be followed by document IDs\ntwo and they are natural results in this only just because we process\nthe data in a sequential order. At some point,\nwe will run out of memory and that would have to write\nthem into the disc. Before we do that we 're going to sort\nthem, just use whatever memory we have. We can sort them and then this time\nwe're going to sort based on term IDs. Note that here,\nwe're using the term IDs as a key to sort. So all the entries that share the same\nterm would be grouped together. In this case,\nwe can see all the IDs of documents that match term 1 would\nbe grouped together. And we're going to write this into\nthat this is a temporary file. And would that allows you to\nuse the memory to process and makes a batch of documents. And we're going to do that for\nall the documents. So we're going to write a lot of\ntemporary files into the disc. And then the next stage is\nwe do merge sort basically. We're going to merge them and\nthen sort them. Eventually, we will get\na single inverted index, where the entries are sorted\nbased on term IDs. And on the top, we're going to see\nthese are the older entries for the documents that match term ID 1. So this is basically, how we can do\nthe construction of inverted index. Even though the data cannot be\nall loaded into the manner. Now, we mention earlier that\nbecause of hostings are very large, it's desirable to compress them. So let's now take a little bit\nhow we compressed inverted index. Well the idea of compression in general,\nis for leverage skewed distributions of values. And we generally have to use\nvariable-length encoding, instead of the fixed-length\nencoding as we use by default in a program manager like C++. And so how can we leverage\nthe skewed distributions of values to compress these values? Well in general, we will use few\nbits to encode those frequent words at the cost of using longer\nbit string code those rare values. So in our case, let's think about how\nwe can compress the TF, tone frequency. Now, if you can picture what\nthe inverted index look like, and you will see in post things,\nthere are a lot of tone frequencies. Those are the frequencies of\nterms in all those documents. Now, if you think about it, what kind\nof values are most frequent there? You probably will be able to guess\nthat small numbers tend to occur far more frequently than large numbers. Why? Well, think about the distribution of\nwords and this is to do the sip of slopes, and many words occur just rarely so\nwe see a lot of small numbers. Therefore, we can use fewer bits for\nthe small, but highly frequent integers and that's cost of using more bits for\nlarger integers. This is a trade off of course. If the values are distributed to uniform,\nthen this won't save us any space, but because we tend to see many small\nvalues, they are very frequent. We can save on average even though\nsometimes when we see a large number we have to use a lot of bits. What about the document IDs\nthat we also saw in postings? Well they are not distributed\nin the skewed way. So how can we deal with that? Well it turns out that we can\nuse a trick called a d-gap and that is to store the difference\nof these term IDs. And we can imagine if a term has\nmatched that many documents then there will be longest of document IDs. So when we take the gap, and we take the\ndifference between adjacent document IDs, those gaps will be small. So again, see a lot of small numbers. Whereas if a term occurred\nin only a few documents, then the gap would be large,\nthe large numbers would not be frequent. So this creates some skewed distribution, that would allow us to\ncompress these values. This is also possible because\nin order to uncover or uncompress these document IDs,\nwe have to sequentially process the data. Because we stored the difference and\nin order to recover the exact document ID we have to first\nrecover the previous document ID. And then we can add the difference to\nthe previous document ID to restore the current document ID. Now this was possible because we only\nneeded to have sequential access to those document IDs. Once we look up the term, we look up all\nthe document IDs that match the term, then we sequentially process them. So it's very natural,\nthat's why this trick actually works. And there are many different methods for\nencoding. So binary code is a commonly used\ncode in just any program language. We use basically fixed glance in coding. Unary code, gamma code, and\ndelta code are all possibilities and there are many other possibilities. So let's look at some\nof them in more detail. Binary coding is really\nequal length coding, and that's a property for\nrandomly distributed values. The unary coding is a variable\nlength in coding method. In this case, integer this 1 will be encoded as x -1, 1 bit followed by 0. So for example, 3 will be encoded as 2,\n1s followed by 0, whereas 5 will be encoded as 4,\n1s, followed by 0, etc. So now you can imagine how many bits do we\nhave to use for a large number like 100? So how many bits do you have to\nuse exactly for a number like 100? Well exactly, we have to use 100 bits. So it's the same number of bits\nas the value of this number. So this is very inefficient if you\nwere likely to see some large numbers. Imagine if you occasionally see a number\nlike 1,000, you have to use 1,000 bits. So this only works well if you\nare absolutely sure that there will be no large numbers, mostly very\noften you see very small numbers. Now, how do you decode this code? Now since these are variable\nlength encoding methods, you can't just count how many bits and\nthen just stop. You can't say 8-bits or 32-bits,\nthen you will start another code. They are variable length, so\nyou will have to rely on some mechanism. In this case for unary, you can see\nit's very easy to see the boundary. Now you can easily see 0 would\nsignal the end of encoding. So you just count up how many 1s you\nhave seen and at the end you hit 0. You have finished one number,\nyou will start another number. Now we just saw that unary\ncoding is too aggressive. In rewarding small numbers, and if you occasionally can see a very\nbig number, it would be a disaster. So what about some other\nless aggressive method? Well gamma coding's one of them and in this method we can use unary coding for a transform form of that. So it's 1 plus the floor of log of x. So the magnitude of this value is\nmuch lower than the original x. So that's why we can afford\nusing unary code for that. And so first I have the unary code for\ncoding this log of x. And this would be followed by\na uniform code or binary code. And this basically the same uniform code,\nand binary code are the same. And we're going to use this coder to code\nthe remaining part of the value of x. And this is basically precisely\nx-1 to the floor of log of x So the unary code are basically\ncalled the flow of log of x, well add one there and here. But the remaining part\nwe'll be using uniform code through actually code the difference between the x and this 2 to the log of x. And it's easy to show that for this difference we only need to use up to this many bits and\nthe floor of log of x bits. And this is easy to understand, if the difference is too large, then we\nwould have a higher floor of log of x. So here are some examples for\nexample, 3 is is encoded as 101. The first two digits are the unary code. So this isn't for the value 2, 10 encodes 2 in unary coding. And so that means the floor of log of x is 1,\nbecause we won't actually use unary codes. In code 1 plus the flow of log of x, since this is two then we know that\nthe flow of log of x is actually 1. So that 3 is still larger than 2 to the 1. So the difference is 1, and\nthe 1 is encoded here at the end. So that's why we have 101 for 3. Now similarly 5 is encoded as 110,\nfollowed by 01. And in this case the unary code in code 3. And so this is a unary code 110 and\nso the flow of log of x is 2. And that means we're going to\ncompute a difference between 5 and the 2 to the 2 and that's 1. And so we now have again 1 at the end. But this time we're going to use 2 bits, because with this level\nof flow of log of x. We could have more numbers a 5, 6, 7 they\nwould all share the same prefix here, 110. So in order to differentiate them, we have to use 2 bits in\nthe end to differentiate them. So you can imagine 6 would be 10 here\nin the end instead of 01 after 10. It's also true that the form of\na gamma code is always the first odd number of bits, and\nin the center there is a 0. That's the end of the unary code. And before that or on the left side\nof this 0, there will be all 1s. And on the right side of this 0,\nit's binary coding or uniform coding. So how can you decode such code? Well you again first do unary coding. Once you hit 0, you have got the unary\ncode and this also tell you how many bits you have to read further\nto decode the uniform code. So this is how you can\ndecode a gamma code. There is also a delta code that's\nbasically the same as a gamma code except that you replace the unary\nprefix with the gamma code. So that's even less\nconservative than gamma code in terms of wording the small integers. So that means, it's okay if you\noccasionally see a large number. It's okay with delta code. It's also fine with the gamma code,\nit's really a big loss for unary code. And they are all operating of course, at different degrees of favoring short or\nfavoring small integers. And that also means they would be\nappropriate for a sorting distribution. But none of them is perfect for\nall distributions. And which method works the best would\nhave to depend on the actual distribution in your dataset. For inverted index compression, people have found that gamma\ncoding seems to work well. So how to uncompress inverted index? I will just talk about this. Firstly, you decode\nthose encoded integers. And we just I think discussed the how we\ndecode unary coding and gamma coding. What about the document IDs that\nmight be compressed using d-gap? Well, we're going to do\nsequential decoding so supposed the encoded I list is x1,\nx2, x3 etc. We first decode x1 to obtain\nthe first document ID, ID1. Then we can decode x2, which is actually the difference between\nthe second ID and the first one. So we have to add the decoder\nvalue of x2 to ID1 to recover the value of the ID at\nthis secondary position. So this is where you can\nsee the advantages of converting document IDs to integers. And that allows us to do\nthis kind of compression. And we just repeat until we\ndecode all the documents. Every time we use the document ID in\nthe previous position to help to recover the document ID in the next position. [MUSIC]",
 "06_lesson-2-6-system-implementation-fast-search.en.txt": "[SOUND] This lecture is about how to do faster\nsearch by using invert index. In this lecture, we're going to continue\nthe discussion of system implementation. In particular, we're going to talk about how to support\na faster search by using invert index. So let's think about what a general\nscoring function might look like. Now of course, the vector space\nmodel is a special case of this, but we can imagine many other retrieval\nfunctions of the same form. So the form of this\nfunction is as follows. We see this scoring function\nof a document D and a query Q is defined as\nfirst a function of fa that adjustment a function that\nwould consider two factors. That I'll assume here at the end, f sub d of d and f sub q of q. These are adjustment factors\nof a document and a query, so they are at the level of a document and\nthe query. So and then inside of this function, we also see there's\nanother function called h. So this is the main part\nof the scoring function and these as I just said of\nthe scoring factors at the level of the whole document and\nthe query. For example, document [INAUDIBLE] and this aggregate punching would\nthen combine all these. Now inside this h function, there are functions that\nwould compute the weights of the contribution of\na matched query term ti. So this g,\nthe function g gives us the weight of a matched query term ti in document d. And this h function would then\naggregate all these weights. So for example,\ntake a sum of all the matched query terms, but it can also be a product or it could\nbe another way of aggregating them. And then finally, this adjustment\nthe functioning would then consider the document level or query level\nfactors to further adjust this score, for example, document [INAUDIBLE]. So, this general form would cover\nmany state of [INAUDIBLE] functions. Let's look at how we can score documents\nwith such a function using virtual index. So, here's a general algorithm\nthat works as follows. First this query level and document level factors can be\npre-computed in the indexing time. Of course, for the query we have to\ncompute it at the query time but for document, for example,\ndocument [INAUDIBLE] can be pre-computed. And then, we maintain a score accumulator\nfor each document d to computer h. An h is an aggregation function\nover all the matching query terms. So how do we do that? For each period term we're going to\ndo fetch the inverted list from the invert index. This will give us all the documents\nthat match this query term and that includes d1, f1 and so dn fn. So each pair is a document ID and\nthe frequency of the term in the document. Then for each entry d sub j and\nf sub j are particular match of the term in this\nparticular document d sub j. We'll going to compute the function\ng that would give us something like weight of this term, so we're computing the weight completion of\nmatching this query term in this document. And then, we're going to update\nthe score accumulator for this document and\nthis would allow us to add this to our accumulator that would\nincrementally compute function h. So this is basically a general\nway to allow pseudo computer or functions of this form by\nusing the inbound index. Note that we don't have to\nattach any of document and that didn't match any query term. Well, this is why it's fast, we only need to process the documents\nthat matched at least one query term. In the end, then we're going to adjust\nthe score the computer this function f sub a and then we can sort. So let's take a look\nat a specific example. In this case, let's assume the scoring\nfunction is a very simple one, it just takes the sum of t f, the role of\nt f, the count of a term in the document. This simplification would help\nshield the algorithm clearly. It's very easy to extend the computation\nto include other weights like the transformation of tf, or [INAUDIBLE]\nor IDF [INAUDIBLE]. So let's take a look at specific example,\nwhere the queries information security and it show some entries of\ninvert index on the right side. Information occurred in four documents and their frequencies are also there,\nsecurity occurred in three documents. So let's see how the arrows works, so\nfirst we iterate overall query terms and we fetch the first query then,\nwhat is that? That's information, right? And imagine we have all these\nscore accumulators who score the, scores for these documents. We can imagine there will be other but then they will only be\nallocated as needed. So before we do any waiting of terms, we don't even need a score of. That comes actually we have these score\naccumulators eventually allocating. So lets fetch the interest from\nthe entity [INAUDIBLE] for information, that the first one. So these four accumulators obviously\nwould be initialize as zeros. So, the first entry is d1 and 3, 3 is occurrences of\ninformation in this document. Since our scoring function assume that the\nscore is just a sum of these raw counts. We just need to add a 3 to the score\naccumulator to account for the increase of score due to matching\nthis term information, a document d1. And then, we go to the next entry,\nthat's d2 and 4 and then we add a 4 to the score\naccumulator of d2. Of course, at this point, that we will\nallocate the score accumulator as needed. And so at this point, we allocated\nthe d1 and d2, and the next one is d3, and we add one, we allocate another\nscore [INAUDIBLE] d3 and add one to it. And then finally,\nthe d4 gets a 5, because the term information occurred five\ntimes in this document. Okay, so this completes the processing of\nall the entries in the invert index for information. It processed all the contributions\nof matching information in this four documents. So now, our error will go to\nthe next that's security. So, we're going to fetch all\nthe inverted index entries for security. So, in this case, there are three entries, and\nwe're going to go through each of them. The first is d2 and 3 and that means security occur three\nhumps in d2 and what do we do? Well, we do exactly the same,\nas what we did for information. So, this time we're going to change the\nscore [INAUDIBLE] d2 since it's already allocated and\nwhat we do is we'll add 3 to the existing value which is a 4, so\nwe now get a 7 for d2. D2 score is increased because the match\nthat falls the information and the security. Go to the next entry, that's d4 and\n1, so we would the score for d4 and again, we add 1 to d4 so\nd4 goes from 5 to 6. Finally, we process d5 and a 3. Since we have not yet allocated a score\naccumulated for d5, at this point, we're going to allocate 1 for d5,\nand we're going to add a 3 to it. So, those scores, of the last rule,\nare the final scores for these documents. If our scoring function is just\na simple some of TF values. Now, what if we, actually,\nwould like to do form addition? Well, we going to do the [INAUDIBLE]\nat this point, for each document. So, to summarize this,\nall right, so you can see, we first process the information\ndetermine query term information and we processed all the entries\nin what index for this term. Then we process the security,\nall right, its worst think about what should be the order of processing\nhere when we can see the query terms? It might make a difference especially\nif we don't want to keep all the score accumulators. Let's say, we only want to keep\nthe most promising score accumulators. What do you think would be\na good order to go through? Would you process a common term first or\nwould you process a rare term first? The answers is we just go to who\nshould process the rare term first. A rare term would match a few documents,\nand then the score contribution would be higher,\nbecause the ideal value would be higher. And then, it allows us to attach\nthe most diplomacy documents first. So, it helps pruning\nsome non-promising ones, if we don't need so\nmany documents to be returned to the user. So those are all heuristics for\nfurther improving the accuracy. Here you can also see how we can\nincorporate the idea of waiting, right? So they can [INAUDIBLE] when we\nincorporate [INAUDIBLE] when we process each query time. When we fetch the inverted index we\ncan fetch the document frequency and then we can compute IDF. Or maybe perhaps the IDF value\nhas already been precomputed when we indexed the documents. At that time, we already computed\nthe IDF value that we can just fetch it, so all these can be done at this time. So that would mean when we process\nall the entries for information, these words would be adjusted by the same\nIDF, which is IDF for information. So this is the basic idea of using\ninverted index for fast research and it works well for all kinds of\nformulas that are of the general form. And this generally,\nthe general form covers actually most state of art retrieval functions. So there are some tricks to\nfurther improve the efficiency, some general techniques\nto encode the caching. This is we just store some results of\npopular queries, so that next time when you see the same query,\nyou simply return the stored results. Similarly, you can also slow the list\nof inverted index in the memory for a popular term. And if the query term is popular likely, you will soon need to factor the inverted\nindex for the same term again. So keeping it in the memory would help,\nand these are general techniques for improving efficiency. We can also keep only the most promising\naccumulators because a user generally doesn't want to examine so many documents. We only need to return high\nqualities subset of documents that likely are ranked on the top. For that purpose,\nwe can then prune the accumulators. We don't have to store\nall the accumulators. At some point, we just keep\nthe highest value accumulators. Another technique is to do parallel\nprocessing and that's needed for really process in such a large\ndata set like the web data set. And you scale up to\nthe Web-scale really to have the special techniques you\ndo parallel processing and to distribute the storage\nof files on machines. So here is a list of some text retrieval\ntoolkits, it's not a complete list. You can find more information\nat this URL on the bottom. And here, I listed your four here,\nLucene's one of the most popular toolkits that can support a lot of applications and\nit has very nice support for applications. You can use it to build a search\nengine application very quickly. The downside is that it's not\nthat easy to extend it, and the algorithms implemented they are also\nnot the most advanced algorithms. Lemur or Indri is another\ntoolkit that does not have such a nice support web\napplication as Lucene but it has many advanced search algorithms and\nit's also easy to extend. Terrier is yet another toolkit\nthat also has good support for application capability and\nsome advanced algorithms. So that's maybe in between Lemur or\nLucene, or maybe rather combining\nthe strands of both, so that's also useful tool kit. MeTA is a toolkit that we will use for the problem assignment and\nthis is a new toolkit that has a combination of both text retrieval\nalgorithms and text mining algorithms. And so talking models are implement they\nare a number of text analysis algorithms implemented in the toolkit as\nwell as basic search algorithms. So to summarize all the discussion\nabout the System Implementation, here are the major takeaway points. Inverted index is the primary data\nstructure for supporting a search engine and that's the key to enable\nfaster response to a user's query. And the basic idea is to preprocess\nthe data as much as we can, and we want to do compression\nwhen appropriate. So that we can save disk space and\nwe can speed up IO and processing of inverted index in general. We talked about how to construct the\ninvert index when the data can't fit into the memory. And then we talk about faster search using\nthat index basically, what's we exploit the invective index to accumulate a scores\nfor documents [INAUDIBLE] algorithm. And we exploit the Zipf's law to\navoid the touching many documents that don't match any query term and this algorithm can actually support\na wide range of ranking algorithms. So these basic techniques\nhave great potential for further scaling up using distributed file\nsystem, parallel processing, and caching. Here are two additional readings you\ncan take a look, if you have time and you are interested in\nlearning more about this. The first one is a classical\ntextbook on the efficiency o inverted index and\nthe compression techniques. And how to,\nin general feel that the efficient any inputs of the space,\noverhead and speed. The second one is a newer textbook that\nhas a nice discussion of implementing and evaluating search engines. [MUSIC]",
 "01_lesson-3-1-evaluation-of-tr-systems.en.txt": "[MUSIC] This lecture is about Evaluation of\nText Retrieval Systems In the previous lectures, we have talked about\nthe a number of Text Retrieval Methods, different kinds of ranking functions. But how do we know which\none works the best? In order to answer this question, we have to compare them and that means we\nhave to evaluate these retrieval methods. So this is the main topic of this lecture. First, lets think about why\ndo we have to do evaluation? I already give one reason. That is, we have to use evaluation\nto figure out which retrieval method works better. Now this is very important for\nadvancing our knowledge. Otherwise, we wouldn't know whether a new\nidea works better than an old idea. In the beginning of this course, we talked\nabout the problem of text retrieval. We compare it with data base retrieval. There we mentioned that text retrieval\nis an empirically defined problem. So evaluation must rely on users. Which system works better,\nwould have to be judged by our users. So, this becomes a very\nchallenging problem because how can we get users\ninvolved in the evaluation? How can we do a fair comparison\nof different method? So just go back to the reasons for\nevaluation. I listed two reasons here. The second reason, is basically what I\njust said, but there is also another reason which is to assess the actual\nutility of a Text Regional system. Imagine you're building your\nown such annual applications, it would be interesting knowing how well\nyour search engine works for your users. So in this case, matches must reflect the utility to\nthe actual users in real occasion. And typically, this has to be\ndone by using user starters and using the real search engine. In the second case, or the second reason, the measures actually all need to collated\nwith the utility to actually use this. Thus, they don't have to accurately\nreflect the exact utility to users. So the measure only needs to be good\nenough to tell which method works better. And this is usually done\nthrough a test collection. And this is the main idea that we'll\nbe talking about in this course. This has been very important for\ncomparing different algorithms and for improving search\nengine system in general. So let's talk about what to measure. There are many aspects of searching\nthat we can measure, we can evaluate. And here,\nI listed the three major aspects. One, is effectiveness or accuracy. How accurate are the search results? In this case, we're measuring a system's\ncapability of ranking relevant documents on top of non relevant ones. The second, is efficiency. How quickly can you get the results? How much computing resources\nare needed to answer a query? In this case, we need to measure the space\nand time overhead of the system. The third aspect is usability. Basically the question is,\nhow useful is a system for new user tasks. Here, obviously, interfaces and many other things also important and\nwould typically have to do user studies. Now in this course, we're going to\ntalk mostly about effectiveness and accuracy measures. Because the efficiency and usability dimensions are not\nreally unique to search engines. And so they are needed for\nwithout any other software systems. And there is also good coverage\nof such and other causes. But how to evaluate search\nengine's quality or accuracy is something unique to text retrieval and\nwe're going to talk a lot about this. The main idea that people have proposed\nbefore using a test set to evaluate the text retrieval algorithm is called\nthe Cranfield Evaluation Methodology. This one actually was developed\na long time ago, developed in 1960s. It's a methodology for\nlaboratory test of system components. Its sampling methodology that has\nbeen very useful, not just for search engine evaluation. But also for evaluating virtually\nall kinds of empirical tasks, and for example in natural language processing\nor in other fields where the problem is empirical to find, we typically\nwould need to use such a methodology. And today with the big data challenging with the use of machine\nlearning everywhere. This methodology has been very popular,\nbut it was first developed for a search engine application in the 1960s. So the basic idea of this approach is\nto build a reusable test collection and define measures. Once such a test collection is built,\nit can be used again and again to test different algorithms. And we're going to define measures\nthat allow you to quantify performance of a system and algorithm. So how exactly will this work? Well we can do have a sample collection of\ndocuments and this is adjusted to simulate the real document collection\nin the search application. We're going to also have a sample\nset of queries, or topics. This is a little simulator\nthat uses queries. Then, we'll have to have\nthose relevance judgments. These are judgments of which documents\nshould be returned for which queries. Ideally, they have to be made by\nusers who formulated the queries. Because those are the people that know\nexactly what documents would be used for. And finally, we have to have matches for quantify how well our system's result\nmatches the ideal ranked list. That would be constructed base\non user's relevance judgements. So this methodology is very useful for\nstarting retrieval algorithms, because the test can be reused many times. And it will also provide a fair\ncomparison for all the methods. We have the same criteria or same dataset to be used to\ncompare different algorithms. This allows us to compare\na new algorithm with an old algorithm that was divided many\nyears ago, by using the same standard. So this is the illustration of this works,\nso as I said, we need our queries that are showing here. We have Q1, Q2 etc. We also need the documents and\nthat's called the document caching and on the right side you will see\nwe need relevance judgments. These are basically the binary judgments\nof documents with respect to a query. So for example,\nD1 is judged as being relevant to Q1, D2 is judged as being relevant as well,\nand D3 is judged as not relevant. And the Q1 etc. These will be created by users. Once we have these, and\nwe basically have a test collection. And then if you have two systems,\nyou want to compare them, then you can just run each\nsystem on these queries and the documents and\neach system would then return results. Let's say if the queries Q1 and\nthen we would have the results here. Here I show R sub A as\nthe results from system A. So this is, remember we talked about task of computing approximation\nof the relevant document set. R sub A is system A's approximation here. And R sub B is system B's\napproximation of relevant documents. Now, let's take a look at these results. So which is better? Now imagine if a user,\nwhich one would you like? Now let's take a look at the both results. And there are some differences and there are some documents that\nare returned by both systems. But if you look at the results,\nyou will feel that maybe A is better in the sense that we don't\nhave many number element documents. And among the three documents returned,\nthe two of them are relevant. So that's good, it's precise. On the other hand one council\nsay maybe B is better, because we've got all of\nthem in the documents. We've got three instead of two. So which one is better and\nhow do we quantify this? Well, obviously this question\nhighly depends on a user's task. It depends on users as well. You might even imagine for\nsome users may be system A is better. If the user is not interested in\ngetting all the random documents. Right, in this case the user doesn't\nhave to read a million users will see most of the relevant documents. On the other hand,\none can also imagine the user might need to have as many random\ndocuments as possible. For example, if you're doing a literature\nsurvey you might be in the sigma category, and you might find that\nsystem B is better. So in the case, we will have to also\ndefine measures that will quantify them. And we might need it to define multiple\nmeasures because users have different perspectives of looking at the results. [MUSIC]",
 "02_lesson-3-2-evaluation-of-tr-systems-basic-measures.en.txt": "[SOUND]\nThis lecture is about the basic measures for\nevaluation of text retrieval systems. In this lecture,\nwe're going to discuss how we design basic measures to quantitatively\ncompare two retrieval systems. This is a slide that you have seen\nearlier in the lecture where we talked about the Granville\nevaluation methodology. We can have a test faction that consists\nof queries, documents, and [INAUDIBLE]. We can then run two systems on these\ndata sets to contradict the evaluator. Their performance. And we raise the question,\nabout which set of results is better. Is system A better or is system B better? So let's now talk about how to\naccurately quantify their performance. Suppose we have a total of 10 relevant\ndocuments in the collection for this query. Now, the relevant judgments show on\nthe right in [INAUDIBLE] obviously. And we have only seen 3 [INAUDIBLE] there,\n[INAUDIBLE] documents there. But, we can imagine there are other Random\ndocuments in judging for this query. So now, intuitively,\nwe thought that system A is better because it\ndid not have much noise. And in particular we have seen\nthat among the three results, two of them are relevant but in system B, we have five results and\nonly three of them are relevant. So intuitively it looks like\nsystem A is more accurate. And this infusion can be captured\nby a matching holder position, where we simply compute to what extent\nall the retrieval results are relevant. If you have 100% position, that would mean that all\nthe retrieval documents are relevant. So in this case system A has\na position of two out of three System B has some\nsweet hold of 5 and this shows that system\nA is better frequency. But we also talked about System B\nmight be prefered by some other units would like to retrieve as many\nrandom documents as possible. So in that case we'll have to compare\nthe number of relevant documents that they retrieve and\nthere's another method called recall. This method uses the completeness\nof coverage of random documents In your retrieval result. So we just assume that there are ten\nrelevant documents in the collection. And here we've got two of them,\nin system A. So the recall is 2 out of 10. Whereas System B has called a 3,\nso it's a 3 out of 10. Now we can see by recall\nsystem B is better. And these two measures turn out to\nbe the very basic of measures for evaluating search engine. And they are very important because\nthey are also widely used in many other test evaluation problems. For example, if you look at\nthe applications of machine learning, you tend to see precision recall numbers\nbeing reported and for all kinds of tasks. Okay so, now let's define these\ntwo measures more precisely. And these measures are to evaluate a set\nof retrieved documents, so that means we are considering that approximation\nof the set of relevant documents. We can distinguish 4 cases depending\non the situation of the documents. A document can be retrieved or\nnot retrieved, right? Because we are talking\nabout a set of results. A document can be also relevant or not relevant depending on whether the user\nthinks this is a useful document. So we can now have counts of documents in. Each of the four categories again\nhave a represent the number of documents that have been retrieved and\nrelevant. B for documents that are not retrieved but\nrather etc. No with this table then\nwe can define precision. As the ratio of the relevant retrieved documents A to the total\nof relevant retrieved documents. So, this is just A divided\nby The sum of a and c. The sum of this column. Singularly recall is defined by\ndividing a by the sum of a and b. So that's again to divide a by. The sum of the row instead of the column. All right, so we can see precision and\nrecall is all focused on looking at the a, that's the number of\nretrieved relevant documents. But we're going to use\ndifferent denominators. Okay, so what would be an ideal result. Well, you can easily see being\nthe ideal case would have precision and recall oil to be 1.0. That means We have got 1% of\nall the Relevant documents in our results, and all of the results\nthat we returned all Relevant. At least there's no single\nNot Relevant document returned. In reality, however, high recall tends\nto be associated with low precision. And you can imagine why that's the case. As you go down the to try to get as\nmany random documents as possible, you tend to encounter a lot of documents,\nso the precision has to go down. Note that this set can also\nbe defined by a cut off. In the rest of this, that's why although\nthese two measures are defined for retrieve the documents, they are actually\nvery useful for evaluating a rank list. They are the fundamental measures in\ntask retrieval and many other tasks. We often are interested in The precision\nat ten documents for web search. This means we look at how many documents among the top ten results\nare actually relevant. Now, this is a very meaningful measure, because it tells us how many relevant\ndocuments a user can expect to see On the first page of where they\ntypically show ten results. So precision and recall\nare the basic matches and we need to use them to further evaluate a search\nengine, but they are the Building blocks. We just said that there tends to be\na trailoff between precision and recall, so naturally it would be\ninteresting to combine them. And here's one method that's often used,\ncalled F-measure And it's a [INAUDIBLE] mean of precision and\nrecall as defined on this slide. So, you can see at first, compute the. Inverse of R and P here,\nand then it would interpret the 2 by using coefficients\ndepending on parameter beta. And after some transformation you can\neasily see it would be of this form. And in any case it just becomes\nan agent of precision and recall, and beta is a parameter,\nthat's often set to 1. It can control the emphasis\non precision or recall always set beta to 1 We end up having a special\ncase of F-Measure, often called F1. This is a popular measure that's often\nused as a combined precision and recall. And the formula looks very simple. It's just this, here. Now it's easy to see that if\nyou have a Larger precision, or larger recall than f\nmeasure would be high. But, what's interesting is that\nthe trade off between precision and recall is captured\nan interesting way in f1. So, in order to understand that, we can first look at the natural\nWhy not just the combining and using the symbol arithmetically\nas efficient here? That would be likely the most natural way\nof combining them So what do you think? If you want to think more,\nyou can pause the video. So why is this not as good as F1? Or what's the problem with this? Now, if you think about\nthe arithmetic mean, you can see this is\nthe sum of multiple terms. In this case,\nit's the sum of precision and recall. In the case of a sum, the total value\ntends to be dominated by the large values. that means if you have a very high P or\nvery high R then you really don't care about whether the other value\nis low so the whole sum would be high. Now this is not desirable because one\ncan easily have a perfect recall. We have perfect recall easily. Can we imagine how? It's probably very easy to\nimagine that we simply retrieve all the documents in the collection and\nthen we have a perfect recall. And this will give us 0.5 as the average. But such results are clearly not\nvery useful for the users even though the average using this\nformula would be relevantly high. In contrast you can see F 1 would\nreward a case where precision and recall are roughly That seminar, so it would a case where you had\nextremely high value for one of them. So this means f one encodes\na different trade off between that. Now this example shows\nactually a very important. Methodology here. But when you try to solve a problem you\nmight naturally think of one solution, let's say in this it's\nthis error mechanism. But it's important not to\nsettle on this source. It's important to think whether you\nhave other ways to combine that. And once you think about the multiple\nvariance It's important to analyze their difference, and then think about\nwhich one makes more sense. In this case, if you think more carefully, you will think that F1\nprobably makes more sense. Than the simple. Although in other cases there\nmay be different results. But in this case the seems not reasonable. But if you don't pay attention\nto these subtle differences you might just take a easy way to\ncombine them and then go ahead with it. And here later, you will find that,\nthe measure doesn't seem to work well. All right. So this methodology is actually very\nimportant in general, in solving problems. Try to think about the best solution. Try to understand the problem very well,\nand then know why you needed this measure, and why\nyou need to combine precision and recall. And then use that to guide you in\nfinding a good way to solve the problem. To summarize, we talked about\nprecision which addresses the question are there retrievable\nresults all relevant? We also talk about the Recall. Which addresses the question, have all of\nthe relevant documents been retrieved. These two, are the two,\nbasic matches in text and retrieval in. They are used for\nmany other tasks, as well. We talk about F measure as a way to\ncombine Precision Precision and recall. We also talked about the tradeoff\nbetween precision and recall. And this turns out to depend\non the user's search tasks and we'll discuss this point\nmore in a later lecture. [MUSIC]",
 "03_lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1.en.txt": "[MUSIC] This lecture is about,\nhow we can evaluate a ranked list? In this lecture, we will continue\nthe discussion of evaluation. In particular, we are going to look at, how we can\nevaluate a ranked list of results. In the previous lecture,\nwe talked about, precision-recall. These are the two basic measures for, quantitatively measuring\nthe performance of a search result. But, as we talked about, ranking, before, we framed that the text of retrieval\nproblem, as a ranking problem. So, we also need to evaluate the,\nthe quality of a ranked list. How can we use precision-recall\nto evaluate, a ranked list? Well, naturally, we have to look after the\nprecision-recall at different, cut-offs. Because in the end, the approximation\nof relevant documents, set, given by a ranked list, is determined\nby where the user stops browsing. Right?\nIf we assume the user, securely browses, the list of results, the user would,\nstop at some point, and that point would determine the set. And then,\nthat's the most important, cut-off, that we have to consider,\nwhen we compute the precision-recall. Without knowing where\nexactly user would stop, then we have to consider, all\nthe positions where the user could stop. So, let's look at these positions. Look at this slide, and\nthen, let's look at the, what if the user stops at the,\nthe first document? What's the precision-recall at this point? What do you think? Well, it's easy to see, that this document\nis So, the precision is one out of one. We have, got one document,\nand that's relevent. What about the recall? Well, note that, we're assuming that,\nthere are ten relevant documents, for this query in the collection,\nso, it's one out of ten. What if the user stops\nat the second position? Top two. Well, the precision is the same,\n100%, two out of two. And, the record is two out of ten. What if the user stops\nat the third position? Well, this is interesting,\nbecause in this case, we have not got any, additional relevant document,\nso, the record does not change. But the precision is lower,\nbecause we've got number [INAUDIBLE] so, what's exactly the precision? Well, it's two out of three, right? And, recall is the same, two out of ten. So, when would see another point,\nwhere the recall would be different? Now, if you look down the list,\nwell, it won't happen until, we have, seeing another relevant document. In this case D5, at that point, the,\nthe recall is increased through three out of ten, and,\nthe precision is three out of five. So, you can see, if we keep doing this,\nwe can also get to D8. And then, we will have\na precision of four out of eight, because there are eight documents,\nand four of them are relevant. And, the recall is a four out of ten. Now, when can we get,\na recall of five out of ten? Well, in this list, we don't have it,\nso, we have to go down on the list. We don't know, where it is? But, as convenience, we often assume that,\nthe precision is zero, at all the, the othe,\nthe precision are zero at all the other levels of recall,\nthat are beyond the search results. So, of course,\nthis is a pessimistic assumption, the actual position would be higher,\nbut we make, make this assumption, in order to, have an easy way to, compute another measure called Average\nPrecision, that we will discuss later. Now, I should also say, now, here you see, we make these assumptions that\nare clearly not, accurate. But, this is okay, for\nthe purpose of comparing to, text methods. And, this is for the relative comparison,\nso, it's okay, if the actual measure, or actual, actual number deviates\na little bit, from the true number. As long as the deviation, is not biased toward any particular\nretrieval method, we are okay. We can still,\naccurately tell which method works better. And, this is important point,\nto keep in mind. When you compare different algorithms, the key's to avoid any\nbias toward each method. And, as long as, you can avoid that. It's okay, for you to do transformation\nof these measures anyway, so, you can preserve the order. Okay, so, we'll just talk about, we can get a lot of precision-recall\nnumbers at different positions. So, now, you can imagine,\nwe can plot a curve. And, this just shows on the,\nx-axis, we show the recalls. And, on the y-axis, we show the precision. So, the precision line was marked as .1,\n.2, .3, and, 1.0. Right?\nSo, this is, the different, levels of recall. And,, the y-axis also has,\ndifferent amounts, that's for precision. So, we plot the, these, precision-recall\nnumbers, that we have got, as points on this picture. Now, we can further, and\nlink these points to form a curve. As you'll see, we assumed all the other, precision\nas the high-level recalls, be zero. And, that's why, they are down here,\nso, they are all zero. And this, the actual curve probably will\nbe something like this, but, as we just discussed, it, it doesn't matter that\nmuch, for comparing two methods. because this would be,\nunderestimated, for all the method. Okay, so, now that we,\nhave this precision-recall curve, how can we compare ranked to back list? All right, so, that means,\nwe have to compare two PR curves. And here, we show, two cases. Where system A is showing red,\nsystem B is showing blue, there's crosses. All right, so, which one is better? I hope you can see,\nwhere system A is clearly better. Why?\nBecause, for the same level of recall, see same level of recall here,\nand you can see, the precision point by system A is better,\nsystem B. So, there's no question. In here, you can imagine, what does the\ncode look like, for ideal search system? Well, it has to have perfect,\nprecision at all the recall points, so, it has to be this line. That would be the ideal system. In general, the higher the curve is,\nthe better, right? The problem is that,\nwe might see a case like this. This actually happens often. Like, the two curves cross each other. Now, in this case, which one is better? What do you think? Now, this is a real problem,\nthat you actually, might have face. Suppose, you build a search engine,\nand you have a old algorithm, that's shown here in blue, or system B. And, you have come up with a new idea. And, you test it. And, the results are shown in red,\ncurve A. Now, your question is, is your new\nmethod better than the old method? Or more, practically,\ndo you have to replace the algorithm that you're already using, your, in your search\nengine, with another, new algorithm? So, should we use system,\nmethod A, to replace method B? This is going to be a real decision,\nthat you to have to make. If you make the replacement, the search\nengine would behave like system A here, whereas, if you don't do that,\nit will be like a system B. So, what do you do? Now, if you want to spend more time\nto think about this, pause the video. And, it's actually very\nuseful to think about that. As I said, it's a real decision that you\nhave to make, if you are building your own search engine, or if you're working, for\na company that, cares about the search. Now, if you have thought about this for a moment, you might realize that,\nwell, in this case, it's hard to say. Now, some users might like a system A,\nsome users might like, like system B. So, what's the difference here? Well, the difference is just that,\nyou know, in the, low level of recall,\nin this region, system B is better. There's a higher precision. But in high recall region,\nsystem A is better. Now, so, that also means,\nit depends on whether the user cares about the high recall, or\nlow recall, but high precision. You can imagine, if someone is just going\nto check out, what's happening today, and want to find out something\nrelevant in the news. Well, which one is better? What do you think? In this case, clearly, system B is better, because the user is unlikely\nexamining a lot of results. The user doesn't care about high recall. On the other hand,\nif you think about a case, where a user is doing you are,\nstarting a problem. You want to find, whether your idea ha,\nhas been started before. In that case, you emphasize high recall. So, you want to see,\nas many relevant documents as possible. Therefore, you might, favor, system A. So, that means, which one is better? That actually depends on users,\nand more precisely, users task. So, this means, you may not necessarily\nbe able to come up with one number, that would accurately\ndepict the performance. You have to look at the overall picture. Yet, as I said, when you have\na practical decision to make, whether you replace ours with another, then you may have to actually come up with\na single number, to quantify each, method. Or, when we compare many different\nmethods in research, ideally, we have one number to compare, them with, so, that\nwe can easily make a lot of comparisons. So, for all these reasons, it is desirable\nto have one, single number to match it up. So, how do we do that? And, that,\nneeds a number to summarize the range. So, here again it's\nthe precision-recall curve, right? And, one way to summarize\nthis whole ranked, list, for this whole curve,\nis look at the area underneath the curve. Right?\nSo, this is one way to measure that. There are other ways to measure that,\nbut, it just turns out that,, this particular way of matching\nit has been very, popular, and has been used, since a long time ago for\ntext And, this is, basically, in this way, and\nit's called the average precision. Basically, we're going to take a, a look\nat the, every different, recall point. And then, look out for the precision. So, we know, you know,\nthis is one precision. And, this is another,\nwith, different recall. Now, this, we don't count to this one, because the recall level is the same,\nand we're going to, look at the, this number, and that's precision at\na different recall level et cetera. So, we have all these, you know, added up. These are the precisions\nat the different points, corresponding to retrieving the first\nrelevant document, the second, and then, the third, that follows, et cetera. Now, we missed the many relevant\ndocuments, so, in all of those cases, we just, assume,\nthat they have zero precisions. And then, finally, we take the average. So, we divide it by ten, and which is the total number of relevant\ndocuments in the collection. Note that here,\nwe're not dividing this sum by four. Which is a number retrieved\nrelevant documents. Now, imagine, if I divide by four,\nwhat would happen? Now, think about this, for a moment. It's a common mistake that people,\nsometimes, overlook. Right, so, if we, we divide this by four,\nit's actually not very good. In fact, that you are favoring a system,\nthat would retrieve very few random documents, as in that case,\nthe denominator would be very small. So, this would be, not a good matching. So, note that this denomina,\ndenominator is ten, the total number of relevant documents. And, this will basically ,compute\nthe area, and the needs occur. And, this is the standard method,\nused for evaluating a ranked list. Note that, it actually combines\nrecall and, precision. But first, you know, we have\nprecision numbers here, but secondly, we also consider recall, because if missed\nmany, there would be many zeros here. All right, so,\nit combines precision and recall. And furthermore, you can see this\nmeasure is sensitive to a small change of a position of a relevant document. Let's say, if I move this relevant\ndocument up a little bit, now, it would increase this means,\nthis average precision. Whereas, if I move any relevant document,\ndown, let's say, I move this relevant document down, then it would decrease,\nuh,the average precision. So, this is a very good, because it's a very sensitive to\nthe ranking of every relevant document. It can tell, small differences\nbetween two ranked lists. And, that is what we want, sometimes one algorithm only works\nslightly better than another. And, we want to see this difference. In contrast, if we look at\nthe precision at the ten documents. If we look at this, this whole set, well, what, what's the precision,\nwhat do you think? Well, it's easy to see,\nthat's a four out of ten, right? So, that precision is very meaningful,\nbecause it tells us, what user would see? So, that's pretty useful, right? So, it's a meaningful measure,\nfrom a users perspective. But, if we use this measure to\ncompare systems, it wouldn't be good, because it wouldn't be sensitive to where\nthese four relevant documents are ranked. If I move them around the precision\nat ten, still, the same. Right.\nSo, this is not a good measure for\ncomparing different algorithms. In contrast, the average precision\nis a much better measure. It can tell the difference of, different, a difference in ranked list in,\nsubtle ways. [MUSIC]",
 "04_lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2.en.txt": "[SOUND] So average precision is computer for just one. one query.\nBut we generally experiment with many different queries and this is to\navoid the variance across queries. Depending on the queries you use you\nmight make different conclusions. Right, so\nit's better then using more queries. If you use more queries then,\nyou will also have to take the average of the average\nprecision over all these queries. So how can we do that? Well, you can naturally. Think of just doing arithmetic mean as we always tend to, to think in, in this way. So, this would give us what's called\na \"Mean Average Position\", or MAP. In this case, we take arithmetic mean of all the average\nprecisions over several queries or topics. But as I just mentioned in\nanother lecture, is this good? We call that. We talked about the different ways\nof combining precision and recall. And we conclude that the arithmetic\nmean is not as good as the MAP measure. But here it's the same. We can also think about the alternative\nways of aggregating the numbers. Don't just automatically assume that,\nthough. Let's just also take the arithmetic\nmean of the average position over these queries. Let's think about what's\nthe best way of aggregating them. If you think about the different ways,\nnaturally you will, probably be able to think about\nanother way, which is geometric mean. And we call this kind of average a gMAP. This is another way. So now, once you think about\nthe two different ways. Of doing the same thing. The natural question to ask is,\nwhich one is better? So. So, do you use MAP or gMAP? Again, that's important question. Imagine you are again\ntesting a new algorithm in, by comparing the ways your old\nalgorithms made the search engine. Now you tested multiple topics. Now you've got the average precision for\nthese topics. Now you are thinking of looking\nat the overall performance. You have to take the average. But which, which strategy would you use? Now first, you should also think about the\nquestion, well did it make a difference? Can you think of scenarios where using\none of them would make a difference? That is they would give different\nrankings of those methods. And that also means depending on\nthe way you average or detect the. Average of these average positions. You will get different conclusions. This makes the question\nbecoming even more important. Right?\nSo, which one would you use? Well again, if you look at\nthe difference between these. Different ways of aggregating\nthe average position. You'll realize in arithmetic mean,\nthe sum is dominating by large values. So what does large value here mean? It means the query is relatively easy. You can have a high pres,\naverage position. Whereas gMAP tends to be\naffected more by low values. And those are the queries that\ndon't have good performance. The average precision is low. So if you think about the,\nimproving the search engine for those difficult queries,\nthen gMAP would be preferred, right? On the other hand, if you just want to. Have improved a lot. Over all the kinds of queries or\nparticular popular queries that might be easy and you want to make the perfect and\nmaybe MAP would be then preferred. So again, the answer depends on\nyour users, your users tasks and their pref, their preferences. So the point that here is to think\nabout the multiple ways to solve the same problem, and then compare them,\nand think carefully about the differences. And which one makes more sense. Often, when one of them might\nmake sense in one situation and another might make more sense\nin a different situation. So it's important to pick out under\nwhat situations one is preferred. As a special case of the mean average\nposition, we can also think about the case where there was precisely\none rank in the document. And this happens often, for example,\nin what's called a known item search. Where you know a target page, let's\nsay you have to find Amazon, homepage. You have one relevant document there,\nand you hope to find it. That's call a \"known item search\". In that case,\nthere's precisely one relevant document. Or in another application,\nlike a question and answering, maybe there's only one answer. Are there. So if you rank the answers, then your goal is to rank that one\nparticular answer on top, right? So in this case, you can easily\nverify the average position, will basically boil down\nto reciprocal rank. That is, 1 over r where r is the rank\nposition of that single relevant document. So if that document is ranked\non the very top or is 1, and then it's 1 for reciprocal rank. If it's ranked at the,\nthe second, then it's 1 over 2. Et cetera. And then we can also take a, a average\nof all these average precision or reciprocal rank over a set of topics, and that would give us something\ncalled a mean reciprocal rank. It's a very popular measure. For no item search or, you know, an problem where you have\njust one relevant item. Now again here, you can see this\nr actually is meaningful here. And this r is basically\nindicating how much effort a user would have to make in order\nto find that relevant document. If it's ranked on the top it's low effort\nthat you have to make, or little effort. But if it's ranked at 100\nthen you actually have to, read presumably 100 documents\nin order to find it. So, in this sense r is also a meaningful\nmeasure and the reciprocal rank will take the reciprocal of r,\ninstead of using r directly. So my natural question here\nis why not simply using r? I imagine if you were to design\na ratio to, measure the performance of a random system,\nwhen there is only one relevant item. You might have thought about\nusing r directly as the measure. After all,\nthat measures the user's effort, right? But, think about if you take a average\nof this over a large number of topics. Again it would make a difference. Right, for one single topic, using r or using 1 over r wouldn't\nmake any difference. It's the same. Larger r with corresponds\nto a small 1 over r, right? But the difference would only show when,\nshow up when you have many topics. So again, think about the average of Mean\nReciprocal Rank versus average of just r. What's the difference? Do you see any difference? And would, would this difference\nchange the oath of systems. In our conclusion. And this, it turns out that,\nthere is actually a big difference, and if you think about it, if you want to\nthink about it and then, yourself, then pause the video. Basically, the difference is,\nif you take some of our directory, then. Again it will be dominated\nby large values of r. So what are those values? Those are basically large values that\nindicate that lower ranked results. That means the relevant items\nrank very low down on the list. And the sum that's also the average\nthat would then be dominated by. Where those relevant documents\nare ranked in, in ,in, in the lower portion of the ranked. But from a users perspective we care\nmore about the highly ranked documents. So by taking this transformation\nby using reciprocal rank. Here we emphasize more on\nthe difference on the top. You know, think about\nthe difference between 1 and the 2, it would make a big difference, in 1 over\nr, but think about the 100, and 1, and where and when won't make much\ndifference if you use this. But if you use this there will\nbe a big difference in 100 and let's say 1,000, right. So this is not the desirable. On the other hand, a 1 and\n2 won't make much difference. So this is yet another case where there\nmay be multiple choices of doing the same thing and then you need to figure\nout which one makes more sense. So to summarize,\nwe showed that the precision-recall curve. Can characterize the overall\naccuracy of a ranked list. And we emphasized that the actual\nutility of a ranked list depends on how many top ranked results\na user would actually examine. Some users will examine more. Than others. An average person uses a standard measure\nfor comparing two ranking methods. It combines precision and recall and it's sensitive to the rank\nof every random document. [MUSIC]",
 "05_lesson-3-5-evaluation-of-tr-systems-multi-level-judgements.en.txt": "[MUSIC] This lecture is about how to evaluate\nthe text retrieval system when we have multiple levels of judgements. In this lecture, we will continue\nthe discussion of evaluation. We're going to look at how to\nevaluate a text retrieval system, when we have multiple\nlevels of judgements. So far we have talked about\nthe binary judgements, that means a document is judged as\nbeing relevant or not relevant. But earlier, we also talk about\nthe relevance as a medal of degrees. So we often can distinguish\nvery high relevant documents, those are very useful documents,\nfrom moderately relevant documents. They are okay, they are useful perhaps. And further from now, we're adding\nthe documents, those are not useful. So imagine you can have ratings for\nthese pages. Then, you would have\nmultiple levels of ratings. For example, here I show example of three\nlevels, 3 for relevant, sorry 3 for very relevant, 2 for marginally relevant,\nand 1 for non-relevant. Now, how do we evaluate the search\nengine system using these judgements? Obvious that the map doesn't work, average\nof precision doesn't work, precision, and recall doesn't work,\nbecause they rely on binary judgements. So let's look at some top ranked\nresults when using these judgements. Imagine the user would be mostly\ncare about the top ten results here. And we marked the rating levels,\nor relevance levels, for these documents as shown here,\n3, 2, 1, 1, 3, etcetera. And we call these gain. And the reason why we call it\nthe gain is because the measure that we are infusing is called the NDCG\nnormalized or accumulated gain. So this gain, basically,\ncan measure how much a gain of random information a user can obtain by\nlooking at each document, right? So looking at the first document,\nthe user can gain 3 points. Looking at the non-relevant document\nuser would only gain 1 point. Looking at the moderator or\nmarginally relevant, document the user would get 2 points,\netcetera. So, this gain to each of the measures is\na utility of the document from a user's perspective. Of course, if we assume the user\nstops at the 10 documents and we're looking at the cutoff at 10,\nwe can look at the total gain of the user. And what's that? Well, that's simply the sum of these,\nand we call it the Cumulative Gain. So if the user stops after the position 1,\nthat's just a 3. If the user looks at another document,\nthat's a 3+2. If the user looks at the more documents,\nthen the cumulative gain is more. Of course this is at the cost of\nspending more time to examine the list. So cumulative gain gives\nus some idea about how much total gain the user would have if\nthe user examines all these documents. Now, in NDCG, we also have another letter\nhere, D, discounted cumulative gain. So, why do we want to do discounting? Well, if you look at this cumulative gain,\nthere is one deficiency, which is it did not consider the rank\nposition of these documents. So for example, looking at this sum here, and we only know there is 1\nhighly relevant document, 1 marginally relevant document,\n2 non-relevant documents. We don't really care\nwhere they are ranked. Ideally, we want these two to be ranked\non the top which is the case here. But how can we capture that intuition? Well we have to say, well this is 3 here\nis not as good as this 3 on the top. And that means the contribution\nof the gain from different positions has to be\nweighted by their position. And this is the idea of discounting,\nbasically. So we're going to to say, well, the first\none does not need to be discounted because the user can be assumed\nthat will always see this document. But the second one,\nthis one will be discounted a little bit because there's a small possibility\nthat the user wouldn't notice it. So we divide this gain by\na weight based on the position. So log of 2,\n2 is the rank position of this document. And when we go to the third position,\nwe discounted even more, because the normalizer is log of 3,\nand so on and so forth. So when we take such a sum that a lower\nranked document would not contribute that much as a highly ranked document. So that means if you, for example,\nswitch the position of this, let's say this position, and this one, and then\nyou would get more discount if you put, for example very relevant\ndocument here as opposed to here. Imagine if you put the 3 here,\nthen it would have to be discounted. So it's not as good as if\nyou we would put the 3 here. So this is the idea of discounting. Okay, so now at this point that we have\ngot a discounted cumulative gain for measuring the utility of this ranked\nlist with multiple levels of judgements. So are we happy with this? Well, we can use this to rank systems. Now, we still need to do a little bit more in order to make this measure\ncomparable across different topics. And this is the last step, and by the way,\nhere we just show the DCG at 10, so this is the total sum of DCG,\nall these 10 documents. So the last step is called N,\nnormalization. And if we do that,\nthen we'll get the normalized DCG. So how do we do that? Well, the idea here is we're\ngoing to normalize DCG by the ideal DCG at the same cutoff. What is the ideal DCG? Well, this is the DCG of an ideal ranking. So imagine if we have 9 documents in\nthe whole collection rated 3 here. And that means in total we\nhave 9 documents rated 3. Then our ideal rank lister would have put\nall these 9 documents on the very top. So all these would have to be 3 and\nthen this would be followed by a 2 here. Because that's the best we could\ndo after we have run out of the 3. But all these positions would be 3. Right? So this would our ideal ranked list. And then we had computed the DCG for\nthis ideal rank list. So this would be given by this\nformula that you see here. And so this ideal DCG would then\nbe used as the normalizer DCG. So here. And this idea of DCG would\nbe used as a normalizer. So you can imagine now,\nnormalization essentially is to compare the actual DCG with the best DCG\nyou can possibly get for this topic. Now why do we want to do this? Well, by doing this we'll map the DCG\nvalues into a range of 0 through 1. So the best value, or the highest value,\nfor every query would be 1. That's when your rank list is,\nin fact, the ideal list but otherwise, in general,\nyou will be lower than one. Now, what if we don't do that? Well, you can see, this transformation,\nor this normalization, doesn't really affect the relative\ncomparison of systems for just one topic, because this ideal\nDCG is the same for all the systems, so the ranking of systems based on\nonly DCG would be exactly the same as if you rank them based\non the normalized DCG. The difference however is\nwhen we have multiple topics. Because if we don't do normalization, different topics will have\ndifferent scales of DCG. For a topic like this one,\nwe have 9 highly relevant documents, the DCG can get really high,\nbut imagine in another case, there are only two very relevant documents\nin total in the whole collection. Then the highest DCG that\nany system could achieve for such a topic would not be very high. So again, we face the problem of\ndifferent scales of DCG values. When we take an average, we don't want the average to be\ndominated by those high values. Those are, again, easy queries. So, by doing the normalization,\nwe can have avoided the problem, making all the queries contribute\nto equal to the average. So, this is a idea of NDCG, it's used for measuring a rank list based on multiple\nlevel of relevance judgements. In a more general way this\nis basically a measure that can be applied to any ranked task\nwith multiple level of judgements. And the scale of the judgements\ncan be multiple, can be more than binary not only more than binary they\ncan be much multiple levels like 1, 0, 5 or\neven more depending on your application. And the main idea of this measure,\njust to summarize, is to measure the total utility\nof the top k documents. So you always choose a cutoff and\nthen you measure the total utility. And it would discount the contribution\nfrom a lowly ranked document. And then finally, it would do normalization to ensure comparability across queries. [MUSIC]",
 "06_lesson-3-6-evaluation-of-tr-systems-practical-issues.en.txt": "[SOUND]. This lecture is about some practical\nissues that you would have to address in evaluation of text retrieval systems. In this lecture, we will continue\nthe discussion of evaluation. We'll cover some practical\nissues that you have to solve in actual evaluation of\ntext retrieval systems. So, in order to create\nthe test collection, we have to create a set of queries. A set of documents and\na set of relevance judgments. It turns out that each is\nactually challenging to create. First, the documents and\nqueries must be representative. They must represent the real queries and\nreal documents that the users handle. And we also have to use many queries and many documents in order to\navoid a bias of conclusions. For the matching of relevant\ndocuments with the queries. We also need to ensure that there exists a\nlot of relevant documents for each query. If a query has only one, that's\na relevant option we can actually then. It's not very informative to\ncompare different methods using such a query because there's not\nthat much room for us to see difference. So ideally, there should be more\nrelevant documents in the clatch but yet the queries also should represent\nthe real queries that we care about. In terms of relevance judgments,\nthe challenge is to ensure complete judgments of all\nthe documents for all the queries. Yet, minimizing human and fault, because we have to use human\nlabor to label these documents. It's very labor intensive. And as a result, it's impossible to\nactually label all the documents for all the queries, especially considering\na giant data set like the web. So this is actually a major challenge,\nit's a very difficult challenge. For measures, it's also challenging,\nbecause we want measures that would accurately reflect\nthe perceived utility of users. We have to consider carefully\nwhat the users care about. And then design measures to measure that. If your measure is not\nmeasuring the right thing, then your conclusion would be misled. So it's very important. So we're going to talk about\na couple of issues here. One is the statistical significance test. And this also is a reason why\nwe have to use a lot of queries. And the question here is how sure can\nyou be that observe the difference doesn't simply result from\nthe particular queries you choose. So here are some sample results of\naverage position for System A and System B into different experiments. And you can see in the bottom,\nwe have mean average of position. So the mean, if you look at the mean\naverage of position, the mean average of positions are exactly the same\nin both experiments, right? So you can see this is 0.20,\nthis is 0.40 for System B. And again here it's also 0.20 and\n0.40, so they are identical. Yet, if you look at these exact average\npositions for different queries. If you look at these numbers in detail,\nyou would realize that in one case, you would feel that you can trust\nthe conclusion here given by the average. In the another case, in the other case,\nyou will feel that, well, I'm not sure. So, why don't you take a look at all these\nnumbers for a moment, pause the media. So, if you look at the average,\nthe mean average of position, we can easily, say that well,\nSystem B is better, right? So, after all it's 0.40 and this is twice as much as 0.20,\nso that's a better performance. But if you look at these two experiments,\nlook at the detailed results. You will see that, we've been more\nconfident to say that, in the case one, in experiment one. In this case. Because these numbers seem to be\nconsistently better for System B. Whereas in Experiment 2, we're not sure\nbecause looking at some results like this, after System A is better and\nthis is another case System A is better. But yet if we look at only average,\nSystem B is better. So, what do you think? How reliable is our conclusion,\nif we only look at the average? Now in this case, intuitively,\nwe feel Experiment 1 is more reliable. But how can we quantitate\nthe answer to this question? And this is why we need to do\nstatistical significance test. So, the idea of the statistical\nsignificance test is basically to assess the variants across\nthese different queries. If there is a big variance, that means the results could fluctuate\na lot according to different queries. Then we should believe that,\nunless you have used a lot of queries, the results might change if we\nuse another set of queries. Right, so this is then not so if you have c high variance\nthen it's not very reliable. So let's look at these results\nagain in the second case. So, here we show two different\nways to compare them. One is a sign test where\nwe just look at the sign. If System B is better than System A,\nwe have a plus sign. When System A is better we\nhave a minus sign, etc. Using this case, if you see this,\nwell, there are seven cases. We actually have four cases\nwhere System B is better. But three cases of System A is better,\nintuitively, this is almost like a random results,\nright? So if you just take a random\nsample of you flip seven coins and if you use plus to denote the head and\nminus to denote the tail and that could easily be the results of just\nrandomly flipping these seven coins. So, the fact that the average is\nlarger doesn't tell us anything. We can't reliably conclude that. And this can be quantitatively\nmeasured by a p value. And that basically means the probability that this result is\nin fact from a random fluctuation. In this case, probability is 1.0. It means it surely is\na random fluctuation. Now in Willcoxan test,\nit's a non-parametric test, and we would be not only\nlooking at the signs, we'll be also looking at\nthe magnitude of the difference. But we can draw a similar conclusion, where you say it's very\nlikely to be from random. To illustrate this, let's think\nabout that such a distribution. And this is called a now distribution. We assume that the mean is zero here. Lets say we started with\nassumption that there's no difference between the two systems. But we assume that because of random\nfluctuations depending on the queries, we might observe a difference. So the actual difference might\nbe on the left side here or on the right side here, right? So, and this curve kind of shows\nthe probability that we will actually observe values that\nare deviating from zero here. Now, so if we look at this picture then,\nwe see that if a difference is observed here, then the chance is very high that this is\nin fact a random observation, right? We can define a region of\nlikely observation because of random fluctuation and\nthis is that 95% of all the outcomes. And in this then the observed may\nstill be from random fluctuation. But if you observe a value in this\nregion or a difference on this side, then the difference is unlikely\nfrom random fluctuation. All right, so there's a very small\nprobability that you are observe such a difference just because\nof random fluctuation. So in that case, we can then conclude\nthe difference must be real. So System B is indeed better. So this is the idea of\nStatical Significance Test. The takeaway message here is that you\nhave to use many queries to avoid jumping into a conclusion. As in this case,\nto say System B is better. There are many different ways of doing\nthis Statistical Significance Test. So now, let's talk about the other\nproblem of making judgments and, as we said earlier,\nit's very hard to judge all the documents completely unless it's\na very small data set. So the question is,\nif we can afford judging all the documents in the collection,\nwhich is subset should we judge? And the solution here is Pooling. And this is a strategy that has been used\nin many cases to solve this problem. So the idea of Pooling is the following. We would first choose a diverse\nset of ranking methods. These are Text Retrieval systems. And we hope these methods can help us\nnominate like the relevant documents. So the goal is to pick out\nthe relevant documents. We want to make judgements on relevant\ndocuments because those are the most useful documents from users perspectives. So then we're going to have\neach to return top-K documents. The K can vary from systems. But the point is to ask them to suggest\nthe most likely relevant documents. And then we simply combine\nall these top-K sets to form a pool of documents for\nhuman assessors. To judge, so imagine you have many\nsystems each were ten k documents. We take the top-K documents,\nand we form a union. Now, of course, there are many\ndocuments that are duplicated because many systems might have retrieved\nthe same random documents. So there will be some duplicate documents. And there are also unique documents\nthat are only returned by one system. So the idea of having diverse set of ranking methods is to\nensure the pool is broad. And can include as many possible\nrelevant documents as possible. And then, the users would,\nhuman assessors would make complete the judgments on this data set, this pool. And the other unjudged the documents are\nusually just assumed to be non relevant. Now if the pool is large enough,\nthis assumption is okay. But if the pool is not very large,\nthis actually has to be reconsidered. And we might use other\nstrategies to deal with them and there are indeed other\nmethods to handle such cases. And such a strategy is generally okay for comparing systems that\ncontribute to the pool. That means if you participate\nin contributing to the pool, then it's unlikely that it\nwould penalize your system because the problematic\ndocuments have all been judged. However, this is problematic for evaluating a new system that may\nhave not contributed to the pool. In this case, a new system might\nbe penalized because it might have nominated some read only documents\nthat have not been judged. So those documents might be\nassumed to be non relevant. That's unfair. So to summarize the whole part of textual\nevaluation, it's extremely important. Because the problem is the empirically\ndefined problem, if we don't rely on users, there's no way to\ntell whether one method works better. If we have in the property\nexperiment design, we might misguide our research or\napplications. And we might just draw wrong conclusions. And we have seen this is\nin some of our discussions. So make sure to get it right for\nyour research or application. The main methodology is the Cranfield\nevaluation methodology. And they are the main paradigm used in\nall kinds of empirical evaluation tasks, not just a search engine variation. Map and nDCG are the two main\nmeasures that you should definitely know about and they are appropriate for\ncomparing ranking algorithms. You will see them often\nin research papers. Precision at 10 documents is easier\nto interpret from user's perspective. So that's also often useful. What's not covered is some other\nevaluation strategy like A-B Test. Where the system would mix two,\nthe results of two methods, randomly. And then would show\nthe mixed results to users. Of course, the users don't see\nwhich result, from which method. The users would judge those results or click on those documents in\na search engine application. In this case then, the search engine\ncan check or click the documents and see if one method has contributed\nmore through the click the documents. If the user tends to click on one,\nthe results from one method, then it suggests that\nmessage may be better. So this is what leverages the real users\nof a search engine to do evaluation. It's called A-B Test and\nit's a strategy that is often used by the modern search engines or\ncommercial search engines. Another way to evaluate IR or textual retrieval is user studies and\nwe haven't covered that. I've put some references here\nthat you can look at if you want to know more about that. So, there are three\nadditional readings here. These are three mini books about\nevaluation and they are all excellent in covering a broad review of\nInformation Retrieval Evaluation. And it covers some of the things\nthat we discussed, but they also have a lot of others to offer. [MUSIC]",
 "01_lesson-4-1-probabilistic-retrieval-model-basic-idea.en.txt": "[SOUND]\nThis lecture is about\nthe Probabilistic Retrieval Model. In this lecture, we're going to continue the discussion\nof the Text Retrieval Methods. We're going to look at another kind of\nvery different way to design ranking functions than the Vector Space Model\nthat we discussed before. In probabilistic models,\nwe define the ranking function, based on the probability that this\ndocument is relevant to this query. In other words, we introduce\na binary random variable here. This is the variable R here. And we also assume that the query and the documents are all observations\nfrom random variables. Note that in the vector-based models,\nwe assume they are vectors, but here we assume they are the data\nobserved from random variables. And so, the problem of retrieval becomes\nto estimate the probability of relevance. In this category of models,\nthere are different variants. The classic probabilistic model has\nled to the BM25 retrieval function, which we discussed in in\nthe vectors-based model because its a form is actually\nsimilar to a backwards space model. In this lecture,\nwe will discuss another sub class in this P class called a language\nmodeling approaches to retrieval. In particular, we're going to discuss\nthe query likelihood retrieval model, which is one of the most effective\nmodels in probabilistic models. There was also another line called\nthe divergence from randomness model which has led to the PL2 function, it's also one of the most effective\nstate of the art retrieval functions. In query likelihood, our assumption\nis that this probability of relevance can be approximated by the probability\nof query given a document and relevance. So intuitively, this probability just\ncaptures the following probability. And that is if a user likes document d,\nhow likely would the user enter query q ,in\norder to retrieve document d? So we assume that the user likes d,\nbecause we have a relevance value here. And then we ask the question about how\nlikely we'll see this particular query from this user? So this is the basic idea. Now, to understand this idea,\nlet's take a look at the general idea or the basic idea of\nProbabilistic Retrieval Models. So here, I listed some imagined\nrelevance status values or relevance judgments of queries and\ndocuments. For example, in this line, it shows that q1 is a query\nthat the user typed in. And d1 is a document\nthat the user has seen. And 1 means the user thinks\nd1 is relevant to q1. So this R here can be also approximated\nby the click-through data that a search engine can collect by watching how you\ninteracted with the search results. So in this case, let's say\nthe user clicked on this document. So there's a 1 here. Similarly, the user clicked on d2 also,\nso there is a 1 here. In other words,\nd2 is assumed to be relevant to q1. On the other hand,\nd3 is non-relevant, there's a 0 here. And d4 is non-relevant and then d5 is\nagain, relevant, and so on and so forth. And this part, maybe,\ndata collected from a different user. So this user typed in q1 and then found\nthat the d1 is actually not useful, so d1 is actually non-relevant. In contrast, here we see it's relevant. Or this could be the same query typed\nin by the same user at different times. But d2 is also relevant, etc. And then here,\nwe can see more data about other queries. Now, we can imagine we\nhave a lot of such data. Now we can ask the question, how can we then estimate\nthe probability of relevance? So how can we compute this\nprobability of relevance? Well, intuitively that just means if we look at all the entries\nwhere we see this particular d and this particular q, how likely we'll\nsee a one on this other column. So basically that just means that\nwe can just collect the counts. We can first count how many\ntimes we have seen q and d as a pair in this table and\nthen count how many times we actually have also seen\n1 in the third column. And then, we just compute the ratio. So let's take a look at\nsome specific examples. Suppose we are trying to compute this\nprobability for d1, d2 and d3 for q1. What is the estimated probability? Now, think about that. You can pause the video if needed. Try to take a look at the table. And try to give your\nestimate of the probability. Have you seen that,\nif we are interested in q1 and d1, we'll be looking at these two pairs? And in both cases, well, actually, in one of the cases, the user\nhas said this is 1, this is relevant. So R = 1 in only one of the two cases. In the other case, it's 0. So that's one out of two. What about the d1 and the d2? Well, they are here, d1 and d2, d1 and d2, in both cases, in this case, R = 1. So it's a two out of two and\nso on and so forth. So you can see with this approach, we can actually score these documents for\nthe query, right? We now have a score for d1,\nd2 and d3 for this query. And we can simply rank them\nbased on these probabilities and so that's the basic idea\nprobabilistic retrieval model. And you can see it makes a lot of sense,\nin this case, it's going to rank d2 above\nall the other documents. Because in all the cases,\nwhen you have c and q1 and d2, R = 1. The user clicked on this document. So this also should show that\nwith a lot of click-through data, a search engine can learn a lot from\nthe data to improve their search engine. This is a simple example\nthat shows that with even with small amount of entries here we can\nalready estimate some probabilities. These probabilities would give us\nsome sense about which document might be more relevant or more useful\nto a user for typing this query. Now, of course, the problems that we\ndon't observe all the queries and all the documents and\nall the relevance values, right? There would be a lot of unseen documents,\nin general, we have only collected the data from the\ndocuments that we have shown to the users. And there are even more unseen queries\nbecause you cannot predict what queries will be typed in by users. So obviously,\nthis approach won't work if we apply it to unseen queries or unseen documents. Nevertheless, this shows the basic idea\nof probabilistic retrieval model and it makes sense intuitively. So what do we do in such a case when\nwe have a lot of unseen documents and unseen queries? Well, the solutions that we have\nto approximate in some way. So in this particular case called\na query likelihood retrieval model, we just approximate this by\nanother conditional probability. p(q given d, R=1). So in the condition part, we assume that\nthe user likes the document because we have seen that the user\nclicked on this document. And this part shows that\nwe're interested in how likely the user would\nactually enter this query. How likely we will see this\nquery in the same row. So note that here, we have made\nan interesting assumption here. Basically, we're going to do, assume that\nwhether the user types in this query has something to do with whether\nuser likes the document. In other words,\nwe actually make the following assumption. And that is a user formulates a query\nbased on an imaginary relevant document. Where if you just look at this\nas conditional probability, it's not obvious we\nare making this assumption. So what I really meant is that\nto use this new conditional probability to help us score,\nthen this new conditional probability will have to somehow\nbe able to estimate this conditional probability without\nrelying on this big table. Otherwise we would be having\nsimilar problems as before, and by making this assumption,\nwe have some way to bypass this big table, and try to just model how the user\nformulates the query, okay? So this is how you can\nsimplify the general model so that we can derive a specific\nrelevant function later. So let's look at how this model work for\nour example. And basically, what we are going to do in this case\nis to ask the following question. Which of these documents is most\nlikely the imaginary relevant document in the user's mind when\nthe user formulates this query? So we ask this question and we quantify\nthe probability and this probability is a conditional probability of observing\nthis query if a particular document is in fact the imaginary relevant\ndocument in the user's mind. Here you can see we've computed all\nthese query likelihood probabilities. The likelihood of queries\ngiven each document. Once we have these values, we can then rank these documents\nbased on these values. So to summarize, the general idea\nof modern relevance in the proper risk model is to assume the we introduce\na binary random variable R, here. And then, let the scoring function be defined\nbased on this conditional probability. We also talked about approximating\nthis by using the query likelihood. And in this case we have a ranking\nfunction that's basically based on the probability of\na query given the document. And this probability should be interpreted\nas the probability that a user who likes document d, would pose query q. Now, the question of course is, how do\nwe compute this conditional probability? At this in general has to do with how\nyou compute the probability of text, because q is a text. And this has to do with a model\ncalled a Language Model. And these kind of models\nare proposed to model text. So more specifically, we will be\nvery interested in the following conditional probability\nas is shown in this here. If the user liked this document,\nhow likely the user would pose this query. And in the next lecture we're going to do, giving introduction to language\nmodels that we can see how we can model text that was a probable\nrisk model, in general. [MUSIC]",
 "02_lesson-4-2-statistical-language-model.en.txt": "[SOUND] This lecture is about\nthe statistical language model. In this lecture, we're going to give an introduction\nto statistical language model. This has to do with how do you model\ntext data with probabilistic models. So it's related to how we model\nquery based on a document. We're going to talk about\nwhat is a language model. And then we're going to talk about the\nsimplest language model called the unigram language model, which also happens to be\nthe most useful model for text retrieval. And finally, what this class\nwill use is a language model. What is a language model? Well, it's just a probability\ndistribution over word sequences. So here, I'll show one. This model gives the sequence Today\nis Wednesday a probability of 0.001. It give Today Wednesday is a very, very small probability\nbecause it's non-grammatical. You can see the probabilities\ngiven to these sentences or sequences of words can vary\na lot depending on the model. Therefore, it's clearly context dependent. In ordinary conversation, probably Today is Wednesday is most\npopular among these sentences. Imagine in the context of\ndiscussing apply the math, maybe the eigenvalue is positive,\nwould have a higher probability. This means it can be used to\nrepresent the topic of a text. The model can also be regarded\nas a probabilistic mechanism for generating text. And this is why it's also often\ncalled a generating model. So what does that mean? We can imagine this is a mechanism that's visualised here as a stochastic system\nthat can generate sequences of words. So, we can ask for a sequence,\nand it's to send for a sequence from the device if you want,\nand it might generate, for example, Today is Wednesday, but it could\nhave generated any other sequences. So for example,\nthere are many possibilities, right? So in this sense,\nwe can view our data as basically a sample observed from\nsuch a generating model. So, why is such a model useful? Well, it's mainly because it can quantify\nthe uncertainties in natural language. Where do uncertainties come from? Well, one source is simply\nthe ambiguity in natural language that we discussed earlier in the lecture. Another source is because we don't\nhave complete understanding, we lack all the knowledge\nto understand the language. In that case,\nthere will be uncertainties as well. So let me show some examples of questions\nthat we can answer with a language model that would have interesting\napplications in different ways. Given that we see John and feels,\nhow likely will we see happy as opposed to habit as the next\nword in a sequence of words? Now, obviously, this would be very useful\nfor speech recognition because happy and habit would have similar acoustic sound,\nacoustic signals. But, if we look at the language model, we know that John feels happy would be\nfar more likely than John feels habit. Another example, given that we\nobserve baseball three times and game once in a news article,\nhow likely is it about sports? This obviously is related to text\ncategorization and information retrieval. Also, given that a user is\ninterested in sports news, how likely would the user\nuse baseball in a query? Now, this is clearly related\nto the query likelihood that we discussed in the previous lecture. So now,\nlet's look at the simplest language model, called a unigram language model. In such a case, we assume that we generate a text by\ngenerating each word independently. So this means the probability of\na sequence of words would be then the product of\nthe probability of each word. Now normally,\nthey're not independent, right? So if you have single word in like\na language, that would make it far more likely to observe model than if\nyou haven't seen the language. So this assumption is not\nnecessarily true, but we make this assumption\nto simplify the model. So now the model has precisely N\nparameters, where N is vocabulary size. We have one probability for each word, and\nall these probabilities must sum to 1. So strictly speaking,\nwe actually have N-1 parameters. As I said,\ntext can then be assumed to be assembled, drawn from this word distribution. So for example,\nnow we can ask the device or the model to stochastically generate\nthe words for us, instead of sequences. So instead of giving a whole sequence, like Today is Wednesday,\nit now gives us just one word. And we can get all kinds of words. And we can assemble these\nwords in a sequence. So that will still allow you\nto compute the probability of Today is Wednesday as the product\nof the three probabilities. As you can see, even though we have not\nasked the model to generate the sequences, it actually allows us to compute\nthe probability for all the sequences, but this model now only needs N\nparameters to characterize. That means if we specify\nall the probabilities for all the words, then the model's\nbehavior is completely specified. Whereas if we don't make this assumption,\nwe would have to specify probabilities for all kinds of combinations\nof words in sequences. So by making this assumption, it makes it\nmuch easier to estimate these parameters. So let's see a specific example here. Here I show two unigram language\nmodels with some probabilities. And these are high probability\nwords that are shown on top. The first one clearly suggests\na topic of text mining, because the high probability\nwas all related to this topic. The second one is more related to health. Now we can ask the question, how likely were observe a particular\ntext from each of these two models? Now suppose we sample\nwords to form a document. Let's say we take the first distribution,\nwould you like to sample words? What words do you think would be\ngenerated while making a text or maybe mining maybe another word? Even food, which has a very small probability,\nmight still be able to show up. But in general, high probability\nwords will likely show up more often. So we can imagine what general text\nof that looks like in text mining. In fact, with small probability, you might be able to actually generate\nthe actual text mining paper. Now, it will actually be meaningful,\nalthough the probability will be very, very small. In an extreme case, you might\nimagine we might be able to generate a text mining paper that would be\naccepted by a major conference. And in that case,\nthe probability would be even smaller. But it's a non-zero probability, if we assume none of the words\nhave non-zero probability. Similarly from the second topic, we can imagine we can generate\na food nutrition paper. That doesn't mean we cannot generate this\npaper from text mining distribution. We can, but the probability would be very,\nvery small, maybe smaller than even generating a paper that can be accepted\nby a major conference on text mining. So the point is that\nthe keeping distribution, we can talk about the probability of\nobserving a certain kind of text. Some texts will have higher\nprobabilities than others. Now let's look at the problem\nin a different way. Suppose we now have available\na particular document. In this case, many of the abstract or\nthe text mining table, and we see these word counts here. The total number of words is 100. Now the question you ask here\nis an estimation question. We can ask the question which model, which one of these distribution has\nbeen used to generate this text, assuming that the text has been generated\nby assembling words from the distribution. So what would be your guess? What we have to decide are what\nprobabilities text mining, etc., would have. Suppose the view for a second, and\ntry to think about your best guess. If you're like a lot of people,\nyou would have guessed that well, my best guess is text has a probability\nof 10 out of 100 because I've seen text 10 times, and\nthere are in total 100 words. So we simply normalize these counts. And that's in fact the word justified, and your intuition is consistent\nwith mathematical derivation. And this is called the maximum\nlikelihood estimator. In this estimator,\nwe assume that the parameter settings of those that would give our observe\nthe data the maximum probability. That means if we change these\nprobabilities, then the probability of observing the particular text\ndata would be somewhat smaller. So you can see,\nthis has a very simple formula. Basically, we just need to look at\nthe count of a word in a document, and then divide it by the total number of\nwords in the document or document lens. Normalize the frequency. A consequence of this is, of course, we're going to assign\nzero probabilities to unseen words. If we have an observed word, there will be no incentive to assign a\nnon-zero probability using this approach. Why? Because that would take away probability\nmass for these observed words. And that obviously wouldn't maximize the probability of this\nparticular observed text data. But one has still question whether\nthis is our best estimate. Well, the answer depends on what kind\nof model you want to find, right? This estimator gives a best model\nbased on this particular data. But if you are interested in a model\nthat can explain the content of the full paper for this abstract, then you\nmight have a second thought, right? So for thing,\nthere should be other words in the body of that article, so\nthey should not have zero probabilities, even though they're not\nobserved in the abstract. So we're going to cover this\na little bit more later in this class in the query\nlikelihood model. So let's take a look at some possible\nuses of these language models. One use is simply to use\nit to represent the topics. So here I show some general\nEnglish background texts. We can use this text to\nestimate a language model, and the model might look like this. Right, so on the top, we have those\nall common words, the, a, is, we, etc., and then we'll see some\ncommon words like these, and then some very,\nvery rare words in the bottom. This is a background language model. It represents the frequency of\nwords in English in general. This is the background model. Now let's look at another text,\nmaybe this time, we'll look at the computer\nscience research papers. So we have a collection of\ncomputer science research papers, we do as mentioned again, we can just\nuse the maximum likelihood estimator, where we simply normalize the frequencies. Now in this case, we'll get\nthe distribution that looks like this. On the top, it looks similar because\nthese words occur everywhere, they are very common. But as we go down,\nwe'll see words that are more related to computer science,\ncomputer software, text, etc. And so although here, we might also see\nthese words, for example, computer, but we can imagine the probability here is\nmuch smaller than the probability here. And we will see many other words here that\nwould be more common in general English. So you can see this distribution\ncharacterizes a topic of the corresponding text. We can look at even the smaller text. So in this case,\nlet's look at the text mining paper. Now if we do the same,\nwe have another distribution, again the can be expected\nto occur in the top. The sooner we see text, mining,\nassociation, clustering, these words have relatively\nhigh probabilities. In contrast, in this distribution, the\ntext has a relatively small probability. So this means, again,\nbased on different text data, we can have a different model,\nand the model captures the topic. So we call this document\nthe language model, and we call this collection language model. And later, you will see how they're\nused in the retrieval function. But now,\nlet's look at another use of this model. Can we statistically find what words\nare semantically related to computer? Now how do we find such words? Well, our first thought is that let's take\na look at the text that match computer. So we can take a look at all the documents\nthat contain the word computer. Let's build a language model. We can see what words we see there. Well, not surprisingly, we see these\ncommon words on top as we always do. So in this case, this language model gives\nus the conditional probability of seeing the word in the context of computer. And these common words will\nnaturally have high probabilities. But we also see the computer itself and software will have relatively\nhigh probabilities. But if we just use this model, we cannot just say all these words\nare semantically related to computer. So ultimately, what we'd like to\nget rid of is these common words. How can we do that? It turns out that it's possible\nto use language model to do that. But I suggest you think about that. So how can we know what\nwords are very common, so that we want to kind\nof get rid of them? What model will tell us that? Well, maybe you can think about that. So the background language model\nprecisely tells us this information. It tells us what was\nour common in general. So if we use this background model, we would know that these words\nare common words in general. So it's not surprising to observe\nthem in the context of computer. Whereas computer has a very\nsmall probability in general, so it's very surprising that we have seen\ncomputer with this probability, and the same is true for software. So then we can use these two\nmodels to somehow figure out the words that are related to computer. For example, we can simply take the ratio\nof these group probabilities and normalize the topic of language model\nby the probability of the word in the background language model. So if we do that, we take the ratio,\nwe'll see that then on the top, computer is ranked, and\nthen followed by software, program, all these words\nrelated to computer. Because they occur very frequently in the\ncontext of computer, but not frequently in the whole collection, whereas these common\nwords will not have a high probability. In fact,\nthey have a ratio about 1 down there because they are not really\nrelated to computer. By taking the sample of text\nthat contains the computer, we don't really see more occurrences\nof that than in general. So this shows that even with\nthese simple language models, we can do some limited\nanalysis of semantics. So in this lecture,\nwe talked about language model, which is basically a probability\ndistribution over text. We talked about the simplest language\nmodel called unigram language model, which is also just a word distribution. We talked about the two\nuses of a language model. One is we represent the topic in a\ndocument, in a collection, or in general. The other is we discover\nword associations. In the next lecture, we're going to talk\nabout how language model can be used to design a retrieval function. Here are two additional readings. The first is a textbook on statistical\nnatural language processing. The second is an article that\nhas a survey of statistical language models with a lot of\npointers to research work. [MUSIC]",
 "03_lesson-4-3-query-likelihood-retrieval-function.en.txt": "[SOUND] This lecture is about query likelihood, probabilistic retrieval model. In this lecture, we continue the discussion of\nprobabilistic retrieval model. In particular, we're going to talk about\nthe query light holder retrieval function. In the query light holder retrieval model,\nour idea is model. How like their user who likes a document\nwith pose a particular query? So in this case,\nyou can imagine if a user likes this particular document about\na presidential campaign news. Now we assume,\nthe user would use this a document as a basis to impose a query to try and\nretrieve this document. So again, imagine use a process\nthat works as follows. Where we assume that\nthe query is generated by assembling words from the document. So for example, a user might\npick a word like presidential, from this document and\nthen use this as a query word. And then the user would pick\nanother word like campaign, and that would be the second query word. Now this of course is an assumption\nthat we have made about how a user would pose a query. Whether a user actually followed this\nprocess may be a different question, but this assumption has allowed us to formerly\ncharacterize this conditional probability. And this allows us to also not rely on\nthe big table that I showed you earlier to use empirical data to\nestimate this probability. And this is why we can use this\nidea then to further derive retrieval function that we can\nimplement with the program language. So as you see the assumption\nthat we made here is each query word is independent of the sample. And also each word is basically\nobtained from the document. So now let's see how this works exactly. Well, since we are completing\na query likelihood then the probability here is just\nthe probability of this particular query, which is a sequence of words. And we make the assumption that each\nword is generated independently. So as a result, the probability\nof the query is just a product of the probability of each query word. Now how do we compute\nthe probability of each query word? Well, based on the assumption that a word is picked from the document\nthat the user has in mind. Now we know the probability of each word\nis just the relative frequency of each word in the document. So for example, the probability of\npresidential given the document. Would be just the count\nof presidential document divided by the total number of words\nin the document or document s. So with these assumptions we now have\nactually a simple formula for retrieval. We can use this to rank our documents. So does this model work? Let's take a look. Here are some example documents\nthat you have seen before. Suppose now the query is\npresidential campaign and we see the formula here on the top. So how do we score this document? Well, it's very simple. We just count how many times do\nwe have seen presidential or how many times do we have seen campaigns,\netc. And we see here 44, and\nwe've seen presidential twice. So that's 2 over the length of\ndocument 4 multiplied by 1 over the length of document 4 for\nthe probability of campaign. And similarly, we can get probabilities\nfor the other two documents. Now if you look at these numbers or\nthese formulas for scoring all these documents,\nit seems to make sense. Because if we assume d3 and\nd4 have about the same length, then looks like a nominal rank d4\nabove d3 and which is above d2. And as we would expect,\nlooks like it did captures a TF query state, and so\nthis seems to work well. However, if we try a different\nquery like this one, presidential campaign update\nthen we might see a problem. Well what problem? Well think about the update. Now none of these documents\nhas mentioned update. So according to our assumption that a user\nwould pick a word from a document to generate a query, then the probability of\nobtaining the word update would be what? Would be 0. So that causes a problem,\nbecause it would cause all these documents to have zero probability\nof generating this query. Now why it's fine to have zero probability\nfor d2, which is non-relevant? It's not okay to have 0 for d3 and d4 because now we no longer\ncan distinguish them. What's worse? We can't even distinguish them from d2. So that's obviously not desirable. Now when a [INAUDIBLE] has such result, we should think about what\nhas caused this problem? So we have to examine what\nassumptions have been made, as we derive this ranking function. Now is you examine those assumptions\ncarefully you will realize, what has caused this problem? So take a moment to think about it. What do you think is the reason why update\nhas zero probability and how do we fix it? So if you think about this from the moment\nyou realize that that's because we have made an assumption\nthat every query word must be drawn from the document\nin the user's mind. So in order to fix this, we have to\nassume that the user could have drawn a word not necessarily from the document. So that's the improved model. An improvement here is to say that, well instead of drawing\na word from the document, let's imagine that the user would actually\ndraw a word from a document model. And so I show a model here. And we assume that this document is\ngenerated using this unigram language model. Now, this model doesn't necessarily assign\nzero probability for update in fact, we can assume this model does not\nassign zero probability for any word. Now if we're thinking this way then\nthe generation process is a little bit different. Now the user has this model in mind\ninstead of this particular document. Although the model has to be\nestimated based on the document. So the user can again generate\nthe query using a singular process. Namely, pick a word for example,\npresidential and another word campaign. Now the difference is that this time\nwe can also pick a word like update, even though update doesn't\noccur in the document to potentially generate\na query word like update. So that a query was updated\n1 times 0 probabilities. So this would fix our problem. And it's also reasonable because when our\nthinking of what the user is looking for in a more general way, that is unique\nlanguage model instead of fixed document. So how do we compute\nthis query likelihood? If we make this sum wide\ninvolved two steps. The first one is compute this model, and\nwe call it document language model here. For example, I've shown two pulse models\nhere, it's major based on two documents. And then given a query like a data mining\nalgorithms the thinking is that we'll just compute the likelihood of this query. And by making independence\nassumptions we could then have this probability as a product of\nthe probability of each query word. We do this for both documents, and\nthen we can score these two documents and then rank them. So that's the basic idea of this\nquery likelihood retrieval function. So more generally this ranking function\nwould look like in the following. Here we assume that the query has n words, w1 through wn, and\nthen the scoring function. The ranking function is the probability\nthat we observe this query, given that the user is\nthinking of this document. And this is assume it will be product of\nprobabilities of all individual words. This is based on independent assumption. Now we actually often score\nthe document before this query by using log of the query likelihood\nas shown on the second line. Now we do this to avoid having a lot of small probabilities,\nmean multiply together. And this could cause under flow and we\nmight loose the precision by transforming the value in our algorithm function. We maintain the order of these documents\nyet we can avoid the under flow problem. And so if we take longer than\ntransformation of course, the product would become a sum\nas you on the second line here. So the sum of all the query\nwords inside of the sum that is one of the probability of\nthis word given by the document. And then we can further rewrite\nthe sum to a different form. So in the first sum here, in this sum, we have it over all the query words and\nquery word. And in this sum we have a sum\nof all the possible words. But we put a counter here\nof each word in the query. Essentially we are only considering\nthe words in the query, because if a word is not in the query,\nthe count will be 0. So we're still considering\nonly these n words. But we're using a different form as\nif we were going to take a sample of all the words in the vocabulary. And of course, a word might occur\nmultiple times in the query. That's why we have a count here. And then this part is log of\nthe probability of the word, given by the document language model. So you can see in this retrieval function, we actually know the count\nof the word in the query. So the only thing that we don't know\nis this document language model. Therefore, we have converted\nthe retrieval problem include the problem of estimating\nthis document language model. So that we can compute the probability of\neach query word given by this document. And different estimation methods would\nlead to different ranking functions. This is just like a different way to\nplace document in the vector space which leads to a different ranking\nfunction in the vector space model. Here different ways to\nestimate will lead to a different ranking function for\nquery likelihood. [MUSIC]",
 "04_lesson-4-4-statistical-language-model-part-1.en.txt": "[SOUND]\nThis lecture is about smoothing\nof language models. In this lecture, we're going to continue talking about\nthe probabilistic retrieval model. In particular,\nwe're going to talk about the smoothing of language model in the query\nlikelihood retrieval method. So you have seen this slide\nfrom a previous lecture. This is the ranking function\nbased on the query likelihood. Here, we assume that the independence of\ngenerating each query word And the formula would look like the following where\nwe take a sum of all the query words. And inside the sum there is a log\nof probability of a word given by the document or document image model. So the main task now is to estimate this document language model as we\nsaid before different methods for estimating this model would lead\nto different retrieval functions. So in this lecture, we're going to\nbe looking to this in more detail. So how do we estimate this language model? Well the obvious choice would be\nthe maximum likelihood estimate that we have seen before. And that is we're going to normalize\nthe word frequencies in the document. And estimate the probability\nit would look like this. This is a step function here. Which means all of the words that have the same frequency count will\nhave identical problem with it. This is another freedom to count,\nthat has different probability. Note that for words that have not\noccurred in the document here they will have 0 probability. So we know this is just like the model\nthat we assume earlier in the lecture. Where we assume that the use of\nthe simple word from the document to a formula to clear it. And there's no chance of assembling any\nword that's not in the document and we know that's not good. So how do we improve this? Well in order to assign\na none 0 probability to words that have not been observed in\nthe document, we would have to take away some probability mass from the words\nthat are observed in the document. So for example here, we have to take away\nsome probability of the mass because we need some extra probability mass for\nthe words otherwise they won't sum to 1. So all these probabilities must sum to 1. So to make this transformation and to\nimprove the maximum likelihood estimated by assigning non zero probabilities to\nwords that are not observed in the data. We have to do smoothing and\nsmoothing has to do with improving the estimate by considering\nthe possibility that if the author had been asking to write more words for the document,\nthe author might have written other words. If you think about this factor\nthen the a smoothed language model would be a more accurate than\nthe representation of the actual topic. Imagine you have seen an abstract\nof a research article. Let's say this document is abstract. If we assume and see words in this\nabstract that we have a probability of 0. That would mean there's\nno chance of sampling a word outside the abstract\nof the formulated query. But imagine a user who is interested\nin the topic of this subject. The user might actually\nchoose a word that's not in that chapter to use as query. So obviously,\nif we has asked this author to write more author would have written\na full text of the article. So smoothing of the language\nmodel is an attempt to try to recover the model for\nthe whole article. And then of course,\nwe don't have knowledge about any words that are not\nobserved in the abstract. So that's why smoothing is\nactually a tricky problem. So let's talk a little more about\nhow to smooth a language model. The key question here is, what probability\nshould be assigned to those unseen words? And there are many different\nways of doing that. One idea here, that's very useful for\nretrieval is let the probability of unseen word be proportional to its probability\ngiven by a reference language model. That means if you don't observe\nthe word in the dataset. We're going to assume that its\nprobability is kind of governed by another reference language\nmodel that we will construct. It will tell us which unseen words\nwould have a higher probability. In the case of retrieval,\na natural choice would be to take the collection language model\nas the reference language model. That is to say, if you don't\nobserve a word in the document, we're going to assume that\nthe probability of this word would be proportional to the probability\nof word in the whole collection. So more formally, we'll be estimating the probability\nof a word key document as follows. If the word is seen in\nthe document then the probability would be this counted the maximum\nlikelihood estimate P sub c here. Otherwise, if the word is not seen in the\ndocument we're going to let probability be proportional to the probability\nof the word in the collection. And here the coefficient that offer is to control the amount of probability\nmass that we assign to unseen words. Obviously, all these\nprobabilities must sum to 1, so alpha sub d is constrained in some way. So what if we plug in this\nsmoothing formula into our query likelihood ranking function? This is what we will get. In this formula, we have this as a sum over all the query words and those that we have written here as the sum\nof all the vocabulary, you see here. This is the sum of all\nthe words in the vocabulary, but not that we have a count\nof the word in the query. So in fact, we are just taking\na sample of query words. This is now a common\nway that we would use, because of its convenience\nin some transformations. So this is as I said,\nthis is sum of all the query words. In our smoothing method,\nwe assume that the words that are not observed in the method would have\na somewhat different form of probability. Name it's four, this foru. So we're going to do then,\ndecompose the sum into two parts. One sum is over all the query words\nthat are matching the document. That means that in this sum, all the words have a non zero probability\nin the document. Sorry, it's the non zero count\nof the word in the document. They all occur in the document. And they also have to of course\nhave a non zero count in the query. So these are the query words\nthat are matching the document. On the other hand, in this sum we\nare taking a sum of all the words that are not all query was\nnot matching the document. So they occur in the query\ndue to this term, but they don't occur in the document. In this case, these words have this probability because\nof our assumption about the smoothing. That here, these seen words\nhave a different probability. Now, we can go further by\nrewriting the second sum as a difference of two other sums. Basically, the first sum is\nthe sum of all the query words. Now, we know that the original sum\nis not over all the query words. This is over all the query words that\nare not matched in the document. So here we pretend that they\nare actually over all the query words. So we take a sum over all the query words. Obviously, this sum has extra\nterms that are not in this sum. Because, here we're taking\nsum over all the query words. There, it's not matched in the document. So in order to make them equal, we will\nhave to then subtract another sum here. And this is the sum over all the query\nwords that are matching in the document. And this makes sense, because here\nwe are considering all query words. And then we subtract the query\nthat was matched in the document. That would give us the query that\nwas not matched in the document. And this is almost a reverse\nprocess of the first step here. And you might wonder why\ndo we want to do that. Well, that's because if we do this, then we have different forms\nof terms inside of these sums. So now, you can see in this sum\nwe have all the words matched, the query was matching the document\nwith this kind of term. Here we have another sum over the same set\nof terms, matched query terms in document. But inside the sum, it's different. But these two sums can clearly be merged. So if we do that, we'll get another form of the formula that looks like\nbefore me at the bottom here. And note that this is\na very interesting formula. Because here we combine\nthese two that all or some of the query words matching in\nthe document in the one sum here. And the other sum now is\ndecomposing into two parts. And these two parts\nlook much simpler just, because these are the probabilities\nof unseen words. This formula is very interesting\nbecause you can see the sum is now over the match the query terms. And just like in the vector space model,\nwe take a sum of terms that are in the intersection of\nquery vector and the document vector. So it already looks a little bit\nlike the vector space model. In fact, there's even more similarity\nhere as we explain on this slide. [MUSIC]",
 "05_lesson-4-5-statistical-language-model-part-2.en.txt": "[SOUND] So I showed you how we rewrite the query like holder which is a function into\na form that looks like the formula of this slide after if we make\nthe assumption about the smoothing, the language model based on\nthe collection language model. Now if you look at this rewriting,\nit will actually give us two benefits. The first benefit is it helps us better\nunderstand this ranking function. In particular, we're going to show that\nfrom this formula we can see smoothing with the collection language model would\ngive us something like a TF-IDF weighting and length normalization. The second benefit is that\nit also allows us to compute the query like holder more efficiently. In particular we see that\nthe main part of the formula is a sum over the match\nof the query terms. So this is much better than if we\ntake a sum over all the words. After we smooth the document the damage\nmodel we essentially have non zero problem for all the words. So this new form of the formula is\nmuch easier to score or to compute. It's also interesting to note that the last term here is actually\nindependent of the document. Since our goal is to\nrank the documents for the same query we can ignore this term for\nranking. Because it's going to be the same for\nall the documents. Ignoring it wouldn't affect\nthe order of the documents. Inside the sum, we also see that each matched query\nterm would contribute a weight. And this weight actually is very interesting because it\nlooks like a TF-IDF weighting. First we can already see it has\na frequency of the word in a query just like in the vector space model. When we take a thought product, we see the word frequency in\nthe query to show up in such a sum. And so naturally this part would\ncorrespond between the vector element from the documented vector. And here indeed we can see it actually encodes a weight that has similar\nin factor to TF-IDF weight. I'll let you examine it, can you see it? Can you see which part is capturing TF? And which part is\na capturing IDF weighting? So if want you can pause\nthe video to think more about it. So have you noticed that this P sub\nseen is related to the term frequency in the sense that if a word occurs\nvery frequently in the document, then the s made through probability\nhere will tend to be larger. So this means this term is really\ndoing something like a TF weight. Now have you also noticed that\nthis term in the denominator is actually achieving the factor of IDF? Why, because this is the popularity\nof the term in a collection. But it's in the denominator, so if the\nprobability in the collection is larger then the weight is actually smaller. And this means a popular term. We actually have a smaller weight and this\nis precisely what IDF weighting is doing. Only that we now have\na different form of TF and IDF. Remember IDF has a logarithm\nof documented frequency. But here we have something different. But intuitively it\nachieves a similar effect. Interestingly, we also have something\nrelated to the length of libation. Again, can you see which factor is related\nto the document length in this formula? What I just say is that this term\nis related to IDF weighting. This collection probability,\nbut it turns out that this term here is actually related\nto document length normalization. In particular, F of sub d might\nbe related to document length. So it encodes how much probability\nmass we want to give to unseen worlds. How much smoothing do we want to do? Intuitively, if a document is long, then we need to do less smoothing because\nwe can assume that data is large enough. We probably have observed all the words\nthat the author could have written. But if the document is short then r of\nsub t could be expected to be large. We need to do more smoothing. It's likey there are words that have\nnot been written yet by the author. So this term appears to paralyze\nthe non document in that other sub D would tend to be longer\nthan or larger than for a long document. But note that alpha sub d\nalso occurs here and so this may not actually be necessary\nparalyzing long documents. The effect is not so clear yet. But as we will see later, when we\nconsider some specific smoothing methods, it turns out that they do\nparalyze long documents. Just like in TF-IDF weighting and document length normalization\nformula in the vector space model. So, that's a very interesting\nobservation because it means we don't even have to think about\nthe specific way of doing smoothing. We just need to assume that if we smooth\nwith this collection memory model, then we would have a formula that\nlooks like TF-IDF weighting and documents length violation. What's also interesting that we have\nvery fixed form of the ranking function. And see we have not heuristically\nput a logarithm here. In fact, you can think about why\nwe would have a logarithm here. You look at the assumptions that\nwe have made, it would be clear it's because we have used a logarithm\nof query like for scoring. And we turned the product into a sum\nof logarithm of probability, and that's why we have this logarithm. Note that if only want to heuristically\nimplement a TF weighting and IDF weighting, we don't necessary\nhave to have a logarithm here. Imagine if we drop this logarithm,\nwe would still have TF and IDF weighting. But what's nice with problem risk modeling\nis that we are automatically given the logarithm function here. And that's basically a fixed form\nof the formula that we did not really have to heuristically design,\nand in this case if you try to drop the logarithm the model probably won't\nwork as well as if you keep the logarithm. So a nice property of problem risk\nmodeling is that by following some assumptions and the probability rules\nwe'll get a formula automatically. And the formula would have\na particular form like in this case. And if we heuristically design\nthe formula we may not necessarily end up having such a specific formula. So to summarize, we talked about the need\nfor smoothing the document imaging model. Otherwise it would give zero probability\nfor unseen words in the document, and that's not good for\nstoring a query with such an unseen word. It's also necessary, in general,\nto improve the accuracy of estimating the model represent\nthe topic of this document. The general idea of smoothing in retrieval\nis to use the connecting memory model to, to give us some clue about which unseen\nwords should have a higher probability. That is, the probability of an unseen\nword is assumed to be proportional to its probability in the collection. With this assumption, we've shown that we\ncan derive a general ranking formula for query likelihood that has\neffect of TF-IDF weighting and document length normalization. We also see that, through some rewriting, the scoring of such a ranking function\nis primarily based on sum of weights on matched query terms,\njust like in the vector space model. But, the actual ranking\nfunction is given us automatically by the probability rules and\nassumptions that we have made. And like in the vector space model\nwhere we have to heuristically think about the form of the function. However, we still need to address\nthe question how exactly we should smooth the document and the model. How exactly we should\nuse the reference and model based on the connection\nto adjust the probability of the maximum micro is made of and\nthis is the topic of the next batch. [MUSIC]",
 "06_lesson-4-6-smoothing-methods-part-1.en.txt": "[SOUND] This lecture is about the specific smoothing methods for language models\nused in probabilistic retrieval model. In this lecture, we will continue\nthe discussion of language models for information retrieval, particularly\nthe query likelihood retrieval method. And we're going to talk about specifically\nthe smoothing methods used for such a retrieval function. So this is a slide from a previous\nlecture where we show that with a query likelihood ranking and smoothing\nwith the collection language model, we add up having a retrieval function\nthat looks like the following. So this is the retrieval function based on\nthese assumptions that we have discussed. You can see it's a sum of all\nthe matching query terms, here. And inside its sum is the count\nof the term in the query and some weight for the term in the document. We have t of i, the f weight here, and\nthen we have another constant here in n. So clearly if we want to implement this\nfunction using programming language, we still need to figure\nout a few variables. In particular, we're going to need to\nknow how to estimate the probability of a word exactly and how do we set alpha. So in order to answer this question,\nwe have to think about very specific smoothing methods, and\nthat is main topic of this lecture. We're going to talk about\ntwo smoothing methods. The first is simple linear\ninterpolation with a fixed coefficient. And this is also called\na Jelinek-Mercer smoothing. So the idea is actually very simple. This picture shows how\nwe estimate a document language model by using\nmaximum likelihood estimate. That gives us word counts normalized by\nthe total number of words in the text. The idea of using this method is to maximize the probability\nof the observed text. As a result,\nif a word like network is not observed in the text, it's going to get\n0 probability, as shown here. So the idea of smoothing, then,\nis to rely on collection language model where this word is not going to have\na zero probability to help us decide what nonzero probability should\nbe assigned to such a word. So we can note that network has\na nonzero probability here. So in this approach what we do is we do\na linear interpolation between the maximum likelihood placement here and\nthe collection language model, and this is computed by the smoothing parameter\nlambda, which is between 0 and 1. So this is a smoothing parameter. The larger lambda is,\nthe more smoothing we will have. So by mixing them together, we achieve the goal of assigning nonzero\nprobabilities to a word like network. So let's see how it works for\nsome of the words here. For example, if we compute\nthe smooth probability for text. Now the maximum likelihood\nestimated gives us 10 over 100, and that's going to be here. But the collection probability is this. So we'll just combine them\ntogether with this simple formula. We can also see the word network,\nwhich used to have a zero probability, now is getting a non-zero\nprobability of this value. And that's because the count is\ngoing to be zero for network here. But this part is nonzero, and\nthat's basically how this method works. Now if you think about this and\nyou can easily see now the alpha sub d in this smoothing\nmethod is basically lambda. Because that's remember the coefficient\nin front of the probability of the word given by the collection\nlanguage model here. Okay, so\nthis is the first smoothing method. The second one is similar but\nit has a tie-in into the coefficient for linear interpolation. It's often called Dirichlet Prior,\nor Bayesian, Smoothing. So again here we face problem\nof zero probability for an unseen word like network. Again we will use the collection\nlanguage model, but in this case, we're going to combine them\nin somewhat different ways. The formula first can be seen as\na interpolation of the maximum likelihood estimate and\nthe collection language model as before, as in the J-M smoothing method. Only that the coefficient now\nis not lambda, a fixed number, but a dynamic coefficient in this form, where mu is a parameter,\nit's a non-negative value. And you can see if we\nset mu to a constant, the effect is that a long document would\nactually get a smaller coefficient here. Because a long document\nwill have longer lengths, therefore the coefficient\nis actually smaller. And so a long document would have\nless smoothing, as we would expect. So this seems to make more sense\nthan a fixed coefficient smoothing. Of course,\nthis part would be of this form so that the two coefficients would sum to 1. Now this is one way to\nunderstand this smoothing. Basically, it means it's a dynamic\ncoefficient interpolation. There is another way to understand\nthis formula which is even easier to remember, and\nthat's on this side. So it's easier to see how we can rewrite\nthe smoothing method in this form. Now in this form we can easily\nsee what change we have made to the maximum likelihood estimate,\nwhich would be this part. So normalize the count\nby the document length. So in this form we can see what we did is\nwe add this to the count of every word. So what does this mean? Well, this is basically something related\nto the probability of the word in the collection. And we multiply that by the parameter mu. And when we combine this\nwith the count here, essentially we are adding\npseudocounts to the observed text. We pretend every word has\ngot this many pseudocount. So the total count would be\nthe sum of these pseudocounts and the actual count of\nthe word in the document. As a result, in total we would\nhave added this many pseudocounts. Why?\nBecause if you take somewhat this one over all the words, then we'll see the\nprobability of the words would sum to 1, and that gives us just mu. So this is the total number of\npseudocounts that we added. And so\nthese probabilities would still sum to 1. So in this case, we can easily\nsee the method is essentially to add this as a pseudocount to this data. Pretend we actually augment the data\nby including some pseudo data defined by the collection language model. As a result, we have more counts is that the total counts for\na word would be like this. And as a result, even if a word has zero\ncount here, let's say if we have zero count here, then it would still have\nnonzero count because of this part. So this is how this method works. Let's also take a look at\nsome specific example here. So for text again we will\nhave 10 as the original count that we actually observe, but\nwe also add some pseudocount. And so the probability of\ntext would be of this form. Naturally, the probability of\nnetwork would be just this part. And so here you can also see\nwhat's alpha sub d here. Can you see it? If you want to think about it,\nyou can pause the video. But you'll notice that this\npart is basically alpha sub d. So we can see, in this case, alpha sub d does depend on the document, because this length\ndepends on the document, whereas in the linear interpolation, the J-M smoothing method,\nthis is a constant. [MUSIC]",
 "07_lesson-4-7-smoothing-methods-part-2.en.txt": "[SOUND] So let's plug in these model masses into the ranking function to\nsee what we will get, okay? This is a general smoothing. So a general ranking function for\nsmoothing with subtraction and you have seen this before. And now we have a very specific smoothing\nmethod, the JM smoothing method. So now let's see what what's a value for\noffice of D here. And what's the value for p sub c here? Right, so we may need to decide this in order to figure out the exact\nform of the ranking function. And we also need to figure\nout of course alpha. So let's see. Well this ratio is basically this,\nright, so, here, this is the probability\nof c board on the top, and this is the probability\nof unseen war or, in other words basically 11\ntimes basically the alpha here, this, so it's easy to see that. This can be then rewritten as this. Very simple. So we can plug this into here. And then here, what's the value for alpha? What do you think? So it would be just lambda, right? And what would happen if we plug in\nthis value here, if this is lambda. What can we say about this? Does it depend on the document? No, so it can be ignored. Right? So we'll end up having this\nranking function shown here. And in this case you can easy to see, this a precisely a vector space\nmodel because this part is a sum over all the matched query terms,\nthis is an element of the query map. What do you think is a element\nof the document up there? Well it's this, right. So that's our document left element. And let's further examine what's\ninside of this logarithm. Well one plus this. So it's going to be nonnegative,\nthis log of this, it's going to be at least 1, right? And these, this is a parameter,\nso lambda is parameter. And let's look at this. Now this is a TF. Now we see very clearly\nthis TF weighting here. And the larger the count is,\nthe higher the weighting will be. We also see IDF weighting,\nwhich is given by this. And we see docking the lan's\nrelationship here. So all these heuristics\nare captured in this formula. What's interesting that\nwe kind of have got this weighting function automatically\nby making various assumptions. Whereas in the vector space model, we had to go through those heuristic\ndesign in order to get this. And in this case note that\nthere's a specific form. And when you see whether this\nform actually makes sense. All right so what do you think\nis the denominator here, hm? This is a math of document. Total number of words,\nmultiplied by the probability of the word given by the collection, right? So this actually can be interpreted\nas expected account over word. If we're going to draw, a word,\nfrom the connection that we model. And, we're going to draw as many as\nthe number of words in the document. If you do that,\nthe expected account of a word, w, would be precisely given\nby this denominator. So, this ratio basically,\nis comparing the actual count, here. The actual count of the word in the\ndocument with expected count given by this product if the word is in fact following\nthe distribution in the clutch this. And if this counter is larger than\nthe expected counter in this part, this ratio would be larger than one. So that's actually a very\ninteresting interpretation, right? It's very natural and intuitive,\nit makes a lot of sense. And this is one advantage of using\nthis kind of probabilistic reasoning where we have made explicit assumptions. And, we know precisely why\nwe have a logarithm here. And, why we have these probabilities here. And, we also have a formula that\nintuitively makes a lot of sense and does TF-IDF weighting and\ndocumenting and some others. Let's look at the,\nthe Dirichlet Prior Smoothing. It's very similar to\nthe case of JM smoothing. In this case,\nthe smoothing parameter is mu and that's different from\nlambda that we saw before. But the format looks very similar. The form of the function\nlooks very similar. So we still have linear operation here. And when we compute this ratio, one will find that is that\nthe ratio is equal to this. And what's interesting here is that we\nare doing another comparison here now. We're comparing the actual count. Which is the expected account of the world\nif we sampled meal worlds according to the collection world probability. So note that it's interesting we don't\neven see docking the lens here and lighter in the JMs model. All right so this of course\nshould be plugged into this part. So you might wonder, so\nwhere is docking lens. Interestingly the docking lens\nis here in alpha sub d so this would be plugged into this part. As a result what we get is\nthe following function here and this is again a sum over\nall the match query words. And we're against the queer,\nthe query, time frequency here. And you can interpret this as\nthe element of a document vector, but this is no longer\na single dot product, right? Because we have this part,\nI know that n is the name of the query, right? So that just means if\nwe score this function, we have to take a sum over\nall the query words, and then do some adjustment of\nthe score based on the document. But it's still, it's still clear\nthat it does documents lens modulation because this lens\nis in the denominator so a longer document will\nhave a lower weight here. And we can also see it has tf here and\nnow idf. Only that this time the form of the\nformula is different from the previous one in JMs one. But intuitively it still implements TFIDF\nwaiting and document lens rendition again, the form of the function is dictated\nby the probabilistic reasoning and assumptions that we have made. Now there are also\ndisadvantages of this approach. And that is, there's no guarantee\nthat there's such a form of the formula will actually work well. So if we look about at this geo function,\nall those TF-IDF waiting and document lens rendition for example it's unclear whether\nwe have sub-linear transformation. Unfortunately we can see here there\nis a logarithm function here. So we do have also the,\nso it's here right? So we do have the sublinear\ntransformation, but we do not intentionally do that. That means there's no guarantee that\nwe will end up in this, in this way. Suppose we don't have logarithm,\nthen there's no sub-linear transformation. As we discussed before, perhaps\nthe formula is not going to work so well. So that's an example of the gap\nbetween a formal model like this and the relevance that we have to model, which is really a subject\nmotion that is tied to users. So it doesn't mean we cannot fix this. For example, imagine if we did\nnot have this logarithm, right? So we can take a risk and\nwe're going to add one, or we can even add double logarithm. But then, it would mean that the function\nis no longer a proper risk model. So the consequence of\nthe modification is no longer as predictable as\nwhat we have been doing now. So, that's also why, for example,\nPM45 remains very competitive and still, open channel how to use\npublic risk models as they arrive, better model than the PM25. In particular how do we use query\nlike how to derive a model and that would work consistently\nbetter than DM 25. Currently we still cannot do that. Still interesting open question. So to summarize this part, we've talked\nabout the two smoothing methods. Jelinek-Mercer which is doing the fixed\ncoefficient linear interpolation. Dirichlet Prior this is what add a pseudo\ncounts to every word and is doing adaptive interpolation in that the coefficient\nwould be larger for shorter documents. In most cases we can see, by using these\nsmoothing methods, we will be able to reach a retrieval function where\nthe assumptions are clearly articulate. So they are less heuristic. Explaining the results also show\nthat these, retrieval functions. Also are very effective and they are\ncomparable to BM 25 or pm lens adultation. So this is a major advantage\nof probably smaller where we don't have to do\na lot of heuristic design. Yet in the end that we naturally\nimplemented TF-IDF weighting and doc length normalization. Each of these functions also has\nprecise ones smoothing parameter. In this case of course we still need\nto set this smoothing parameter. There are also methods that can be\nused to estimate these parameters. So overall,\nthis shows by using a probabilistic model, we follow very different strategies\nthen the vector space model. Yet, in the end, we end up uh,with\nsome retrievable functions that look very similar to\nthe vector space model. With some advantages in having\nassumptions clearly stated. And then, the form dictated\nby a probabilistic model. Now, this also concludes our discussion of\nthe query likelihood probabilistic model. And let's recall what\nassumptions we have made in order to derive the functions\nthat we have seen in this lecture. Well we basically have made four\nassumptions that I listed here. The first assumption is that the relevance\ncan be modeled by the query likelihood. And the second assumption with med is, are\nquery words are generated independently that allows us to decompose\nthe probability of the whole query into a product of probabilities\nof old words in the query. And then,\nthe third assumption that we have made is, if a word is not seen,\nthe document or in the late, its probability proportional to\nits probability in the collection. That's a smoothing with\na collection ama model. And finally, we made one of these\ntwo assumptions about the smoothing. So we either used JM smoothing or\nDirichlet prior smoothing. If we make these four assumptions\nthen we have no choice but to take the form of the retrieval\nfunction that we have seen earlier. Fortunately the function has a nice\nproperty in that it implements TF-IDF weighting and document machine and\nthese functions also work very well. So in that sense, these functions are less heuristic\ncompared with the vector space model. And there are many extensions of this,\nthis basic model and you can find the discussion of them in\nthe reference at the end of this lecture. [MUSIC]",
 "01_lesson-5-1-feedback-in-text-retrieval.en.txt": "[SOUND]\nThis lecture is about the feedback\nin text retrieval. So in this lecture, we will continue with\nthe discussion of text retrieval methods. In particular, we're going to talk\nabout the feedback in text retrieval. This is a diagram that shows\nthe retrieval process. We can see the user would type in a query. And then, the query would be\nsent to a retrieval engine or search engine, and\nthe engine would return results. These results would be issued to the user. Now, after the user has\nseen these results, the user can actually make judgements. So for example, the user says,\nwell, this is good and this document is not very useful and\nthis is good again, etc. Now, this is called a relevance judgment\nor relevance feedback because we've got some feedback information from\nthe user based on the judgements. And this can be very useful to the system, knowing what exactly is\ninteresting to the user. So the feedback module would\nthen take this as input and also use the document collection\nto try to improve ranking. Typically it would involve\nupdating the query so the system can now render the results\nmore accurately for the user. So this is called relevance feedback. The feedback is based on relevance\njudgements made by the users. Now, these judgements are reliable but the users generally don't want to make\nextra effort unless they have to. So the down side is that it involves\nsome extra effort by the user. There's another form of feedback\ncalled pseudo relevance feedback, or blind feedback,\nalso called automatic feedback. In this case, we can see once\nthe user has gotten [INAUDIBLE] or in fact we don't have to invoke users. So you can see there's\nno user involved here. And we simply assume that the top\nrank documents to be relevant. Let's say we have assumed\ntop 10 as relevant. And then, we will then use this\nassume the documents to learn and to improve the query. Now, you might wonder, how could this help if we simply\nassume the top rank of documents? Well, you can imagine these top\nrank of documents are actually similar to relevant documents\neven if they are not relevant. They look like relevant documents. So it's possible to learn some related\nterms to the query from this set. In fact, you may recall that we\ntalked about using language model to analyze what association, to learn\nrelated words to the word of computer. And there, what we did is we\nfirst use computer to retrieve all the documents that contain computer. So imagine now the query\nhere is a computer. And then, the result will be those\ndocuments that contain computer. And what we can do then is\nto take the top n results. They can match computer very well. And we're going to count\nthe terms in this set. And then, we're going to then use\nthe background language model to choose the terms that are frequent in this set\nbut not frequent in the whole collection. So if we make a contrast between\nthese two what we can find is that related to terms\nto the word computer. As we have seen before. And these related words can then be added\nto the original query to expand the query. And this would help us bring the documents\nthat don't necessarily match computer but match other words like program and\nsoftware. So this is very effective for\nimproving the search result. But of course, pseudo-relevancy\nvalues are completely unreliable. We have to arbitrarily set a cut off. So there's also something in\nbetween called implicit feedback. In this case,\nwhat we do is we do involve users, but we don't have to ask\nusers to make judgments. Instead, we're going to observe how the\nuser interacts with the search results. So in this case we'll look\nat the clickthroughs. So the user clicked on this one. And the user viewed this one. And the user skipped this one. And the user viewed this one again. Now, this also is a clue about whether\nthe document is useful to the user. And we can even assume that we're\ngoing to use only the snippet here in this document,\nthe text that's actually seen by the user instead of the actual\ndocument of this entry. The link they are saying web search\nmay be broken but it doesn't matter. If the user tries to fetch this\ndocument because of the displayed text we can assume these displayed\ntext is probably relevant is interesting to you so\nwe can learn from such information. And this is called interesting feedback. And we can, again,\nuse the information to update the query. This is a very important\ntechnique used in modern. Now, think about the Google and Bing and they can collect a lot of user\nactivities while they are serving us. So they would observe what documents\nwe click on, what documents we skip. And this information is very valuable. And they can use this to\nimprove the search engine. So to summarize, we talked about\nthe three kinds of feedback here. Relevant feedback where the user\nmakes explicit judgements. It takes some user effort, but\nthe judgment information is reliable. We talk about the pseudo feedback where\nwe seem to assume top brand marking will be relevant. We don't have to involve the user\ntherefore we could do that, actually before we return\nthe results to the user. And the third is implicit feedback\nwhere we use clickthroughs. Where we involve the users, but the user doesn't have to make\nit explicitly their fault. Make judgement. [MUSIC]",
 "02_lesson-5-2-feedback-in-vector-space-model-rocchio.en.txt": "[SOUND]\nThis lecture is about the feedback\nin the vector space model. In this lecture, we continue talking\nabout the feedback in text retrieval. Particularly, we're going to talk about\nfeedback in the vector space model. As we have discussed before,\nin the case of feedback the task of text retrieval system is removed from\nexamples in improved retrieval accuracy. We will have positive examples. Those are the documents that\nassume would be relevant or be charged with being relevant. All the documents that\nare viewed by users. We also have negative examples. Those are documents known\nto be non-relevant. They can also be the documents\nthat are skipped by users. The general method in\nthe vector space model for feedback is to modify our query vector. We want to place the query vector in\na better position to make it accurate. And what does that mean exactly? Well, if we think about the query vector\nthat would mean we would have to do something to the vector elements. And in general,\nthat would mean we might add new terms. Or we might just weight of old terms or\nassign weights to new terms. As a result, in general,\nthe query will have more terms. We often call this query expansion. The most effective method in\nthe vector space model for feedback is called the Rocchio Feedback, which was\nactually proposed several decades ago. So the idea is quite simple. We illustrate this idea by\nusing a two dimensional display of all the documents in the collection and\nalso the query vector. So now we can see the query\nvector is here in the center, and these are all the documents. So when we use the query back there and\nuse the same narrative function to find the most similar documents,\nwe are basically doing a circle here and that these documents would be\nbasically the top-ranked documents. And these process are relevant documents,\nand these are relevant documents,\nfor example, it's relevant, etc. And then these minuses are negative\ndocuments, like these. So our goal here is trying to move\nthis query back to some position, to improve the retrieval accuracy. By looking at this diagram,\nwhat do you think? Where should we move the query vector so that we can improve\nthe retrieval accuracy? Intuitively, where do you\nwant to move query vector? If you want to think more,\nyou can pause the video. If you think about this picture, you can\nrealize that in order to work well in this case you want the query vector to be as\nclose to the positive vectors as possible. That means ideally, you want to place\nthe query vectors somewhere here. Or we want to move the query\nvector closer to this point. Now so what exactly is this point? Well, if you want these relevant\ndocuments to rank on the top, you want this to be in the center of\nall these relevant documents, right? Because then if you draw\na circle around this one, you'll get all these relevant documents. So that means we can move the query\nvector towards the centroid of all the relevant document vectors. And this is basically the idea of Rocchio. Of course, you can consider\nthe centroid of negative documents and we want to move away from\nthe negative documents. Now your match that we're talking about\nmoving vector closer to some other vec and away from other vectors. It just means that we have this formula. Here you can see this is\noriginal query vector and this average basically is the centroid\nvector of relevant documents. When we take the average of these vectors, then were computing\nthe centroid of these vectors. Similarly, this is the average of\nnon-relevant document like this. So it's essentially of\nnon-relevant documents. And we have these three parameters here,\nalpha, beta, and gamma. They are controlling\nthe amount of movement. When we add these two vectors together, we're moving the query vector\ncloser to the centroid. This is when we add them together. When we subtracted this part, we kind of move the query\nvector away from that centroid. So this is the main idea\nof Rocchio feedback. And after we have done this, we will get a new query vector which\ncan be used to score documents. This new query vector,\nwill then reflect the move of this original query vector toward this\nrelevant centroid vector and away from the non-relevant value. Okay, so let's take a look at the example. This is the example that\nwe've seen earlier. Only that I deemed that display\nof the actual documents. I only showed the vector\nrepresentation of these documents. We have five documents here and we have to read in the documents here, right. And they're displayed in red. And these are the term vectors. Now I have just assumed some of weights. A lot of terms,\nwe have zero weights of course. Now these are negative arguments. There are two here. There is another one here. Now in this Rocchio method, we first\ncompute the centroid of each category. And so let's see,\nlook at the centroid vector of the positive documents, we simply just,\nso it's very easy to see. We just add this with this one\nthe corresponding element. And then that's down here and\ntake the average. And then we're going to add\nthe corresponding elements and then just take the average. And so we do this for all this. In the end, what we have is this one. This is the average vector of these two,\nso it's a centroid of these two. Let's also look at the centroid\nof the negative documents. This is basically the same. We're going to take the average\nof the three elements. And these are the corresponding\nelements in the three vectors, and so on and so forth. So in the end, we have this one. Now in the Rocchio feedback\nmethod we're going to combine all these with the original\nquery vector which is this. So now let's see how we\ncombine them together. Well, that's basically this. So we have a parameter alpha\ncontrolling the original query times weight that's one. And now we have beta to control\nthe inference of the positive centroid of the weight, that's 1.5. That comes from here. All right, so this goes here. And we also have this negative\nweight here gamma here. And this way, it has come from,\nof course, the negative centroid here. And we do exactly the same for\nother terms, each is for one term. And this is our new vector. And we're going to use this new query\nvector, this one to rank the documents. You can imagine what would happen, right? Because of the movement that this one\nwould matches these red documents much better because we moved\nthis vector closer to them. And it's going to penalize these black\ndocuments, these non relevent documents. So this is precisely what\nwe wanted from feedback. Now of course if we apply this method in\npractice we will see one potential problem and that is the original query has\nonly four terms that are now zero. But after we do query explaining and merging, we'll have many times\nthat would have non zero weights. So the calculation will\nhave to involve more terms. In practice,\nwe often truncate this matter and only retain the terms\nwith highest weights. So let's talk about how we\nuse this method in practice. I just mentioned that they're\noften truncated vector. Consider only a small number of\nwords that have highest weights in the centroid vector. This is for efficiency concern. I also said here that negative examples,\nor non-relevant examples tend not to be very useful, especially\ncompared with positive examples. Now you can think about why. One reason is because negative documents\ntend to distract the query in all directions. So, when you take the average, it doesn't really tell you where\nexactly it should be moving to. Whereas positive documents\ntend to be clustered together. And they will point you to\na consistent direction. So that also means that sometimes we don't\nhave to use those negative examples. But note that in some cases, in difficult\nqueries where most results are negative, negative feedback after is very useful. Another thing is to avoid over-fitting. That means we have to keep relatively\nhigh weight on the original query terms. Why? Because the sample that we see in\nfeedback Is a relatively small sample. We don't want to overly\ntrust the small sample. And the original query terms\nare still very important. Those terms are heightened by the user and the user has decided that those\nterms are most important. So in order to prevent\nthe us from over-fitting or drifting, prevent topic drifting due to\nthe bias toward the feed backing symbols. We generally would have to keep a pretty\nhigh weight on the original terms so it was safe to do that. And this is especially true for\npseudo relevance feedback. Now, this method can be used for both relevance feedback and\npseudo-relevance feedback. In the case of pseudo-feedback, the prime\nand the beta should be set to a smaller value because the relevant examples\nare assumed not to be relevant. They're not as reliable as\nthe relevance feedback. In the case of relevance feedback,\nwe obviously could use a larger value. So those parameters,\nthey have to be set empirically. And the Rocchio Method is\nusually robust and effective. It's still a very popular method for\nfeedback. [MUSIC]",
 "03_lesson-5-3-feedback-in-text-retrieval-feedback-in-lm.en.txt": "[SOUND]\nThis lecture is about the feedback in\nthe language modeling approach. In this lecture, we will continue the\ndiscussion of feedback in text retrieval. In particular, we're going to talk about the feedback\nin language modeling approaches. So we derive the query likelihood ranking\nfunction by making various assumptions. As a basic retrieval function,\nall those formulas worked well. But if we think about the feedback\ninformation, it's a little bit awkward to use query likelihood to perform feedback,\nbecause a lot of times the feedback information is\nadditional information about the query. But we assume the query has\ngenerated it by assembling words from a language model in\nthe query likelihood method. It's kind of unnatural to sample\nwords that form feedback documents. As a result, researchers proposed a way\nto generalize query likelihood function, and it's called Kullback-Leibler\ndivergence retrieval model. And this model is actually going\nto make the query likelihood retrieval function much\ncloser to vector space model. Yet this form of the language model\ncan be regarded as a generalization of query likelihood, in the sense that it can\ncover query likelihood as a special case. And in this case, then feedback can be achieved through\nsimply query model estimation or updating. This is very similar to Rocchio,\nwhich updates the query vector. So let's see what is this\nKL-divergence retrieval model. So on the top, what you see is a query\nlikelihood retrieval function, this one. And then KL-divergence, or\nalso called cross entropy, retrieval model is basically to generalize the frequency part here\ninto a language model. So basically it's the difference given by the probabilistic model here to\ncharacterize what the user is looking for, versus the count of query words there. And this difference allows us to plug in\nvarious different ways to estimate this. So this can be estimated\nin many different ways, including using feedback information. But this is called a KL-divergence,\nbecause this can be interpreted as matching\nthe KL-divergence of two distributions. One is the query model,\ndenoted by this distribution. One is the document\nlanguage model here and smooth them with a collection\nlanguage model, of course. And we are not going to talk\nabout the detail of that, and you'll find it in some references. It's also called cross entropy because,\nin fact, we ignore some terms in\nthe KL-divergence function and we will end up having\nactually cross entropy. And both are terms of information theory. But anyway, for our purposes here, you can just see the two\nformulas look almost identical, except that here we have a probability of\na word given by a query language model. And here the sum is over all the words\nthat are in the document and also with the nonzero probability for\nthe query model. So it's kind of, again, a generalization\nof sum over all the matching query words. Now you can also easily see we can recover\nthe query likelihood retrieval function by simply setting this query model to the\nrelative frequency of a word in the query. This is very easy to\nsee once you plug this into here you can eliminate this\nquery length as a constant. And then you will get exactly like that. So you can see the equivalence. And that's also why this KL-divergence\nmodel can be regarded as a generalization of query likelihood, because we can cover\nquery likelihood as a special case. But it would also allow us\nto do much more than that. So this is how we can use the\nKL-divergence model to then do feedback. The picture shows that we first\nestimate a document language model, then we estimate a query language model,\nand we compute the KL-divergence. This is often denoted by a D here. But this basically means this is\nexactly like the vector space model, because we compute a vector for the\ndocument, then compute another vector for the query, and\nthen we compute the distance. Only that these vectors are of special\nforms, they are probability distributions. And then we get the results and\nwe can find some feedback documents. Let's assume they are mostly\npositive documents, although we could also consider\nboth kinds of documents. So what we could do is, like in Rocchio,\nwe're going to compute another language model called the feedback\nlanguage model here. Again, this is going to be another vector\njust like the computing centroid of vector in Rocchio. And then this model can be combined\nwith the original query model using a linear interpolation, and\nthis would then give us an update model, just like, again, in Rocchio. So here we can see the parameter alpha\ncan control the amount of feedback. If it's set to zero,\nthen essentially there is no feedback. If it's set to one, we get full feedback\nand we ignore the original query. And this is generally not desirable,\nright? So unless you are absolutely sure you\nhave seen a lot of relevant documents, then the query terms are not important. So of course, the main question here is,\nhow do you compute this theta F? This is the big question here, and\nonce you can do that, the rest is easy. So here we will talk about\none of the approaches, and there are many approaches, of course. This approach is based\non generative model, and I'm going to show you how it works. This will use a generative mixture model. So this picture shows that\nwe have this model here, the feedback model that\nwe want to estimate. And the basis is the feedback documents. Let's say we are observing\nthe positive documents. These are the clicked documents by users\nor random documents judged by users, or are simply top ranked documents\nthat we assume to be relevant. Now imagine how we can\ncompute a centroid for these documents by using language model. One approach is simply to assume these documents are generated\nfrom this language model. As we did before, what we could do\nis just normalize the word frequency here to here and\nthen we will get this word distribution. Now the question is whether this\ndistribution is good for feedback. Well, you can imagine the top\nranked word would be what? What do you think? Well, those words would be common words. As we always see in a language model, the top ranked words are actually\ncommon words like the, a, etc. So it's not very good for feedback,\nbecause we would be adding a lot of such words to our query when we interpolate\nthis with the original query model. So this was not good, so\nwe need to do something. In particular, we are trying to\nget rid of those common words. And we have seen actually one way\nto do that by using background language model in the case of\nlearning the associations of words, the words that are related\nto the word computer. We could do that and that would be\nanother way to do this, but here we are going to talk about another approach\nwhich is a more principled approach. In this case, we're going to say well,\nyou said that there are common words here in these documents that should not\nbelong to this topic model, right? So now what we can do is to assume that,\nwell, those words are generated from\nbackground language model, so they will generate those words like the,\nfor example. And if we use maximum likelihood estimate, note that if all the words here\nmust be generated from this model, then this model is forced to assign\nhigh probabilities to a word like the, because it occurs so frequently here. Note that in order to reduce its\nprobability in this model, we have to have another model, which is this one,\nto help explain the word the here. And in this case, it's not appropriate to use the background\nlanguage model to achieve this goal because this model would assign high\nprobabilities to these common words. So in this approach, then, we assume this machine that was generating\nthese words would work as follows. We have a source control up here. Imagine we flip a coin here to\ndecide what distribution to use. With probability of lambda,\nthe coin shows up as head and we're going to use\nthe background language model. And we're going to do that in\nsample word from that model. With probability of 1 minus lambda,\nwe're going to decide to use a known topic model, here,\nthat we would like to estimate. And we're going to then\ngenerate a word here. If we make this assumption and this whole\nthing will be just one model, and we call this a mixture model because there are two\ndistributions that are mixed together. And we actually don't know when\neach distribution is used. So again,\nthink of this whole thing as one model, and we can still ask for words and it will\nstill give us a word in a random manner. And of course, which word will show up\nwill depend on both this distribution and that distribution. In addition,\nit would also depend on this lambda, because if you say lambda is very high and\nit's going to always use the background distribution,\nyou will get different words. Then if you say, well, lambda is\nvery small, we're going to use this. So all of these\nare parameters in this model. And then if you're thinking this way, basically we can do exactly\nthe same as what we did before. We're going to use maximum likelihood\nestimator to adjust this model, to estimate the parameters. Basically we're going to\nadjust this parameter so that we can best explain all the data. The difference now is that we are not\nasking this model a known to explain this. But rather we are going to ask this whole\nmodel, mixture model, to explain the data. Because it has got some help\nfrom the background model, it doesn't have to assign high\nprobabilities to words like the. As a result, it will then assign higher\nprobabilities to other words that are common here but\nnot having high probability here. So those would be common here. And if they're common, they would\nhave to have high probabilities, according to a maximum\nlikelihood estimate method. And if they are rare here,\nthen you don't get much help from this background model. As a result, this topic model\nmust assign high probabilities. So the high probability words,\naccording to the topic model, would be those that are common here but\nrare in the background. So this is basically a little bit\nlike an idea of weighting here. But this would allow us to achieve the\neffect of removing these topic words that are meaningless in the feedback. So mathematically, what we have is\nto compute the likelihood, again, local likelihood,\nof the feedback documents. And note that we also have another\nparameter, lambda here, but we assume that the lambda denotes\nthe noise in the feedback document. So we are going to,\nlet's say set this to a parameter. Let's say 50% of the words are noise or\n90% are noise. And this can then be\nassumed it will be fixed. If we assume this is fixed, then we only\nhave these probabilities as parameters, just like in the simple\nunigram language model. We have n parameters,\nn is the number of words. And then the likelihood\nfunction would look like this. It's very similar to the global\nlikelihood function we see before, except that inside the logarithm\nthere's a sum here. And this sum is because we\nconsider two distributions. And which one is used would depend on\nlambda, and that's why we have this form. But mathematically, this is the function\nwith theta as unknown variables. So this is just a function. All the other values are known except for\nthis guy. So we can then choose this\nprobability distribution to maximize this log likelihood, the same idea as the maximum likelihood\nestimate as a mathematical problem. We just have to solve this\noptimization problem. We essentially would try all\nthe theta values until we find one that gives this whole thing\nthe maximum probability. So it's a well-defined math problem. Once we have done that, we obtain this\ntheta F that can then be interpolated with original query model to the feedback. So here are some examples of\nthe feedback model learned from a web document collection. And we do pseudo-feedback we just\nuse the top ten documents and we use this mixture model. So the query is airport security. What we do is we first retrieve ten\ndocuments from the web database and this is of course pseudo-feedback. And then we're going to feed that\nmixture model to this ten document set. And these are the words\nlearned using this approach. This is the probability of a word given\nby the feedback model in both cases. So in both cases you can see the highest probability words include the very\nrelevant words to the query. So airport security, for example, these query words still show up as high\nprobabilities in each case naturally, because they occur frequently\nin the top ranked documents. But we also see beverage,\nalcohol, bomb, terrorist, etc. So these are relevant to this topic,\nand they, if combined with original query, can help\nus much more accurately on documents. And also they can help us bring up\ndocuments that only mention some of these other words, maybe, for example,\njust airport and then bomb, for example. So this is how pseudo-feedback works. It shows that this model really works and\npicks up some related words to the query. What's also interesting is that if\nyou look at the two tables here and you compare them,\nthen you'll see, in this case, when lambda is set to a small value,\nthen we'll see some common words here. And that means, well,\nwe don't use the background model often. Remember, lambda confuses the probability\nof using background model to generate the text. If we don't rely much on background model, we still have to use this topic model\nto account for the common words. Whereas if we set lambda\nto a very high value, we will use the background model\nvery often to explain these words. Then there's no burden on\nexpanding those common words in the feedback documents\nby the topic model. So as a result, the topic model\nhere is very discriminative. It contains all the relevant\nwords without common words. So this can be added to the original\nquery to achieve feedback. So to summarize, in this lecture we have talked about\nthe feedback in language model approach. In general,\nfeedback is to learn from examples. These examples can be assumed examples,\ncan be pseudo-examples, like assume the top ten documents\nthat are assumed to be relevant. They could be based on user interactions, like feedback based on clickthroughs or\nimplicit feedback. We talked about the three major\nfeedback scenarios, relevance feedback, pseudo feedback, and implicit feedback. We talked about how to use Rocchio to\ndo feedback in vector space model and how to use query model estimation for\nfeedback in language model. And we briefly talked about\nthe mixture model and the basic idea. There are many other methods. For example, the relevance model is a very effective\nmodel for estimating query model. So you can read more about these\nmethods in the references that are listed at the end of this lecture. So there are two additional readings here. The first one is a book that\nhas a systematic review and discussion of language models for\ninformation retrieval. And the second one is a important research paper that's about relevance\nbased language models, and it's a very effective way\nof computing query model. [MUSIC]",
 "04_lesson-5-4-web-search-introduction-web-crawler.en.txt": "This lecture is about Web Search. In this lecture,\nwe're going to talk about one of the most important applications of\ntext retrieval, web search engines. So let's first look at some\ngeneral challenges and opportunities in web search. Now, many informational\nretrieval algorithms had been developed\nbefore the web was born. So when the web was born,\nit created the best opportunity to apply those algorithms to major application\nproblem that everyone would care about. So naturally, there have to be some\nfurther extensions of the classical search algorithms to address new\nchallenges encountered in web search. So here are some general challenges. First, this is a scalability challenge. How to handle the size of the web and ensure completeness of\ncoverage of all information. How to serve many users quickly and\nby answering all their queries. And so that's one major challenge and before the web was born the scale\nsearch was relatively small. The second problem is that there's\nno quality information and there are often spams. The third challenge is\nDynamics of the Web. The new pages are constantly create and\nsome pages may be updated very quickly, so it makes it harder to\nkeep it indexed fresh. So these are some of the challenges\nthat we have to solve in order to deal with high quality web searching. On the other hand there are also some\ninteresting opportunities that we can leverage to include the search results. There are many additional heuristics,\nfor example, using links that we can\nleverage to improve scoring. Now everything that we talked about\nsuch as the vector space model are general algorithms. They can be applied to any search\napplications, so that's the advantage. On the other hand, they also don't take\nadvantage of special characteristics of pages or documents in the specific\napplications, such as web search. Web pages are linked with each other,\nso obviously, the linking is something\nthat we can also leverage. So, because of these challenges and\nopportunities and there are new techniques that have been developed for\nweb search or due to need for web search. One is parallel indexing and searching and this is to address\nthe issue of scalability. In particular, Google's imaging of\nmap reduce is very influential and has been very helpful in that aspect. Second, there are techniques\nthat are developing for addressing the problem of spams,\nso spam detection. We'll have to prevent those spam\npages from being ranked high. And there are also techniques\nto achieve robust ranking. And we're going to use a lot\nof signals to rank pages, so that it's not easy to spam the search\nengine with a particular trick. And the third line of techniques is link analysis and these are techniques that can allow us to improve such results\nby leveraging extra information. And in general in web searching,\nwe're going to use multiple features for ranking not just for link analysis. But also exploring all kinds\nof crawls like the layout or anchor text that describes\na link to another page. So, here's a picture showing\nthe basic search engine technologies. Basically, this is the web on the left and\nthen user on the right side and we're going to help this user to get\nthe access for the web information. And the first component is a Crawler that\nwould crawl pages and then the second component is Indexer that would take\nthese pages create the inverted index. The third component there is a Retriever\nand that would use inverted index to answer user's query by talking\nto the user's browser. And then the search results will be given\nto the user and when the browser would show those results, it allows\nthe user to interact with the web. So, we're going to talk about\neach of these components. First of all, we're going to talk about\nthe crawler, also called a spider or software robot that would do something\nlike crawling pages on the web. To build a toy crawler is relatively easy, because you just need to start\nwith a set of seed pages. And then fetch pages from the web and\nparse these pages and figure out new links. And then add them to the priority que and\nthen just explore those additional links. But to be able to real crawler\nactually is tricky and there are some complicated issues\nthat we have to deal with. For example robustness,\nwhat if the server doesn't respond, what if there's a trap that generates\ndynamically generated webpages that might attract your crawler to\nkeep crawling on the same side and to fetch dynamic generated pages? The results of this issue\nof crawling courtesy and you don't want to overload one particular\nserver with many crawling requests and you have to respect the robot\nexclusion protocol. You also need to handle different\ntypes of files, there are images, PDF files,\nall kinds of formats on the web. And you have to also\nconsider URL extension, so sometimes those are CGI scripts and\nthere are internal references, etc, and sometimes you have\nJavaScripts on the page and they also create challenges. And you ideally should also recognize\nredundant pages because you don't have to duplicate those pages. And finally, you may be interested\nin the discover hidden URLs. Those are URLs that may not be linked\nto any page, but if you truncate the URL to a shorter path, you might\nbe able to get some additional pages. So what are the Major Crawling Strategies? In general, Breadth-First is most common because\nit naturally balances the sever load. You would not keep probing a particular\nserver with many requests. Also parallel crawling is very\nnatural because this task is very easy to parallelize. And there is some variations\nof the crawling task, and one interesting variation\nis called a focused crawling. In this case, we're going to crawl just\nsome pages about a particular topic. For example,\nall pages about automobiles, all right. And this is typically going to\nstart with a query, and then you can use the query to get some\nresults from a major search engine. And then you can start it with those\nresults and then gradually crawl more. The one channel in crawling, is you will find the new\nchannels that people created and people probably are creating\nnew pages all the time. And this is very challenging if\nthe new pages have not been actually linked to any old pages. If they are, then you can probably find\nthem by re-crawling the old pages, so these are also some interesting\nchallenges that have to be solved. And finally, we might face the scenario\nof incremental crawling or repeated crawling, right. Let's say,\nif you want to build a web search engine, and you first crawl a lot\nof data from the web. But then,\nonce you have cracked all the data, in the future you just need\nto crawl the updated pages. In general, you don't have to\nre-crawl everything, right? It's not necessary. So in this case, your goal is to\nminimize the resource overhead by using minimum resources\nto just the update pages. So, this is actually a very\ninteresting research question here, and this is a open research question,\nin that there aren't many standard algorithms established yet\nfor doing this task. But in general, you can imagine,\nyou can learn, from the past experience. So the two major factors that\nyou have to consider are, first will this page\nbe updated frequently? And do I have to quote this page again? If the page is a static page and\nthat hasn't being changed for months, you probably don't have to re-crawl it\neveryday because it's unlikely that it will changed frequently. On the other hand, if it's a sports score\npage that gets updated very frequently and you may need to re-crawl it and\nmaybe even multiple times on the same day. The other factor to consider is,\nis this page frequently accessed by users? If it is, then it means that\nit is a high utility page and then thus it's more important to\nensure such a page to refresh. Compared with another page that has\nnever been fetched by any users for a year, then even though that\npage has been changed a lot then. It's probably not that necessary to\ncrawl that page or at least it's not as urgent as to maintain the freshness\nof frequently accessed page by users. So to summarize, web search is one of\nthe most important applications of text retrieval and there are some new\nchallenges particularly scalability, efficiency, quality information. There are also new opportunities\nparticularly rich link information and layout, etc. A crawler is an essential component\nof web search applications and in general, you can find two scenarios. One is initial crawling and\nhere we want to have complete crawling of the web if you are doing\na general search engine or focused crawling if you want to just\ntarget as a certain type of pages. And then, there is another scenario that's\nincremental updating of the crawl data or incremental crawling. In this case,\nyou need to optimize the resource, try to use minimum resource\nto get the [INAUDIBLE] [MUSIC]",
 "05_lesson-5-5-web-indexing.en.txt": "[SOUND] This lecture is about the Web Indexing. In this lecture, we will continue\ntalking about the Web Search and we're going to talk about how\nto create a Web Scale Index. So once we crawl the web,\nwe've got a lot of web pages. The next step is to use the indexer\nto create the inverted index. In general, we can use the same\ninformation retrieval techniques for creating an index and that is what we\ntalked about in previous lectures, but there are there are new\nchallenges that we have to solve. For web scale indexing, and the two main\nchallenges are scalability and efficiency. The index would be so large, that it cannot actually fit into\nany single machine or single disk. So we have to store the data\non virtual machines. Also, because the data is so\nlarge, it's beneficial to process the data in parallel, so\nthat we can produce index quickly. Now to address these challenges,\nGoogle has made a number of innovations. One is the Google File System that's\na general File system, that can help programmers manage files stored\non a cluster of machines. The second is MapReduce. This is a general software framework for\nsupporting parallel computation. Hadoop is the most well known open\nsource implementation of MapReduce. Now used in many applications. So, this is the architecture\nof the google file system. It uses a very simple centralized management mechanism to manage\nall the specific locations of. Files, so\nit maintains the file namespace and look up a table to know where\nexactly each file is stored. The application client will then\ntalk to this GFS master, and that obtains specific locations of\nthe files they want to process. And once the GFS file kind obtained\nthe specific location about the files, then the application client can talk\nto the specific servers whether data actually sits directly, so\nyou can avoid involving other node. In the network. So when this file system stores\nthe files on machines, the system also with great fixed sizes of chunks, so\nthe data files are separated into. Many chunks. Each chunk is 64 MB, so it's pretty big. And that's appropriate for\nlarge data processing. These chunks are replicated\nto ensure reliability. So this is something that the programmer\ndoesn't have to worry about, and it's all taken care\nof by this file system. So from the application perspective, the programmer would see this\nas if it's a normal file. And the programmer doesn't have to\nknow where exactly it is stored and can just invoke high level. Operators to process the file. And another feature is that the data\ntransfer is directly between application and chunk servers. So it's efficient in this sense. On top of the Google file system, Google also proposed MapReduce as a general\nframework for parallel programming. Now, this is very useful to support\na task like building inverted index. And so, this framework is, Hiding a lot of low-level\nfeatures from the program. As a result, the programmer can make\na minimum effort to create an application that can be run a large\ncluster in parallel. So some of the low level details\nare hidden in the framework including the specific and network communications or\nload balancing or where the task are executed. All these details are hidden\nfrom the programmer. There is also a nice feature which\nis the built in fault tolerance. If one server is broken, the server is down, and\nthen some tasks may not be finished. Then the MapReduce mapper will know\nthat the task has not been done. So it automatically dispatches a task\non other servers that can do the job. And therefore, again the program\ndoesn't have to worry about that So here's how MapReduce works. The input data would be separated\ninto a number of key value pairs. Now what exactly is in the value\nwould depend on the data and it's actually a fairly general framework\nto allow you to just partition the data into different parts and each part\ncan be then processed in parallel. Each key value pair would be and\nsend it to a map function. The program was right the map function,\nof course. And then the map function will\nprocess this Key Value pair and then generate a number of\nother Key Value pairs. Of course, the new key is usually\ndifferent from the old key that's given to the map as input. And these key value pairs\nare the output of the map function and all the outputs of all the map\nfunctions would be then collected, and then there will be for\nthe sorting based on the key. And the result is that,\nall the values that are associated with the same key will be\nthen grouped together. So now we've got a pair of of a key and\nseparate values attached to this key. So this would then be sent\nto a reduce function. Now, of course, each reduce function\nwill handle a different key, so we will send these output values to multiple reduce functions\neach handling a unique key. A reduce function would then\nprocess the input, which is a key in a set of values to produce\nanother set of key values as the output. So these output values would\nbe then corrected together to form the final output. And so, this is the general\nframework of MapReduce. Now the programmer only needs to write\nthe Map function and the Reduce function. Everything else is actually taken\ncare of by the MapReduce framework. So you can see the program really\nonly needs to do minimum work. And with such a framework, the input data\ncan be partitioned into multiple parts, which is processing parallel first by map,\nand then being the process after\nwe reach the reduced stage. The much more reduced if I'm\n[INAUDIBLE] can also further process the different keys and\ntheir associated values in parallel. So it achieves some, it achieves the purpose of parallel\nprocessing of a large data set. So let's take a look at a simple example. And that's Word Counting. The input is containing words, and the output that we want to generate is\nthe number of occurrences of each word. So it's the Word Count. We know this kind of counting\nwould be useful to, for example, assess the popularity of a word in\na large collection and this is useful for achieving a factor of IDF wading for\nsearch. So how can we solve this problem? Well, one natural thought is that,\nwell this task can be done in parallel by simply counting\ndifferent parts of the file in parallel, and then in the end we just\ncombine all the counts. And that's precisely the idea of\nwhat we can do with MapReduce. We can parallelize on\nlines in this input file. So more specifically, we can assume\nthe input to each map function is a key value pair that represents the line\nnumber and the string on that line. So the first line, for\nexample, has a key of one and that is another word by word and\njust the four words on that line. So this key value pair would\nbe sent to a Map Function. The Map Function then would just\ncount the words in this line. And in this case,\nof course there are only four words. Each world gets a count of one and these are the output that you see here\non this slide from this map function. So the map function is really\nvery simple if you look at what the pseudocode looks\nlike on the right side, you see it simply needs to iterate\nall the words and this line. And then just collect the function which means it would then send the word\nand the count to the collector. The collector would then try to\nsort all these key value pairs from different Map Functions, right? So the function is very simple and\nthe programmer specifies this function as a way to\nprocess each part of the data. Of course, the second line will be\nhandled by a different Map Function which we will produce a single output. Okay, now the output from the map\nfunctions will be then and send it to a collector and the collector\nwould do the internal grouping or sorting. So at this stage, you can see,\nwe have collected a match for pairs. Each pair is a word and\nits count in a line. So, once we see all these pairs. Then we can sort them based on the key,\nwhich is the word. So we will collect all the counts\nof a word, like bye here, together. And similarly, we do that for other words. Like Hadoop, Hello, etc. So each word now is attached to\na number of values, a number of counts. And these counts represent the occurrences\nto solve this word in different lights. So now we have got a new pair of a key and\na set of values, and this pair will then be fed into reduce\nfunction, so the reduce function now would have to finish the job of counting\nthe total occurrences of this word. Now, it has all ready got all\nthese puzzle accounts, so all it needs to do is\nsimply to add them up. So the reduce function here\nis very simple, as well. You have a counter, and\nthen iterate all the other words. That you'll see in this array. And that,\nyou just accumulate accounts, right? And then finally, you output the P and\nthe proto account. And that's precisely what we want as\nthe output of this whole program. So you can see,\nthis is all ready very similar to. To building an Invert index. And if you think about it,\nthe output here is index. And we have already got a dictionary,\nbasically. We have got the count. But what's missing is\nthe document the specific frequency counts of words\nin those documents. So we can modify this slightly to\nactually be able to index in parallel, so here's one way to do that. So in this case, we can assume the input\nfrom Map Function is a pair of a key which denotes the document ID,\nand the value denoting the screen for that document,\nso it's all the words in that document. And so, the map function would do\nsomething very similar to what we have seen in the word campaign example. It simply groups all the counts of\nthis word in this document together. And it would then generate\na set of key value pairs. Each key is a word, and the value is the count of this word in\nthis document plus the document ID. Now, you can easily see why we need to\nadd document ID here, because later in inverted index, we would like to\nkeep this formation, so the Map Function should keep track of it, and this can then\nbe sent to the reduce function later. Now similarly another document D2\ncan be processed in the same way. So in the end, again, there is a sorting\nmechanism that would group them together. And then we will have just a key,\nlike a java, associated with all the documents\nthat match this key. Or all the documents where java occurred. And the counts, so\nthe counts of java in those documents. And this will be collected together. And this will be, so\nfed into the reduce function. So now you can see the reduce function\nhas already got input that looks like an inverted index entry. So it's just the word and all\nthe documents that contain the word and the frequencies of the word\nin those documents. So all you need to do is\nsimply to concatenate them into a continuous chunk of data. And this can be done\nwritten to a file system. So basically the reduce function\nis going to do very minimal. Work. And so, this is a pseudo-code for [INAUDIBLE] that's construction. Here we see two functions,\nprocedure Map and procedure Reduce. And a programmer would specify these two\nfunctions to program on top of map reduce. And you can see basically they\nare doing what I just described. In the case of map, it's going to count the occurrences of a word\nusing the AssociativeArray. And it would output all the counts\ntogether with the document ID here. So, this is the reduce function,\non the other hand, simply concatenates all the input\nthat it has been given, and then put them together as\none single entry for this key. So this is a very simple\nMapReduce function, yet it would allow us to construct an inverted\nindex at very large scale, and the data can be processed\nby different machines. And program doesn't have to\ntake care of the details. So this is how we can do parallel\nindex construction for web search. So to summarize, web scale indexing requires some\nnew techniques that go beyond the. Standard traditional indexing techniques. Mainly, we have to store\nindex on multiple machines. And this is usually done by using a filing\nsystem, like a Google file system. But this should be through a file system. And secondly, it requires creating\nan index an parallel, because it's so large and takes long time to create\nan index for all the documents. So if we can do it in parallel,\nit will be much faster and this is done by using\nthe MapReduce framework. Note that both the GFS and\nMapReduce frameworks are very general, so they can also support\nmany other applications. [MUSIC]",
 "06_lesson-5-6-link-analysis-part-1.en.txt": "[SOUND] This lecture is about link analysis for web search. In this lecture, we're going to talk\nabout the web search and particularly, focusing on how to do link analysis and\nuse the results to improve search. The main topic of this lecture is to look\nat the ranking algorithms for Web Search. In the previous lecture we talked\nabout how to create index. Now that we have index, we want to see\nhow we can improve ranking of Pages. The web. Now standard IR models,\ncan be also applied here. In fact,\nthey are important building blocks, for, improve, for supporting web search. But they aren't sufficient. And mainly for the following reasons. First, on the web, we tend to have\nvery different information needs, for example, people might search for\na webpage, or an entry page. And this is different from\nthe traditional library search, where people are primarily interested\nin collecting literature Information. So this kind of query is often\ncalled a navigational queries. The purpose is to navigate into\na particular type of the page. So for such queries we might benefit\nfrom using link information. Secondly, documents have additional\ninformation and on the web pages, are web format,\nthere are a lot of other clues, such as the layout, the title,\nor link information again. So this has provided opportunity to use extra context information of\nthe document to improve the scoring. And finally,\ninformation quality varies a lot. That means we have to consider\nmany factors to improve the range in the algorithm. This would give us a more robust way\nto rank pages, making it harder for any spammer to just manipulate the one\nsignal to improve the ranking of a page. So as a result, people have made a number of major\nextensions to the ranking algorithms. One line is to exploit\nlinks to improve scoring. And that's the main topic of this lecture. People have also proposed algorithms to\nexploit the loudest, they are implicit. Feedback information the form of\nclick throughs and that's of course in the category of feedback techniques and\nmachine all is often used there. In general in web search the ranking\nalgorithms are based on machine learning algorithms to combine\nall kinds of features. Many of them are based on\nthe standard of virtual models such as BM25 that we talked about [INAUDIBLE]\nto score different parts of documents or to provide additional features\nbased on content matching, but link information\nis also very useful so they provide additional scoring signals. So let's look at links in\nmore detail on the web. So this is a snapshot of some\npart of the web, let's say. So we can see there are many links that\nlink the different pages together. And in this case, you can also\nlook at the center here, there is a description of a link that's pointing\nto the document on the right side. Now, this description text\nis called anchor text. Now if you think about this text,\nit's actually quite useful because it provides some extra\ndescription of that page be points with. So for example, if someone wants\nto bookmark Amazon.com front page the person might say the biggest\nonline bookstore and then the link to Amazon, right? So, the description here after is very\nsimilar to what the user would type in the query box when they are looking for\nor such a page. And that's why it's very useful for\nmanaging pages. Suppose someone types in\nthe query like online bookstore or biggest online bookstore. All right the query would match\nthis anchor text in the page here. And then this actually\nprovides evidence for matching the page that's being\npointed to that is the Amazon. a entry page. So if you match anchor text that\ndescribes an anchor to a page, actually that provides good evidence for\nthe elements of the page being pointed to. So anchor text is very useful. If you look at the bottom part of this\npicture you can also see there are some patterns of some links and these links\nmight indicate the utility of a document. So for example, on the right side you'll see this\npage has received the many inlinks. Now that means many other pages\nare pointing to this page. This shows that this page is quite useful. On the left side you can see this\nis another page that points to many other pages. So this is a director page\nthat would allow you to actually see a lot of other pages. So we can call the first\ncase authority page and the second case half page, but this means\nthe link information can help intuit. One is to provide extra text for matching. The other is to provide some\nadditional scores for the webpage to characterize how likely a page is\na hub, how likely a page is a authority. So people then of course and proposed\nideas to leverage this link information. Google's PageRank which was the main\ntechnique that they used in early days is a good example and\nthat is an algorithm to capture page and popularity, basically to score authority. So the intuitions here are links\nare just like citations in literature. Now think about one page\npointing you to another page, this is very similar to one\npaper citing another paper. So, of course then,\nif a page is cited often, then we can assume this page\nto be more useful in general. So that's a very good intuition. Now PageRank is essentially to take\nadvantage of this Intuition to implement with the principal approach. Intuitively, it is essentially doing\ncitation counting or in link counting. It just improves the simple\nidea in two ways. One it will consider indirect citations. So that means you don't just look\nat how many in links you have. You also look at what are those\npages that are pointing to you. If those pages themselves have a lot\nof in-links, that means a lot. In some sense,\nyou will get some credit from that. But if those pages that\nare pointing to you are not being pointed to by other pages they\nthemselves don't have many in-links, then well, you don't get that much. So that's the idea of\ngetting indirect citation. All right, so you can also understand this idea by\nlooking at again the research papers. If you're cited by let's say ten papers,\nand those ten papers are just workshop papers or some papers\nthat are not very influential, right? So although you've got ten in-links,\nand that's not as good as if you are cited by ten papers that themselves\nhave attracted a lot of other citations. And so in this case where we would\nlike to consider indirect links and page does that. The other idea is it's\ngood to pseudo citations. Assume that basically every page is having\na number zero pseudo citation count. Essentially you are trying to\nimagine there are many virtual links that will link all\nthe pages together so that you actually get the pseudo\ncitations from everyone. The reason why they want to do that. Is this will allow them\nto solve the problem elegantly with linear algebra technique. So, I think maybe the best\nway to understand the PageRank is to think\nof this as through computer probability of random surfer\nvisiting every webpage. [MUSIC]",
 "07_lesson-5-7-link-analysis-part-2.en.txt": "[MUSIC] So let's take a look at this in detail. So in this random surfing\nmodel at any page would assume random surfer would choose\nthe next page to visit. So this is a small graph here. That's of course, over simplification\nof the complicated web. But let's say there are four\ndocuments here, d1, d2, d3 and d4. And let's assume that a random surfer or\nrandom walker can be any of these pages. And then the random\nsurfer could decide to, just randomly jumping to any page or follow a link and\nthen visit the next page. So if the random surfer is at d1, then there is some probability that\nrandom surfer will follow the links. Now there are two outlinks here,\none is pointing to d3, the other is pointing to d4. So the random surfer could pick any\nof these two to reach d3 and d4. But it also assumes that the random so\nfar might get bore sometimes. So the random surfing which decide\nto ignore the actual links and simply randomly jump\ninto any page in the web. So if it does that, it would be able\nto reach any of the other pages even though there's no link you actually,\nyou want from that page. So this is to assume that\nrandom surfing model. Imagine a random surfer is\nreally doing surfing like this, then we can ask the question how\nlikely on average the surfer would actually reach a particular\npage like a d1, a d2, or a d3. That's the average probability of\nvisiting a particular page and this probability is precisely\nwhat a page ranker computes. So the page rank score of\nthe document is the average probability that the surfer\nvisits a particular page. Now intuitively, this would basically\ncapture the inlink account, why? Because if a page has a lot of inlinks, then it would have a higher\nchance of being visited. Because there will be more\nopportunities of having the server to follow a link to come to this page. And this is why the random surfing model actually captures the ID\nof counting the inlinks. Note that it also considers\nthe interacting links, why? Because if the page is that point then\nyou have themselves a lot of inlinks. That would mean the random surfer would\nvery likely reach one of them and therefore, it increase\nthe chance of visiting you. So this is just a nice way to capture\nboth indirect and a direct links. So mathematically, how can we compute this\nproblem in a day in order to see that, we need to take a look at how this\nproblem there is a computing. So first of all let's take a look\nat the transition metrics here. And this is just metrics with\nvalues indicating how likely the random surfer would go\nfrom one page to another. So each rule stands for a starting page. For example, rule one would\nindicate the probability of going to any of the other four pages from d1. And here we see there are only\n2 non 0 entries which is 1/2. So this is because if you look at\nthe graph d1 is pointing to d3 and d4. There is no link from d1 or d2. So we've got 0s for the first 2 columns and 0.5 for d3 and d4. In general, the M in this matrix, M sub ij is the probability\nof going from di to dj. And obviously for each rule,\nthe values should sum to 1, because the surfer would have to go to\nprecisely one of these other pages. So this is a transition metric. Now how can we compute the probability\nof a surfer visiting a page? Well if you look at the surf\nmodel then basically, we can compute the probability\nof reaching a page as follows. So here on the left hand side,\nyou see it's the probability visiting page dj at time plus 1,\nso it's the next time point. On the right hand side, you can see\nthe equation involves the probability of at page di at time t. So you can see the subscript\nin that t here, and that indicates that's the probability that\nthe server was at a document at time t. So the equation basically,\ncaptures the two possibilities of reaching\nat dj at the time t plus 1. What are these two possibilities? Well one is through random surfing and one is through following a link,\nas we just explained. So the first part captures the probability that the random surfer would reach\nthis page by following a link. And you can see the random\nsurfer chooses this strategy with probability 1 minus\nalpha as we assume. And so\nthere is a factor of 1 minus alpha here. But the main party is realist\nsum over all the possible pages that the surfer could have been at time t. There are n pages so\nit's a sum over all possible n pages. Inside the sum is a product\nof two probabilities. One is the probability that the surfer was at di at time t, that's p sub t of di. The other is the transition\nprobability from di to dj. And so in order to reach this dj page, the surfer must first be at di at time t. And then also, would also have to\nfollow the link to go from di to dj. So the probability is the probability\nof being at di at time t multiplied by the probability of going from that\npage to the target page, dj here. The second part is a similar sum, the only\ndifference is that now the transition probability is a uniform\ntransition probability. 1 over n and\nthis part of captures is the probability of reaching this page\nthrough random jumping. So the form is exactly the same and\nthis also allows us to see on why PageRank is essentially assumed\na smoothing of the transition matrix. If you think about this 1 over n as\ncoming from another transition matrix that has all the elements being\n1 over n in uniform matrix. Then you can see very clearly\nessentially we can merge the two parts, because they are of the same form. We can imagine there's a different\nmetrics that's combination of this m and that uniform metrics where\nevery m is 1 over n. And in this sense PageRank uses\nthis idea of smoothing and ensuring that there's no zero entry\nin such as transition matrix. Now of course this is the time dependent\nthe calculation of the probabilities. Now we can imagine, if we'll compute\nthe average of the probabilities, the average of probabilities probably\nwith the sets of file this equation without considering the time index. So let's drop the time index and\njust assume that they will be equal. Now this would give us any equations,\nbecause for each page we have such equation. And if you look at the what\nvariables we have in these equations there are also precisely n variables. So this basically means,\nwe now have a system of n equations with n variables and\nthese are linear equations. So basically, now the problem boils\ndown to solve this system of equations. And here, I also show\nthe equations in the metric form. It's the vector p here equals a matrix or the transpose of the matrix here and\nmultiplied by the vector again. Now, if you still remember some knowledge\nthat you've learned from linear algebra and then you will realize, this is\nprecisely the equation for eigenvector. When multiply the metrics by this vector,\nyou get the same value as this matter and this can be solved by\nusing iterative algorithm. So because the equations here on the back are basically\ntaken from the previous slide. So you'll see the relation between the\npage that ran sports on different pages. And this iterative approach or\npower approach, we simply start with s\nrandomly initialized vector p. And then we repeatedly\njust update this p by multiplying the metrics\nhere by this p factor. I also show a concrete example here. So you can see this now. If we assume alpha is 0.2, then with the example that\nwe show here on the slide, we have the original\ntransition matrix is here. That includes the graph, the actual links\nand we have this smoothing transition metrics, uniform transition metrics\nrepresenting random jumping. And we can combine them together with\na liner interpolation to form another metric that would be like this. So essentially, we can imagine now the web looks like\nthis and can be captured like that. They're all virtual links\nbetween all the pages now. The page we're on now would just\ninitialize the p vector first and then just computed the updating of this p vector by using this\nmetrics multiplication. Now if you rewrite this\nmetric multiplication in terms of individual equations,\nyou'll see this. And this is basically,\nthe updating formula for this particular pages and page score. So you can also see if you want to compute\nthe value of this updated score for d1. You basically multiply\nthis rule by this column, and we'll take the third\nproduct of the two. And that will give us the value for\nthis value. So this is how we updated the vector\nwe started with an initial values for these guys for this. And then we just revise\nthe scores which generate a new set of scores and\nthe updating formula is this one. So we just repeatedly apply this and\nhere it converges. And when the matrix is like this,\nwhere there's no 0 values and it can be guaranteed to converge. And at that point the we will just have\nthe PageRank scores for all the pages. We typically go to sets of\ninitial values just to 1 over n. So interestingly,\nthis updating formula can be also interpreted as propagating\nscores on the graph, can you see why? Or if you look at this formula and\nthen compare that with this graph and can you imagine,\nhow we might be able to interpret this as essentially propagating\nscores over the graph. I hope you will see that indeed, we can imagine we have values\ninitialized on each of these pages. So we can have values here and\nsay, that's a 1 over 4 for each. And then we're going to use these\nmetrics to update this the scores. And if you look at the equation here\nthis one, basically we're going to combine the scores of the pages that\npossibly would lead to reaching this page. So we'll look at all the pages\nthat are pointing to this page and then combine this score and propagate the\nsum of the scores to this document, d1. To look at the scores that we present\nthe probability that the random surfer would be visiting the other\npages before it reached d1. And then just do\nthe propagation to simulate the probability of reaching this page, d1. So there are two interpretations here. One is just the matrix multiplication. We repeat the multiplying\nthat by these metrics. The other is to just think\nof it as a propagating these scores repeatedly on the web. So in practice, the combination of\nPageRank score is actually efficient. Because the matrices is fast and there\nare some, ways we transform the equation. So that you avoid actually\nliterally computing the values for all those elements. Sometimes you may also normalize the\nequation and that will give you a somewhat different form of the equation, but\nthen the ranking of pages will not change. The results of this potential\nproblem of zero-outlink problem. In that case, if a page does not have\nany outlink then the probability of these pages would not sum to 1. Basically, the probability of reaching the\nnext page from this page would not sum to 1, mainly because we have lost\nsome probability to mass. One would assume there's some probability\nthat the surfer would try to follow the links, but\nthen there is no link to follow. And one possible solution is simply to use\na page that is specific damping factor, and that could easily fix this. Basically, that's to say alpha would\nbe 1.0 for a page with no outlink. In that case,\nthe surfer would just have to randomly jump to another page\ninstead of trying to follow a link. There are many extensions of PageRank, one\nextension is to topic-specific PageRank. Note that PageRank doesn't merely\nuse the query information. So we can make PageRank specific however. So for example,\nat the top of a specific page you rank, we can simply assume\nwhen the surfer is bored. The surfer is not randomly\njumping to any page on the web. Instead, he's going to jump to only those\npages that are relevant to our query. For example, if the query is not sports\nthen we can assume that when it's doing random jumping, it's going\nto randomly jump to a sports page. By doing this, then we can buy\na PageRank through topic and sports. And then if you know the current\ntheory is about sports, and then you can use this specialized\nPageRank score to rank documents. That would be better than if you\nuse the generic PageRank score. PageRank is also a channel that can be\nused in many other applications for network analysis particularly for\nexample, social networks. You can imagine if you compute\nthe PageRank scores for social network, where a link\nmight indicate a friendship or a relation, you would get some\nmeaningful scores for people [MUSIC]",
 "08_lesson-5-8-link-analysis-part-3.en.txt": "[SOUND]\nSo we talked about PageRank as\na way to capture the assault. Now, we also looked at some other examples\nwhere a hub might be interesting. So there is another algorithm called HITS,\nand that going to compute the scores for\nauthorities and hubs. The intuitions are pages that are widely\ncited are good authorities and whereas pages that cite many\nother pages are good hubs. I think that the most interesting\nidea of this algorithm HITS, is it's going to use\na reinforcement mechanism to kind of help improve the scoring for\nhubs and the authorities. And so here's the idea, it was assumed that good\nauthorities are cited by good hubs. That means if you are cited by many\npages with good hub scores then that inquiry says, you're an authority. And similarly, good hubs are those\nthat point at good authorities. So if you pointed to a lot\nof good authority pages, then your hubs score would be increased. So then you will have literally reinforced\neach other, because you have pointed so some good hubs. And so you have pointed to some good\nauthorities to get a good hubs score, whereas those authority\nscores would be also improved because they\nare pointing to by a good hub. And this is algorithms is also general it\ncan have many applications in graph and network analysis. So just briefly, here's how it works. We first also construct a matrix, but this\ntime we're going to construct an adjacent matrix and\nwe're not going to normalize the values. So if there's a link there's a 1,\nif there's no link that's 0. Again, it's the same graph. And then we're going to\ndefine the hubs score of page as the sum of the authority scores of\nall the pages that it appoints to. So whether you are hub, really depends on whether you are pointing\nto a lot of good authority pages. That's what it says in the first equation. In the second equation,\nwe define the authorities of a page as a sum of the hub scores of all\nthose pages that appoint to you. So whether you are good authority\nwould depend on whether those pages that are pointing\nto you are good hubs. So you can see this forms\niterative reinforcement mechanism. Now, these three questions can be\nalso written in the metrics format. So what we get here is then the hub\nvector is equal to the product of the adjacency matrix and\nthe authority vector, and this is basically the first equation. And similarly, the second equation\ncan be returned as the authority vector is equal to the product of\na transpose multiplied by the hub vector. Now, these are just different ways\nof expressing these equations. But what's interesting is that\nif you look at the matrix form, you can also plug in the authority\nequation into the first one. So if you do that, you have actually\neliminated the authority vector completely and you get the equations\nof only hubs scores. The hubs score vector is\nequal to a multiplied by a transpose multiplied\nby the hub score again. Similarly, we can do a transformation\nto have equation for just the authorities also. So although we frame the problem\nas computing hubs and authorities, we can actually eliminate one of them to\nobtain equation just for one of them. Now, the difference between this and page\nrandom is that now the matrix is actually a multiplication of the adjacency\nmatrix and it's transpose. So this is different from page rank. But mathematically, then we will\nbe computing the same problem. So in HITS,\nwe typically would initialize the values. Let's say, 1 for all these values, and then we would iteratively apply\nthese equations, essentially. And this is equivalent to multiply\nthat by the metrics a and a transpose. So the arrows of these is exactly\nthe same in the PageRank. But here because the adjacency\nmatrix is not normalized. So what we have to do is after each\niteration we're going to normalize, and this would allow us to\ncontrol the growth of value. Otherwise they would grow larger and\nlarger. And if we do that, and\nthat will basically get HITS. That was the computer, the hubs scores,\nand authority scores for all the pages. And these scores can then be used in\nbranching just like the PageRank scores. So to summarize in this lecture, we have\nseen that link information's very useful. In particular,\nthe anchor text is very useful to increase the text\nrepresentation of a page. And we also talk about the PageRank and page anchor as two major\nlink analysis algorithms. Both can generate scores for web pages\nthat can be used in the ranking function. Note that PageRank and\nthe HITS are also very general algorithms. So they have many applications in\nanalyzing other graphs or networks. [MUSIC]",
 "01_lesson-6-1-learning-to-rank-part-1.en.txt": "[MUSIC] This lecture is about\nthe Learning to Rank. In this lecture, we are going to\ncontinue talking about web search. In particular we're going to talk\nabout the using machine learning to combine different features\nto improve the ranking function. So the question that we address in\nthis lecture is how we can combine many features to generate a single ranking\nfunction to optimize search results? In the previous lectures we have talked\nabout a number of ways to rank documents. We have talked about some retrieval\nmodels like a BM25 or Query Light Code. They can generate a based this course for\nmatching documents with a query. And we also talked about the link\nbased approaches like page rank that can give additional scores\nto help us improve ranking. Now the question now is,\nhow can we combine all these features and potentially many other\nfeatures to do ranking? And this will be very useful for\nranking webpages, not only just to improve accuracy, but also to improve\nthe robustness of the ranking function. So that it's not easy for\na spammer to just perturb a one or a few features to promote a page. So the general idea of learning\nto rank is to use machine learning to combine this\nfeatures to optimize the weights on different features to generate\nthe optimal ranking function. So we will assume that the given\na query document pair Q and D, we can define a number of features. And these features can vary from\ncontent based features such as a score of the document with\nrespect to the query according to a retrieval function such as BM25 or\nQuery Light Hold of punitive commands from a machine or\nPL2 etcetera. It can also be a link based score like or\npage rank score like. It can be also application of retrieval\nmodels to the ink text of the page. Those are the types of descriptions\nof links that point to this page. So, these can all the clues whether\nthis document is relevant, or not. We can even include a feature\nsuch as whether the URL has a tilde because this might be\nindicator of home page or entry page. So all these features can then be combined\ntogether to generate a ranking function. The question is, of course. How can we combine them? In this approach,\nwe simply hypothesize that the probability that this document isn't relevant to this\nquery is a function of all these features. So we can hypothesize this that the probability of relevance\nis related to these features through a particular form of\nthe function that has some parameters. These parameters can control the influence of different\nfeatures of the final relevance. Now this is of course just an assumption. Whether this assumption really\nmakes sense is a big question and that's they have to empirically\nevaluate the function. But by hypothesizing that\nthe relevance is related to these features in the particular way, we can\nthen combine these features to generate the potential more powerful ranking\nfunction, a more robust ranking function. Naturally the next question is how\ndo we estimate those parameters? How do we know which features\nshould have a higher weight, and which features will have lower weight? So this is the task of training or\nlearning, so in this approach what we will\ndo is use some training data. Those are the data that have\nbeen charted by users so that we already know\nthe relevant judgments. We already know which documents should\nbe ranked high for which queries. And this information can be based\non real judgments by users or this can also be approximated by just\nusing click through information, where we can assume the clicked documents\nare better than the skipped documents clicked documents are relevant and\nthe skipped documents are non-relevant. So in general with the fit\nsuch hypothesize ranking function to the training data meaning that we will try to optimize it's\nretrieval accuracy on the training data. And we can adjust these parameters to see how we can optimize the performance of\nthe functioning on the training data in terms of some measures such as MAP or\nNDCG. So the training date would\nlook like a table of tuples. Each tuple has three elements, the query,\nthe document, and the judgement. So it looks very much like our\nrelevance judgement that we talked about in the evaluation\nof retrieval systems. [MUSIC]",
 "02_lesson-6-2-learning-to-rank-part-2.en.txt": "[MUSIC] So now let's take a look at the specific\nmethod that's based on regression. Now, this is one of the many\ndifferent methods, and in fact, it's one of the simplest methods. And I choose this to explain\nthe idea because it's simple. So in this approach, we simply assume\nthat the relevance of document with respect to a query is related to a linear\ncombination of all the features. Here I used Xi to denote the feature. So Xi of Q and D is a feature. And we can have as many\nfeatures as we would like. And we assume that these features\ncan be combined in a linear manner. And each feature is controlled\nby a parameter here, and this beta i is a parameter. That's a weighting parameter. A larger value would mean the feature\nwould have a higher weight, and it would contribute more\nto the scoring function. This specific form of the function\nactually also involves a transformation of\nthe probability of relevance. So this is the probability of relevance. And we know that the probability of\nrelevance is within the range from 0 to 1. And we could have just assumed that\nthe scoring function is related to this linear combination. So we can do a linear regression. But then, the value of this linear\ncombination could easily go beyond 1. So this transformation\nhere would map the 0 to 1 range to the whole\nrange of real values, you can verify it by yourself. So this allows us then to connect\nto the probability of variance which is between 0 and 1 to a linear\ncombination of arbitrary features. And if we rewrite this into a probability\nfunction, we would get the next one. So on this equation, now we'll\nhave the probability of relevance. And on the right hand side,\nwe'll have this form. Now, this form is clearly nonnegative, and it still involves a linear\ncombination of features. And it's also clear that if this value is, this is actually negative of the linear\ncombination in the equation above. If this value here is large, then it would mean this value is small. And therefore,\nthis whole probability would be large. And that's we expect, that basically,\nit would mean if this combination gives us a high value, then\nthe document's more likely irrelevant. So this is our hypothesis. Again, this is not necessarily the best\nhypothesis, but this is a simple way to connect these features with\nthe probability of relevance. So now we have this combination function. The next task is to\nestimate the parameters so that the function cache will be applied. But without knowing the beta values,\nit's harder to apply this function. So let's see how can\nestimate our beta values. All right,\nlet's take a look at a simple example. In this example, we have three features. One is the BM25 score of the document and\nthe query. One is the PageRank score of the document,\nwhich might or might not depend on the query. We might have a topic-sensitive PageRank,\nthat would depend on the query. Otherwise, the general PageRank\ndoesn't really depend on the query. And then we have BM25 score on\nthe anchor test of the document. Now, these are then the feature values for\na particular document query pair. And in this case, the document is D1 and\nthe judgment says that it's relevant. Here's another training instance and\nit's these feature values, but in this case, it's not relevant. This is an oversimplified case where\nwe just have two instances, but it's sufficient to illustrate the point. So what we can do is we use\nthe maximum likelihood estimator to actually estimate the parameters. Basically, we're going to\npredict the relevance status of the document based\non the feature values. That is, given that we observed\nthese feature values here. Can we predict the relevance here? Now, of course, the prediction would be\nusing this function that you see here. And we hypothesize that the probability\nof relevance is related to features in this way. So we are going to see, for what values of\nbeta we can predict the relevance well. What do we mean by predicting\nthe relevance well? Well, we just mean, in the first case, for D1 this expression right here\nshould give high values. In fact, we'll hope this\nto gave a value close to 1. Why?\nBecause this is a relevant document. On the other hand,\nin the second case, for D2, we hope this value will be small, right. Why? Because it's a non-relevant document. So now let's see how this can\nbe mathematically expressed. And this is similar to expressing\nthe probability of document, only that we are not talking about\nthe probability of words, but talking about the probability\nof relevance, 1 or 0. So what's the probability\nof this document being relevant if it has these feature values? Well, this is just this expression. We just need to plug in the Xi's. So that's what we will get. It's exactly like what we have seen above, only that we replaced these\nXi's with now specific values. So for example, this 0.7 goes to here and this 0.11 goes to here. And these are different feature values, and we combine them in\nthis particular way. The beta values are still unknown. But this gives us the probability\nthat this document is relevant, if we assume such a model. Okay? And we want to maximize this probability,\nsince this is a relevant document. What do we do for the second document? Well, we want to compute the probability\nthat the prediction is non-relevant. So this would mean we have to\ncompute 1 minus this expression, since this expression is actually\nthe probability of relevance. So to compute the non-relevance\nfrom relevance, we just do 1 minus\nthe probability of relevance. Okay? So this whole expression then\njust is our probability of predicting these two relevance values. One is 1 here, one is 0. And this whole equation\nis our probability of observing a 1 here and observing a 0 here. Of course, this probability\ndepends on the beta values. So then our goal is to adjust\nthe beta values to make this whole thing reach its maximum,\nmake it as large as possible. So that means we're going to compute this. The beta is just the parameter\nvalues that would maximize this whole likelihood expression. And what it means is,\nif you look at the function, is, we're going to choose betas to\nmake this as large as possible and make this also as large as possible,\nwhich is equivalent to say, make this part as small as possible. And this is precisely what we want. So once we do the training,\nnow we will know the beta values. So then this function\nwould be well-defined. Once beta values are known, both this and\nthis would be completely specified. So for any new query and new document, we can simply compute the features for\nthat pair. And then we just use this formula\nto generate the ranking score. And this scoring function can be used to\nrank documents for a particular query. So that's the basic idea\nof learning to rank. [MUSIC]",
 "03_lesson-6-3-learning-to-rank-part-3.en.txt": "[SOUND]\nThere are many more of the Munster learning algorithms\nthan the regression based approaches and they generally attempt to direct\nthe optimizer retrieval method. Like a MAP or nDCG. Note that the optimization object or\nfunction that we have seen on the previous slide is not directly\nrelated to the retrieval measure. By maximizing the prediction of one or zero, we don't necessarily optimize\nthe ranking of those documents. One can imagine that our\nprediction may not be too bad. And let's say both are around 0.5. So it's kind of in the middle of zero and\none for the two documents. But the ranking can be wrong, so we might\nhave a larger value for E2 and then E1. So that won't be good from\nretrieval perspective, even though function, it's not bad. In contrast, we might have another\ncase where we predicted the values, or around the 0.9, it said. And by the objective function,\nthe error would be larger. But if we didn't get the order\nof the two documents correct, that's actually a better result. So these new, more advanced approaches\nwill try to correct that problem. Of course, then the challenge is\nthat the optimization problem will be harder to solve. And then, researchers have posed\nmany solutions to the problem, and you can read more of the references at\nthe end, know more about these approaches. Now, these learning ranked\napproaches after the general. So there accounts would be be applied\nwith many other ranking problems, not just the retrieval problem. So some people will go\nwith recommender systems, computational advertising,\nor summarization and there are many others that you can\nprobably encounter in your applications.. To summarize this lecture we\nhave talked about using machine learning to combine much more\nfeatures including ranking results. Actually the use of machine learning in information retrieval has\nstarted since many decades ago. So for example, the Rocchio feedback\napproach that we talked about earlier was a machine learning approach\nprior to relevance feedback. But the most recent use of machine\nlearning has been driven by some changes in the environment of\napplications of retrieval systems. First, it's mostly freedom of\navailability of a lot of training data in the form of critical, such as\nthey are more available than before. So the data can provide a lot of\nuseful knowledge about relevance and machine learning methods can be\napplied into a leverage list. Secondly, it's also freedom by\nthe need for combining many features, and this is not only just\nbecause there are more features available on the web that can\nbe naturally used for improved scoring. It's also because by combining them,\nwe can improve the robustness of ranking, so this is desired for\ncombating spams. Modern search engines all use some\nkind of machine learning techniques to combine many features\nto optimize ranking and this is a major feature of these\ncommercial engines such a Google or Bing. The topic of learning to rank is still\nactive research topic in the community, and so we can expect to see new results\nin development in the next few years, perhaps. Here are some additional readings\nthat can give you more information about how learning to rank at works and\nalso some advanced methods. [MUSIC]",
 "04_lesson-6-4-future-of-web-search.en.txt": "[SOUND]. This lecture is about\nthe future of web search. In this lecture, we're going to talk\nabout some possible future trends of web search and intelligent information\nretrieval systems in general. In order to further improve\nthe accuracy of a search engine, it's important that to consider\nspecial cases of information need. So one particular trend could be to\nhave more and more specialized than customized search engines, and they\ncan be called vertical search engines. These vertical search engines can be\nexpected to be more effective than the current general search engines\nbecause they could assume that users are a special group of users that\nmight have a common information need, and then the search engine can be\ncustomized with this ser, so, such users. And because of the customization,\nit's also possible to do personalization. So the search can be personalized, because we have a better\nunderstanding of the users. Because of the restrictions with domain,\nwe also have some advantages in handling the documents, because we can\nhave better understanding of documents. For example, particular words may\nnot be ambiguous in such a domain. So we can bypass the problem of ambiguity. Another trend we can expect to see, is the search engine will\nbe able to learn over time. It's like a lifetime learning or\nlifelong learning, and this is, of course, very attractive because that means the\nsearch engine will self-improve itself. As more people are using it, the search\nengine will become better and better, and this is already happening, because the search engines can learn\nfrom the [INAUDIBLE] of feedback. More users use it, and the quality\nof the search engine allows for the popular queries that are typed in by\nmany users allow it to become better, so this is sort of another\nfeature that we will see. The third trend might be\nto the integration of bottles of information access. So search, navigation, and\nrecommendation or filtering might be combined to form a full-fledged\ninformation management system. And in the beginning of this course,\nwe talked about push versus pull. These are different modes of information\naccess, but these modes can be combined. And similarly, in the pull mode, querying\nand the browsing could also be combined. And in fact we're doing that basically,\ntoday, is the [INAUDIBLE] search endings. We are querying, sometimes browsing,\nclicking on links. Sometimes we've got some\ninformation recommended. Although most of the cases the information\nrecommended is because of advertising. But in the future, you can imagine\nseamlessly integrate the system with multi-mode for information access, and\nthat would be convenient for people. Another trend is that we might see systems that try to go beyond the searches\nto support the user tasks. After all, the reason why people want\nto search is to solve a problem or to make a decision or perform a task. For example consumers might search for opinions about products in\norder to purchase a product, choose a good product by, so\nin this case it would be beneficial to support the whole workflow of purchasing\na product, or choosing a product. In this era, after the common search\nengines already provide a good support. For example, you can sometimes look at the\nreviews, and then if you want to buy it, you can just click on the button to go the\nshopping site and directly get it done. But it does not provide a,\na good task support for many other tasks. For example, for researchers, you might want to find the realm in\nthe literature or site of the literature. And then, there's no, not much support for\nfinishing a task such as writing a paper. So, in general, I think,\nthere are many opportunities in the wait. So in the following few slides, I'll\nbe talking a little bit more about some specific ideas or thoughts that hopefully, can help you in imagining new\napplication possibilities. Some of them might be already relevant\nto what you are currently working on. In general, we can think about any\nintelligent system, especially intelligent information system, as we specified\nby these these three nodes. And so\nif we connect these three into a triangle, then we'll able to specify\nan information system. And I call this\nData-User-Service Triangle. So basically the three questions you\nask would be who are you serving and what kind of data are you are managing and\nwhat kind of service you provide. Right there, this would help us\nbasically specify in your system. And there are many different ways\nto connect them depending on how you connect them,\nyou will have a different kind of systems. So let me give you some examples. On the top,\nyou can see different kinds of users. On the left side, you can see different\ntypes of data or information, and on the bottom,\nyou can see different service functions. Now imagine you can connect\nall these in different ways. So, for example, you can connect\neveryone with web pages, and the support search and\nbrowsing, what do you get? Well, that's web search, right? What if we connect UIUC employees with\norganization documents or enterprise documents to support the search and\nbrowsing, but that's enterprise search. If you connect the scientist\nwith literature information to provide all kinds of service,\nincluding search, browsing, or alert of new random documents or\nmining analyzing research trends, or provide the task with support or\ndecision support. For example, we might be,\nmight be able to provide a support for automatically generating\nrelated work section for a research paper, and\nthis would be closer to task support. Right?\nSo then we can imagine this would\nbe a literature assistant. If we connect the online shoppers\nwith blog articles or product reviews then we can help these people\nto improve shopping experience. So we can provide, for example data mining\ncapabilities to analyze the reviews, to compare products, compare sentiment of\nproducts and to provide task support or decision support to have them\nchoose what product to buy. Or we can connect customer service\npeople with emails from the customers, and, and we can imagine a system\nthat can provide a analysis of these emails to find that the major\ncomplaints of the customers. We can imagine a system we\ncould provide task support by automatically generating\na response to a customer email. Maybe intelligently attach\nalso a promotion message if appropriate, if they detect that that's\na positive message, not a complaint, and then you might take this opportunity\nto attach some promotion information. Whereas if it's a complaint,\nthen you might be able to automatically generate some\ngeneric response first and tell the customer that he or she can\nexpect a detailed response later, etc. All of these are trying to help\npeople to improve the productivity. So this shows that\nthe opportunities are really a lot. It's just only restricted\nby our imagination. So this picture shows the trend\nof the technology, and also, it characterizes the, intelligent\ninformation system in three angles. You can see in the center, there's\na triangle that connects keyword queries to search a bag of words representation. That means the current search engines\nbasically provides search support to users and mostly model\nusers based on keyword queries and sees the data through\nbag of words representation. So it's a very simple approximation of\nthe actual information in the documents. But that's what the current system does. It connects these three nodes\nin such a simple way, or it only provides a basic search function\nand doesn't really understand the user, and it doesn't really understand that\nmuch information in the documents. Now, I showed some trends to push each\nnode toward a more advanced function. So think about the user node here, right? So we can go beyond the keyword queries,\nlook at the user search history, and then further model the user\ncompletely to understand the, the user's task environment,\ntask need context or other information. Okay, so this is pushing for\npersonalization and complete user model. And this is a major\ndirection in research in, in order to build intelligent\ninformation systems. On the document side,\nwe can also see, we can go beyond bag of words implementation\nto have entity relation representation. This means we'll recognize people's names,\ntheir relations, locations, etc. And this is already feasible with\ntoday's natural processing technique. And Google is the reason\nthe initiative on the knowledge graph. If you haven't heard of it,\nit is a good step toward this direction. And once we can get to that level without\ninitiating robust manner at larger scale, it can enable the search engine\nto provide a much better service. In the future we would like to have knowledge representation where we\ncan add perhaps inference rules, and then the search engine would\nbecome more intelligent. So this calls for\nlarge-scale semantic analysis, and perhaps this is more feasible for\nvertical search engines. It's easier to make progress\nin the particular domain. Now on the service side, we see we need to go beyond the search of\nsupport information access in general. So search is only one way to get access\nto information as well recommender systems and push and pull so different\nways to get access to random information. But going beyond access, we also need to help people digest the\ninformation once the information is found, and this step has to do with analysis\nof information or data mining. We have to find patterns or\nconvert the text information into real knowledge that can\nbe used in application or actionable knowledge that can be used for\ndecision making. And furthermore the knowledge\nwill be used to help a user to improve productivity in finishing a task,\nfor example, a decision-making task. Right, so this is a trend. And, and, and so basically,\nin this dimension, we anticipate in the future intelligent information\nsystems will provide intelligent and interactive task support. Now I should also emphasize interactive\nhere, because it's important to optimize the combined intelligence of the users and\nthe system. So we, we can get some help\nfrom users in some natural way. And we don't have to assume the system\nhas to do everything when the human, user, and the machine can collaborate in\nan intelligent way, an efficient way, then the combined intelligence\nwill be high and in general, we can minimize the user's overall\neffort in solving problem. So this is the big picture of future\nintelligent information systems, and this hopefully can provide\nus with some insights about how to make further innovations\non top of what we handled today. [MUSIC]",
 "05_lesson-6-5-recommender-systems-content-based-filtering-part-1.en.txt": "[MUSIC] This lecture is about\nthe Recommender Systems. So far we have talked about a lot\nof aspects of search engines. We have talked about the problem\nof search and ranking problem, different methods for ranking,\nimplementation of search engine and how to evaluate a search engine, etc. This is important because we know\nthat web search engines are by far the most important applications\nof text retrieval. And they are the most useful tools\nto help people convert big raw text data into a small set\nof relevant documents. Another reason why we spend so\nmany lectures on search engines, is because many techniques used in search\nengines are actually also very useful for Recommender Systems,\nwhich is the topic of this lecture. And so, overall, the two systems\nare actually well connected. And there are many techniques\nthat are shared by them. So this is a slide that\nyou have seen before, when we talked about the two\ndifferent modes of text access. Pull and the Push. And we mentioned that recommender\nsystems are the main systems to serve users in the Push Mode, where the systems\nwill take the initiative to recommend the information to the user or\npushes information to the user. And this often works\nwell when the user has stable information need\nin the system has a good. So a Recommender System is sometimes\ncalled a filtering system and it's because recommending useful\nitems to people is like discarding or filtering out the the useless articles, and so\nin this sense they are kind of similar. And in all the cases the system\nmust make a binary decision and usually there's a dynamic source\nof information items, and that you have some knowledge\nabout the users' interest. And then the system would make a decision about whether this item is\ninteresting to the user, and then if it's interesting then the system\nwould recommend the article to the user. So the basic filtering question here is\nreally will this user like this item? Will U like item X? And there are two ways to answer this\nquestion, if you think about it. And one is look at what items U likes and then we can see if X is\nactually like those items. The other is to look at who likes X,\nand we can see if this user looks like a one of those users,\nor like most of those users. And these strategies can be combined. If we follow the first strategy and look at item similarity in the case\nof recommending text objects, then we're talking about a content-based\nfiltering or content-based recommendation. If we look at the second strategy, then,\nit's to compare users and in this case we're user similarity and the technique\nis often called collaborative filtering. So, let's first look at\nthe content-based filtering system. This is what the system would look like. Inside the system, there will be\na Binary Classifier that would have some knowledge about the user's interests, and\nthis is called a User Interest Profile. It maintains this profile to keep\ntrack of all users interests, and then there is a utility function\nto guide the user to make decision a nice plan utility\nfunction in the moment. It helps the system decide\nwhere to set the threshold. And then the accepted documents will\nbe those that have passed the threshold according to the classified. There should be also an initialization\nmodule that would take a user's input, maybe from a user's specified keywords or\nchosen category, etc., and this would be to feed into\nthe system with the initiator's profile. There is also typically a learning\nmodule that would learn from users' feedback over time. Now note that in this case typical\nusers information is stable so the system would have a lot more\nopportunities to observe the users. If the user has taken a recommended item,\nhas viewed that, and this a signal to indicate that\nthe recommended item may be relevant. If the user discarded it,\nno, it's not relevant. And so such feedback can be a long term\nfeedback, and can last for a long time. And the system can collect a lot of\ninformation about the user's interest and this then can then be used\nto improve the classify. Now what's the criteria for\nevaluating such a system? How do we know this filtering\nsystem actually performs well? Now in this case we cannot use the ranking\nevaluation measures like a map because we can't afford waiting for\na lot of documents and then rank the documents to\nmake a decision for the users. And so the system must make\na decision in real time in general to decide whether the item is\nabove the threshold or not. So in other words, we're trying\nto decide on absolute relevance. So in this case, one common user strategy is to use\na utility function to evaluate the system. So here, I show linear utility function. That's defined as for example three\nmultiplied the number of good items that you delivered, minus two multiplied by the\nnumber of bad items that you delivered. So in other words, we could kind of just treat this as almost in a gambling game. If you delete one good item,\nlet's say you win three dollars, you gain three dollars but if you deliver\na bad one you will lose two dollars. And this utility function\nbasically kind of measures how much money you are get by\ndoing this kind of game, right? And so it's clear that if you want\nto maximize this utility function, this strategy should be delivered\nas many good articles as possible, and minimize the delivery of bad articles. That's obvious, right? Now one interesting question here is\nhow should we set these coefficients? I just showed a three and\nnegative two as possible coefficients. But one can ask the question,\nare they reasonable? So what do you think? Do you think that's a reasonable choice? What about the other choices? So for example, we can have 10 and\nminus 1, or 1, minus 10. What's the difference? What do you think? How would this utility function affect\nthe systems' threshold of this issue. Right, you can think of\nthese two extreme cases. (10, -1) + (1, -10), which one do\nyou think would encourage this system to over do it and which one would\nencourage this system to be conservative? If you think about it you will see that\nwhen we get a bigger award for delivering our good document you incur only a small\npenalty for delivering a bad one. Intuitively, you would be\nencouraged to deliver more. And you can try to deliver more in\nhope of getting a good one delivered. And then we'll get a big reward. So on the other hand,\nif you choose (1,-10), you really don't get such a big prize\nif you deliver a good document. On the other hand, you will have\na big loss if you deliver a bad one. You can imagine that, the system would be very reluctant\nto deliver a lot of documents. It has to be absolutely\nsure that it's not. So this utility function has to be\ndesigned based on a specific application. The three basic problems in content-based\nfiltering are the following, first, it has to make\na filtering decision. So it has to be a binary decision maker,\na binary classifier. Given a text document and\na profile description of the user, it has to say yes or no, whether this\ndocument should be deleted or not. So that's a decision module, and\nit should be an initialization module as you have seen earlier and\nthis will get the system started. And we have to initialize the system\nbased on only very limited text exclusion or\nvery few examples from the user. And the third model is\na learning model which you have, has to be able to learn from limited\nrelevance judgements, because we counted them from the user about their\npreferences on the deliver documents. If we don't deliver document\nto the user we'll never be able to know whether\nthe user likes it or not. And we had accumulate a lot of documents\neven then from entire history. All these modules will have to be\noptimized to maximize the utility. So how can we deal with such a system? And there are many different approaches. Here we're going to talk about\nhow to extend a retrieval system, a search engine for information filtering. Again, here's why we've spent a lot of\ntime talking about the search engines. Because it's actually not very hard\nto extend the search engine for information filtering. So here's the basic idea for\nextending a retrieval system for information filtering. First, we can reuse a lot of\nretrieval techniques to do scoring. Right, so we know how to score\ndocuments against queries, etc. We're going to match the similarity\nbetween profile text description and a document. And then we can use a score threshold for\nthe filtering decision. We do retrieval and then we kind of find\nthe scores of documents and then we'll apply a threshold to see whether the\ndocument is passing the threshold or not. And if it's passing the threshold, we're going to say it's relevant and\nwe're going to deliver it to the user. Another component that we have to add is,\nof course, to learn from the history, and we had used is the traditional feedback\ntechniques to learn to improve scoring. And we know rock hill can be using for\nscoring improvement. And, but we have to develop a new\napproaches to learn how to accept this. And we need to set it initially and then we have to learn how to\nupdate the threshold over time. So here's what the system\nmight look like if we just generalize the vector-space model for\nfiltering problems, right? So you can see the document vector could\nbe fed into a scoring module which already exists in a search engine\nthat implements a vector-space model. And the profile will be treated\nas a query essentially, and then the profile vector can be matched with\nthe document vector to generate the score. And then this score would be fed into a\nthresholding module that would say yes or no, and then the evaluation would be based\non the utility for the filtering results. If it says yes and then the document\nwould be sent to the user. And then user could give some feedback. The feedback information would be\nused to both adjust the threshold and to adjust the vector representation. So the vector learning is essentially\nthe same as query modification or feedback in the case of search. The threshold of learning\nis a new component and that we need to talk\na little bit more about. [MUSIC]",
 "06_lesson-6-6-recommender-systems-content-based-filtering-part-2.en.txt": "[SOUND] There are some interesting challenges in threshold for\nthe learning the filtering problem. So here I show the historical data that\nyou can collect in the filtering system, so you can see the scores and\nthe status of relevance. So the first one has a score of 36.5 and\nit's relevant. The second one is not relevant and\nit's separate. Of course, we have a lot of documents for\nwhich we don't know the status, because we have never\ndelivered them to the user. So as you can see here, we only see the judgements of\ndocuments delivered to the user. So this is not a random sample,\nso it's a sensitive data. It's kind of biased, so that creates\nsome difficultly for learning. Secondly, there are in general very little\nlabeled data and very few relevant data, so it's also challenging for\nmachine learning approaches, typically they require more training data. And in the extreme case at\nthe beginning we don't even have any labeled data as well. The system there has to make a decision, so that's a very difficult\nproblem at the beginning. Finally, there is also this issue of\nexploration versus exploitation tradeoff. Now, this means we also want\nto explore the document space a little bit and\nto see if the user might be interested in documents that\nwe have in data labeled. So in other words, we're going to\nexplore the space of user interests by testing whether the user might be\ninterested in some other documents that currently are not matching\nthe user's interests so well. So how do we do that? Well, we could lower the threshold\na little bit until we just deliver some near misses to the user\nto see what the user would respond, to see how the user would\nrespond to this extra document. And this is a tradeoff, because on\nthe one hand, you want to explore, but on the other hand,\nyou don't want to really explore too much, because then you will over\ndeliver non-relevant Information. So exploitation means you would\nexploit what you learn about the user. Let's say you know the user is\ninterested in this particular topic, so you don't want to deviate that much, but if you don't deviate at all then you don't\nexploit so that's also are not good. You might miss opportunity to learn\nanother interest of the user. So this is a dilemma. And that's also a difficulty\nproblem to solve. Now, how do we solve these problems? In general, I think one can use the\nempirical utility optimization strategy. And this strategy is basically to optimize\nthe threshold based on historical data, just as you have seen\non the previous slide. Right, so you can just compute\nthe utility on the training data for each candidate score threshold. Pretend that, what if I cut at this point. What if I cut at the different scoring\nthreshold point, what would happen? What's utility? Since these are training data,\nwe can kind of compute the utility, and we know that relevant status,\nor we assume that we know relevant status based on\napproximation of click-throughs. So then we can just choose the threshold\nthat gives the maximum utility on the training data. But this of course, doesn't account for\nexploration that we just talked about. And there is also the difficulty of\nbiased training sample, as we mentioned. So, in general, we can only get the upper\nbound for the true optimal threshold, because the threshold might\nbe actually lower than this. So, it's possible that this could\ndiscarded item might be actually interesting to the user. So how do we solve this problem? Well, we generally, and as I said we can low with this\nthreshold to explore a little bit. So here's on particular approach\ncalled beta-gamma threshold learning. So the idea is falling. So here I show a ranked list of all the\ntraining documents that we have seen so far, and\nthey are ranked by their positions. And on the y axis we show the utility,\nof course, this function depends on how you specify the coefficients\nin the utility function, but we can then imagine, that depending on the\ncutoff position, we will have a utility. Suppose I cut at this position and\nthat would be a utility. For example,\nidentify some cutting cutoff point. The optimal point,\ntheta optimal, is the point when it will achieve the maximum utility\nif we had chosen this as threshold. And there is also zero utility threshold. You can see at this cutoff\nthe utility is zero. What does that mean? That means if I lower the threshold\na little bit, now I reach this threshold. The utility would be lower but\nit's still non-active at least, right? So it's not as high as\nthe optimal utility. But it gives us as a safe point\nto explore the threshold, as I have explained, it's desirable\nto explore the interest of space. So it's desirable to lower the threshold\nbased on your training there. So that means, in general, we want to set\nthe threshold somewhere in this range. Let's say we can use the alpha to control the deviation from\nthe optimal utility point. So you can see the formula of the\nthreshold would be just the interpolation of the zero utility threshold and\nthe optimal utility threshold. Now, the question is,\nhow should we set alpha? And when should we deviate more\nfrom the optimal utility point? Well, this can depend on multiple factors,\nand the one way to solve the problem is to encourage this threshold\nmechanism to explore up to the zero point, and\nthat's a safe point, but we're not going to necessarily reach\nall the way to the zero point. Rather, we're going to use other\nparameters to further define alpha and this specifically is as follows. So there will be a beta parameter to\ncontrol the deviation from the optimal threshold and this can be based on can\nbe accounting for the over-fitting to the training data let's say, and so\nthis can be just an adjustment factor. But what's more interesting\nis this gamma parameter. Here, and you can see in this formula, gamma is controlling the inference of the number of examples\nin training that are set. So you can see in this formula as N which\ndenotes the number of training examples becomes bigger, then it would\nactually encourage less exploration. In other words, when these very\nsmall it would try to explore more. And that just means if we have seen few examples we're not sure whether we\nhave exhausted the space of interest. So we need to explore but as we have\nseen many examples from the user many that have we feel that we\nprobably don't have to explore more. So this gives us a beta gamma for\nexploration, right. The more examples we have seen\nthe less exploration we need to do. So the threshold would be closer\nto the optimal threshold so that's the basic idea of this approach. This approach actually has been working\nwell in some evaluation studies, particularly effective. And also can work on arbitrary utility\nwith the appropriate lower bound. And explicitly addresses\nthe exploration-exploitation tradeoff and it kind of uses the zero utility\nthreshold point as a safeguard for exploration-exploitation tradeoff. We're not never going to explore\nfurther than the zero utility point. So if you take the analogy of gambling,\nand you don't want to risk on losing money. So it's a safe spend, really\nconservative strategy for exploration. And the problem is of course,\nthis approach is purely heuristic and the zero utility lower boundary is also\noften too conservative, and there are, of course, more advance in machine learning\napproaches that have been proposed for solving this problems and\nthis is their active research area. So to summarize, there are two\nstrategies for recommended systems or filtering systems, one is content based,\nwhich is looking at the item similarity, and the other is collaborative filtering\nthat was looking at the user similarity. We've covered content-based\nfiltering approach. In the next lecture, we will talk\nabout the collaborative filtering. In content-based filtering system,\nwe generally have to solve several problems relative to\nfiltering decision and learning, etc. And such a system can actually be\nbuilt based on a search engine system by adding a threshold mechanism and\nadding adaptive learning algorithm to allow the system to learn from\nlong term feedback from the user. [MUSIC]",
 "07_lesson-6-7-recommender-systems-collaborative-filtering-part-1.en.txt": "This lecture is about\ncollaborative filtering. In this lecture we're going to continue\nthe discussion of recommended systems. In particular, we're going to look at\nthe approach of collaborative filtering. You have seen this slide before when\nwe talked about the two strategies to answer the basic question,\nwill user U like item X? In the previous lecture, we looked at the item similarity,\nthat's content-based filtering. In this lecture, we're going to\nlook at the user similarity. This is a different strategy,\ncalled a collaborative filtering. So first, what is collaborative filtering? It is to make filtering decisions for individual user based on\nthe judgements of other uses. And that is to say we will\ninfer individual's interest or preferences from that\nof other similar users. So the general idea is the following. Given a user u, we're going to first\nfind the similar users, U1 through. And then we're going to\npredict the use preferences based on the preferences of\nthese similar users, U1 through. Now, the user similarity here can\nbe judged based their similarity, the preferences on a common set of items. Now here you can see the exact\ncontent of item doesn't really matter. We're going to look at the only the\nrelation between the users and the items. So this means this\napproach is very general. It can be applied to any items,\nnot just the text of objects. So this approach would work well\nunder the following assumptions. First, users with the same interest\nwill have similar preferences. Second, the users with similar preferences\nprobably share the same interest. So for example, if the interest of\nthe user is in information retrieval, then we can infer the user\nprobably favor SIGIR papers. So those who are interested in\ninformation retrieval researching, probably all favor SIGIR papers. That's an assumption that we make. And if this assumption is true, then it would help collaborative\nfiltering to work well. We can also assume that if we see\npeople favor See SIGIR papers, then we can infer their interest\nis probably information retrieval. So in these simple examples,\nit seems to make sense, and in many cases such assumption\nactually does make sense. So another assumption we have to make\nis that there are sufficiently large number of user preferences\navailable to us. So for example, if you see a lot\nof ratings of users for movies and those indicate their\npreferences on movies. And if you have a lot of such data,\nthen cluttered and filtering can be very effective. If not, there will be a problem, and\nthat's often called a cold start problem. That means you don't have many\npreferences available, so the system could not fully take advantage\nof collaborative filtering yet. So let's look at the filtering\nproblem in a more formal way. So this picture shows that we are, in general, considering a lot of users and we're showing m users here, so U1 through. And we're also considering\na number of objects. Let's say n objects in\norder to O1 through On. And then we will assume that\nthe users will be able to judge those objects and the user could for\nexample give ratings to those items. For example, those items could be movies,\ncould be products and then the users would give\nratings 1 through 5 and see. So what you see here is that we have\nshown some ratings available for some combinations. So some users have watched some movies,\nthey have rated those movies, they obviously won't be able\nto watch all the movies and some users may actually\nonly watch a few movies. So this is in general a small symmetrics. So many items and\nmany entries have unknown values. And what's interesting here is we\ncould potentially infer the value of an element in this matrix\nbased on other values. And that's after the essential question\nin collaborative filtering, and that is, we assume there's an unknown\nfunction here, f. That would map a pair of user and\nobject to a rating. And we have observed the sum\nvalues of this function. And we want to infer the value\nof this function for other pairs that don't have\nthat as available here. So this is very similar to other\nmachinery problems where we'd know the values of the function\non some training data set. And we hope to predict the values of\nthis function on some test data so this is a function approximation. And how can we pick out the function\nbased on the observed ratings. So this is the setup. Now there are many approaches\nto solving this problem. In fact,\nthis is a very active research area or reason that there are special\nconferences dedicated to the problem, major conference devoted to the problem. [MUSIC]",
 "08_lesson-6-8-recommender-systems-collaborative-filtering-part-2.en.txt": "[SOUND]\nAnd here we're going to talk\nabout basic strategy. And that would be based on\nsimilarity of users and then predicting the rating of and object by an active user using the ratings\nof similar users to this active user. This is called a memory based approach\nbecause it's a little bit similar to storing all the user information and when we are considering a particular\nuser we going to try to retrieve the rating users or\nthe similar users to this user case. And then try to use this\ninformation about those users to predict the preference of this user. So here is the general idea and\nwe use some notations here, so x sub i j denotes the rating\nof object o j by user u i and n sub i is average rating\nof object by this user. So this n i is needed because we would like to normalize\nthe ratings of objects by this user. So how do you do normalization? Well, we're going to just subtract\nthe average rating from all the ratings. Now, this is to normalize these ratings so that the ratings from different\nusers would be comparable. Because some users might be more generous,\nand they generally give more high ratings but some others might be\nmore critical so their ratings cannot be directly compared with each\nother or aggregate them together. So we need to do this normalization. Another prediction of\nthe rating on the item by another user or\nactive user, u sub a here can be based on the average\nratings of similar users. So the user u sub a is the user that we\nare interested in recommending items to. And we now are interested in\nrecommending this o sub j. So we're interested in knowing how\nlikely this user will like this object. How do we know that? Where the idea here is to look at\nwhether similar users to this user have liked this object. So mathematically this is to say\nwell the predicted the rating of this user on this app object,\nuser a on object o j is basically combination of the normalized\nratings of different users, and in fact here,\nwe're taking a sum over all the users. But not all users contribute\nequally to the average, and this is conjured by the weights. So this weight controls the inference of the user on the prediction. And of course,\nnaturally this weight should be related to the similarity between ua and\nthis particular user, ui. The more similar they are,\nthen the more contribution user ui can make in predicting\nthe preference of ua. So, the formula is extremely simple. You can see,\nit's a sum of all the possible users. And inside the sum we have their ratings,\nwell, their normalized ratings\nas I just explained. The ratings need to be normalized in\norder to be comparable with each other. And then these ratings\nare weighted by their similarity. So you can imagine w of a and i is just\na similarity of user a and user i. Now what's k here? Well k is simply a normalizer. It's just one over the sum of all\nthe weights, over all the users. So this means, basically, if you consider\nthe weight here together with k, and we have coefficients of weight that\nwill sum to one for all the users. And it's just a normalization strategy so\nthat you get this predictor rating in the same range as these ratings\nthat we used to make the prediction. Right? So this is basically the main idea\nof memory-based approaches for collaborative filtering. Once we make this prediction,\nwe also would like to map back through the rating that the user would actually make, and this is to further\nadd the mean rating or average rating of this user u\nsub a to the predicted value. This would recover a meaningful rating for\nthis user. So if this user is generous, then\nthe average it would be is somewhat high, and when we add that the rating will be\nadjusted to our relatively high rate. Now when you recommend an item to a user\nthis actually doesn't really matter, because you are interested in\nbasically the normalized reading, that's more meaningful. But when they evaluate these\nrather than filter approaches, they typically assume that\nactual ratings of the user on these objects to be unknown and\nthen you do the prediction and then you compare the predicted\nratings with their actual ratings. So, you do have access\nto the actual ratings. But, then you pretend that you don't know,\nand then you compare your systems\npredictions with the actual ratings. In that case, obviously, the systems\nprediction would be adjusted to match the actual ratings of the user and\nthis is what's happening here basically. Okay so this is the memory based approach. Now, of course,\nif you look at the formula, if you want to write\nthe program to implement it, you still face the problem of\ndetermining what is this w function? Once you know the w function, then\nthe formula is very easy to implement. So, indeed, there are many different ways\nto compute this function or this weight, w, and specific approaches generally\ndiffer in how this is computed. So here are some possibilities and you can imagine there\nare many other possibilities. One popular approach is we use\nthe Pearson correlation coefficient. This would be a sum over\ncommonly rated items. And the formula is a standard\nappears in correlation coefficient formula as shown here. So this basically measures\nwhether the two users tended to all give higher ratings to similar\nitems or lower ratings to similar items. Another measure is the cosine measure,\nand this is going to treat the rating vectors as vectors in the vector space. And then,\nwe're going to measure the angle and compute the cosine of\nthe angle of the two vectors. And this measure has been using the vector\nspace model for retrieval, as well. So as you can imagine there are just\nas many different ways of doing that. In all these cases, note that the user's\nsimilarity is based on their preferences on items and we did not actually use\nany content information of these items. It didn't matter these items are,\nthey can be movies, they can be books, they can be products, they can be text documents which\nhas been cabled the content and so this allows such approach to be\napplied to a wide range of problems. Now in some newer approaches of course, we would like to use more\ninformation about the user. Clearly, we know more about the user,\nnot just these preferences on these items. So in the actual filtering system,\nis in collaborative filtering, we could also combine that\nwith content based filtering. We could use more context information,\nand those are all interesting approaches that people are just starting, and\nthere are new approaches proposed. But, this memory based approach has\nbeen shown to work reasonably well, and it's easy to implement in\npractical applications this could be a starting point to see if the strategy\nworks well for your application. So, there are some obvious ways\nto also improve this approach and mainly we would like to improve\nthe user similarity measure. And there are some practical\nissues we deal with here as well. So for example,\nthere will be a lot of missing values. What do you do with them? Well, you can set them to default values\nor the average ratings of the user. And that would be a simple solution. But there are advanced approaches that\ncan actually try to predict those missing values, and then use predictive\nvalues to improve the similarity. So in fact that the memory based apology\ncan predict those missing values, right? So you get you have iterative approach\nwhere you first use some preliminary prediction and then you can use the predictive values to\nfurther improve the similarity function. So this is a heuristic\nway to solve the problem. And the strategy obviously would affect\nthe performance of claritative filtering just like any other heuristics would\nimprove these similarity functions. Another idea which is actually very\nsimilar to the idea of IDF that we have seen in text search is called\na Inverse User Frequency or IUF. Now here the idea is to look at where\nthe two users share similar ratings. If the item is a popular item that\nhas been viewed by many people and seen [INAUDIBLE] to people interested\nin this item may not be so interesting but if it's a rare item,\nit has not been viewed by many users. But these two users deal with this\nitem and they give similar ratings. And, that says more\nabout their similarity. It's kind of to emphasize\nmore on similarity on items that are not\nviewed by many users. [MUSIC]",
 "09_lesson-6-9-recommender-systems-collaborative-filtering-part-3.en.txt": "[SOUND]\nSo to summarize our discussion of\nrecommender systems, in some sense, the filtering task for\nrecommender task is easy, and in some other sense,\nthe task is actually difficult. So it's easy because\nthe user's expectation is low. In this case the system takes initiative\nto push information to the user. The user doesn't really make any effort,\nso any recommendation is better than nothing. All right.\nSo, unless you recommend the noise items or useless documents. If you can recommend\nsome useful information users generally will appreciate it,\nso that's, in that sense that's easy. However, filtering is actually much harder\ntask than retrieval because you have to make a binary decision and you can't\nafford waiting for a lot of items and then you're going to see whether\none item is better than others. You have to make a decision\nwhen you see this item. Think about news filtering. As soon as you see the news enough\nto decide whether the news would be interesting to the user. If you wait for a few days, well, even if\nyou can make accurate recommendation of the most relevant news, the utility is\ngoing to be significantly decreased. Another reason why it's hard\nis because of data sparseness if you think of this\nas a learning problem. Collaborative filtering, for example, is purely based on\nlearning from the past ratings. So if you don't have many ratings there's\nreally not that much you can do, right? And yeah I just mentioned\nthis cold start problem. This is actually a very serious,\nserious problem. But of course there are strategies that\nhave been proposed for the soft problem, and there are different strategies that\nyou can use to alleviate the problem. You can use, for example, more user\ninformation to asses their similarity, instead of using the preferences\nof these users on these items give me additional information\navailable about the user, etc. And we also talk about two strategies for\nfiltering task. One is content-based where\nwe look at items there is collaborative filtering where\nwe look at Use a similarity. And they obviously can be\ncombined in a practical system. You can imagine they generally\nwould have to be combined. So that would give us a hybrid\nstrategy for filtering. And we also could recall that we talked about push versus pull as two strategies\nfor getting access to the text data. And recommender system easy to\nhelp users in the push mode, and search engines are serving\nusers in the pull mode. Obviously the two should be combined,\nand they can be combined. The two have a system\nthat can support user with multiple mode information access. So in the future we could anticipate such\na system to be more useful the user. And either,\nthis is an active research area so there are a lot of new algorithms\nbeing proposed all the time. In particular those new algorithms tend\nto use a lot of context information. Now the context here could be\nthe context of the user and could also be the context of the user. Items. The items are not the isolated. They're connected in many ways. The users might form\nsocial network as well, so there's a rich context there\nthat we can leverage in order to really solve the problem well and\nthen that's active research area where also machine\nlearning algorithms have been applied. Here are some additional readings in the handbook called\nRecommender Systems and has a collection of a lot\nof good articles that can give you an overview\nof a number of specific approaches through recommender systems. [MUSIC]",
 "10_lesson-6-10-course-summary.en.txt": "[NOISE] This lecture is a summary of this course. This map shows the major topics\nwe have covered in this course. And here are some key\nhigh-level take-away messages. First, we talked about natural\nlanguage content analysis. Here the main take-away messages\nis natural language processing is a foundation for text retrieval, but\ncurrently the NLP isn't robust enough so the battle of wars is generally the main\nmethod used in modern search engines. And it's often sufficient before\nmost of the search tasks, but obviously for\nmore complex search tasks then we need a deeper natural language\nprocessing techniques. We then talked about the high\nlevel strategies for text access and\nwe talked about push versus pull. In pull we talked about\nquerying versus browsing. Now in general in future search engines,\nwe should integrate all these techniques to provide a math involved\ninformation access. And now we'll talk about a number of\nissues related to search engines. We talked about the search problem. And we framed that as a ranking problem. And we talked about a number\nof retrieval methods. We start with the overview\nof vector space model and the probabilistic model and then we talked\nabout the vector space model in depth. We also later talked about\nthe language modeling approach, and that's probabilistic model. And here, many take-away message is that\nthe modeling retrieval function tend to look similar, and\nthey generally use various heuristics. Most important ones are TF-IDF weighting,\ndocument length normalization. And the TF is often transformed through\na sub media transformation function. And then we talked about how to\nimplement a retrieval system, and here, the main techniques that we talked about,\nhow to construct an inverted index so that we can prepare the system\nto answer a query quickly. And we talked about how to do a faster\nsearch by using the inverted index. And we then talked about how to\nevaluate the text retrieval system, mainly introduced to\nthe Cranfield Evaluation Methodology. This was a very important\nevaluation methodology that can be applied to many tasks. We talked about the major\nevaluation measures. So, the most important measures for\na search engine are MAP, mean average precision,\nand nDCG Summarize the discount or accumulative gain and also precision and\nrecall are the two basic measures. And we then talked about\nfeedback techniques. And we talked about the Rocchio\nin the Vector Space Model and the mixture model and\nthe language modeling approach. Feedback is a very important\ntechnique especially considering the opportunity of learning from\na lot of pixels on the Web. We then talked about Web search. And here we talked about how\nto use parallel in that scene to solve the scalability issue in that\nscene we're going to use the net reduce. Then we talked about how to use linking\npermission model app to improve search. We talked about page rank and hits as the major hours is to\nanalyzing links on the Web. We then talked about\nlearning through rank. This is the use of machine learning\nto combine multiple features for improvement scoring. Not only that the effectiveness can be\nimproved in using this approach, but we can also improve the robustness of the. The ranking function so that it's\nnot easy to expand a search engine. It just some features to promote the page. And finally we talked about\nthe future of Web search. About the some major reactions\nthat we might to see in the future in improving the count\nof regeneration of such engines. And then finally we talked about\nthe recommended systems and, these are systems to\nincrement the push mode. And we'll talk about the two approaches,\none is content-based, one is collaborative filtering and\nthey can be combined together. Now, an obvious missing piece in this picture is the user, so user interface is also an important\ncomponent in any search engine. Even though the current search interface\nis relatively simple they actually have done a lot of studies of user interfaces\nwhere we do visualization for example. And this is the topic to that, you can learn more by reading this book. It's an excellent book about all kinds\nof studies of search using the face. If you want to know more about\nthe topics that we talked about, you can also read some additional\nreadings that are listed here. In this short course we only\nmanage to cover some basic topics in text retrievals and\nsearch engines. And these resources provide additional\ninformation about more advanced topics and they give a more thorough treatment of\nsome of the topics that we talked about. And a main source is\nthe Synthesis Digital Library that you can see a lot of short\nto textbook or textbooks, or long tutorials. They tend to provide a lot of\ninformation to explain a topic. And there a lot of series that\nare related to this cause. One is information concepts,\nretrieval, and services. One is human langauge technology. And yet another is artificial\nintelligence and machine learning. There are also some major journals and\nconferences listed here that tend to have a lot of research papers\nwe need to and topic of this course. And finally, for more information\nabout resources Including readings, tool kits, etc you can check out his URL. So, if you have not taken the text\nmining course in this data mining specialization series then naturally\nthe next step is to take that course. As this picture shows,\nto mine big text data, we generally need two kinds of techniques. One is text retrieval,\nwhich is covered in this course. And these techniques will help us\nconvert raw big text data into small relevant text data, which are actually\nneeded in the specific application. Now human plays important role in mining\nany text data because text data is written for humans to consume. So involving humans in the process\nof data mining is very important and in this course we have covered\nthe various strategies to help users get access to the most relevant data. These techniques are always so\nessential in any text mining system to help provide prominence and\nto help users interpret the inner patterns that the user will\ndefine through text data mining. So, in general, the user would have\nto go back to the original data to better understand the patterns. So the text mining cause, or rather,\ntext mining and analytics course will be dealing with what to do once\nthe user has a following information. So this is a second step in this\npicture where we would convert the text data into actionable knowledge. And this has to do with helping users to\nfurther digest the found information or to find the patterns and\nto reveal knowledge. In text and such knowledge can\nthen be used in application systems to help decision making or\nto help a user finish a task. So, if you have not taken that course,\nthe natural step and that natural next step would\nbe to take that course. Thank you for taking this course. I hope you had fun and\nfound this course to be useful to you. And I look forward to interacting\nwith you at a future opportunity. [MUSIC]"
}
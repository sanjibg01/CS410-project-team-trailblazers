{
 "01_introduction-to-text-mining-and-analytics.en.txt": "[SOUND] Hello. Welcome to the course Text Mining and\nAnalytics. My name is ChengXiang Zhai. I have a nickname, Cheng. I am a professor of the Department of\nComputer Science at the University of Illinois at Urbana-Champaign. This course is a part of\na data mining specialization offered by the University of\nIllinois at Urbana-Champaign. In addition to this course,\nthere are four other courses offered by Professor Jiawei Han,\nProfessor John Hart and me, followed by a capstone project course that\nall of us will teach together. This course is particularly related to\nanother course in the specialization, mainly text retrieval and search engines\nin that both courses are about text data. In contrast, pattern discovery and\ncluster analysis are about algorithms more applicable to\nall kinds of data in general. The visualization course is also\nrelatively general in that the techniques can be applied to all kinds of data. This course addresses a pressing need for\nharnessing big text data. Text data has been growing\ndramatically recently, mostly because of the advance of\ntechnologies deployed on the web that would enable people to\nquickly generate text data. So, I listed some of\nthe examples on this slide that can show a variety of text\ndata that are available today. For example, if you think about\nthe data on the internet, on the web, everyday we are seeing many\nweb pages being created. Blogs are another kind\nof new text data that are being generated quickly by people. Anyone can write a blog\narticle on the web. New articles of course have always been a main kind of text data that\nbeing generated everyday. Emails are yet another kind of text data. And literature is also representing\na large portion of text data. It's also especially very important\nbecause of the high quality in the data. That is,\nwe encode our knowledge about the word using text data represented by\nall the literature articles. It's a vast amount of knowledge of all the text and\ndata in these literature articles. Twitter is another representative\ntext data representing social media. Of course there are forums as well. People are generating tweets very quickly indeed as we are speaking perhaps many\npeople have already written many tweets. So, as you can see there\nare all kinds of text data that are being generated very quickly. Now these text data present\nsome challenges for people. It's very hard for anyone to\ndigest all the text data quickly. In particular, it's impossible for\nscientists to read all of the for example or for\nanyone to read all the tweets. So there's a need for tools to help\npeople digest text data more efficiently. There is also another\ninteresting opportunity provided by such big text data, and\nthat is it's possible to leverage the amount of text data to\ndiscover interesting patterns to turn text data into actionable knowledge\nthat can be useful for decision making. So for example,\nproduct managers may be interested in knowing the feedback of\ncustomers about their products, knowing how well their\nproducts are being received as compared with the products of competitors. This can be a good opportunity for leveraging text data as we have seen\na lot of reviews of product on the web. So if we can develop a master text\nmining techniques to tap into such a [INAUDIBLE] to extract the knowledge and\nopinions of people about these products, then we can help these product managers\nto gain business intelligence or to essentially feedback\nfrom their customers. In scientific research, for example, scientists are interested in knowing\nthe trends of research topics, knowing about what related fields have discovered. This problem is especially important\nin biology research as well. Different communities tend to\nuse different terminologies, yet they're starting very similar problems. So how can we integrate the knowledge\nthat is covered in different communities to help study a particular problem? It's very important, and\nit can speed up scientific discovery. So there are many such examples\nwhere we can leverage the text data to discover useable knowledge\nto optimize our decision. The main techniques for harnessing big text data are text\nretrieval and text mining. So these are two very much\nrelated technologies.Yet, they have somewhat different purposes. These two kinds of techniques are covered\nin the tool in this specialization. So, text retrieval on search\nengines covers text retrieval, and this is necessary to\nturn big text data into a much smaller but more relevant text\ndata, which are often the data that we need to handle a particular problem or\nto optimize a particular decision. This course covers text mining which\nis a second step in this pipeline that can be used to further process\nthe small amount of relevant data to extract the knowledge or to help\npeople digest the text data easily. So the two courses are clearly related,\nin fact, some of the techniques are shared by\nboth text retrieval and text mining. If you have already taken the text\nretrieval course, then you might see some of the content being repeated\nin this text mining course, although we'll be talking about the techniques\nfrom a very different perspective. If you have not taken\nthe text retrieval course, it's also fine because this\ncourse is self-contained and you can certainly understand all of\nthe materials without a problem. Of course, you might find it\nbeneficial to take both courses and that will give you a very complete set\nof skills to handle big text data. [MUSIC]",
 "02_course-prerequisites-completion.en.txt": "[MUSIC] This lecture is a brief\nintroduction to the course. We'll then do cover\nthe objectives of the course. The prerequisites and the course format,\nthe reference books and how to complete the course. The objectives of the course\nare the following. First we would like to cover\nthe basic concepts and practical techniques in text data mining. So this means we will not be able to cover\nsome advance the techniques in detail but rather we are going to choose\nthe practical useful techniques and then treat them in more depth. We are going to also cover the basic\nconcepts that are very useful for many applications. The second objective is to cover more\ngeneral techniques for text data mining. So we emphasise the coverage\nof general techniques that can be applicable to any\ntext in any natural language. We also hope that these techniques\nto either automatically work on problems without any human effort\nor only requiring minimum human effort. So these criteria have helped us to choose techniques that can be\napplied to many applications. This is in contrast to some\nmore detailed analysis of text data particularly using natural\nlanguage processing techniques. Now, such techniques\nare also very important. And they are indeed necessary for\nsome of the protections where we would like to go in depth to\nunderstand the text data in more detail. Such detailed understanding\ntechniques however are generally not scalable and they tend to\nrequire a lot of human effort. So they cannot be easy to\napplied into any domain. So as you can imagine in practice it\nwould be beneficial to combine both kinds of techniques using\nthe general techniques that we'll be covering in this course as a basis. And improve these techniques\nby using more human effort whenever it's appropriate\nWe also would like to provide a hands on experience\nto you in Macro aspects. First, you can look at some experience\nwas using a text mining toolkit and implementing text mining algorithms. Second, you have the opportunity to\nexperiment with some algorithms for text reminding and\nphysics to try them on some data sets and to understand how to do experiments. And finally you have\nopportunity to participate in a competition of a text-based\nprediction task. You are expected to know the basic concept\nof computer science for example data structures and some other really\nbasic concepts in computers science. You are also expected to be\nfamiliar with programming and comfortable with programming,\nparticularly with C++. This course however is\nnot about programming, so you are not expected\nto do a lot of coding. but we're going to do give you a C++\ntoolkit that's fairly sophisticated so you have to be comfortable\nwith handling such a toolkit. And you may be asked to write\na small amount of code. It's also useful if you\nknow some concepts and techniques in probability and statistics,\nbut it is not necessary, knowing such analogy will help you understand\nsome of the algorithm in more depth. The format of the course is lectures plus quizzes that will be given\nto you in a regular basis. And there is also an optional\nprogramming assignment. Now, we've made programming assignments\noptional not because it's not important, but because we suspect that not\nall of you will have the need for computing resources to do\nthe programming assignment. So naturally, we would encourage all of you to try\nto do the programming assignment. If possible as that would be a great\nway to learn about the knowledge that would teach in this course. The main reference book for this course\nis a recent book that Sean Massung and I have co-authored. The title is Text Data Management and Analysis A practical introduction to\ninformation retrieval and text mining. However this reference book is not\nrequired in the sense that if we follow all the lecture videos closely then you should have little problems because\nworking on the quiz problems and will program your assignment\nto pass the course. However the book would be useful\nto give you a high level and systematic description of all the topics\ncovered in this course plus some others. It would also help you understand\nsome topics in more depth. So if you problems with following some of\nthe videos the book may be useful to you. The book is also the reference book for\nanother book. If you are interested in buying the book,\nthere's a link here. And there should be substantial discount\nfor the students of this course. There are also quite a few other\nuseful reference books and ratings. And they are available though\nthe link at the bottom of this slide. [MUSIC]",
 "01_1-1-overview-text-mining-and-analytics-part-1.en.txt": "[SOUND]\nIn this lecture we give an overview\nof Text Mining and Analytics. First, let's define the term text mining,\nand the term text analytics. The title of this course is\ncalled Text Mining and Analytics. But the two terms text mining, and text\nanalytics are actually roughly the same. So we are not really going to\nreally distinguish them, and we're going to use them interchangeably. But the reason that we have chosen to use both terms in the title is because\nthere is also some subtle difference, if you look at the two phrases literally. Mining emphasizes more on the process. So it gives us a error rate\nmedical view of the problem. Analytics, on the other hand\nemphasizes more on the result, or having a problem in mind. We are going to look at text\ndata to help us solve a problem. But again as I said, we can treat\nthese two terms roughly the same. And I think in the literature\nyou probably will find the same. So we're not going to really\ndistinguish that in the course. Both text mining and\ntext analytics mean that we want to turn text data into high quality\ninformation, or actionable knowledge. So in both cases, we have the problem of dealing with\na lot of text data and we hope to. Turn these text data into something more\nuseful to us than the raw text data. And here we distinguish\ntwo different results. One is high-quality information,\nthe other is actionable knowledge. Sometimes the boundary between\nthe two is not so clear. But I also want to say a little bit about these two different angles of\nthe result of text field mining. In the case of high quality information,\nwe refer to more concise information about the topic. Which might be much easier for\nhumans to digest than the raw text data. For example, you might face\na lot of reviews of a product. A more concise form of information\nwould be a very concise summary of the major opinions about\nthe features of the product. Positive about,\nlet's say battery life of a laptop. Now this kind of results are very useful\nto help people digest the text data. And so this is to minimize a human effort\nin consuming text data in some sense. The other kind of output\nis actually more knowledge. Here we emphasize the utility\nof the information or knowledge we discover from text data. It's actionable knowledge for some\ndecision problem, or some actions to take. For example, we might be able to determine\nwhich product is more appealing to us, or a better choice for\na shocking decision. Now, such an outcome could be\ncalled actionable knowledge, because a consumer can take the knowledge\nand make a decision, and act on it. So, in this case text mining supplies\nknowledge for optimal decision making. But again, the two are not so\nclearly distinguished, so we don't necessarily have\nto make a distinction. Text mining is also\nrelated to text retrieval, which is a essential component\nin many text mining systems. Now, text retrieval refers to\nfinding relevant information from a large amount of text data. So I've taught another separate MOOC\non text retrieval and search engines. Where we discussed various techniques for\ntext retrieval. If you have taken that MOOC,\nand you will find some overlap. And it will be useful To know\nthe background of text retrieval of understanding some of\nthe topics in text mining. But, if you have not taken that MOOC, it's also fine because in this MOOC\non text mining and analytics, we're going to repeat some of the key concepts\nthat are relevant for text mining. But they're at the high level and they also explain the relation between\ntext retrieval and text mining. Text retrieval is very useful for\ntext mining in two ways. First, text retrieval can be\na preprocessor for text mining. Meaning that it can help\nus turn big text data into a relatively small amount\nof most relevant text data. Which is often what's needed for\nsolving a particular problem. And in this sense, text retrieval\nalso helps minimize human effort. Text retrieval is also needed for\nknowledge provenance. And this roughly corresponds\nto the interpretation of text mining as turning text data\ninto actionable knowledge. Once we find the patterns in text data, or actionable knowledge, we generally\nwould have to verify the knowledge. By looking at the original text data. So the users would have to have some text\nretrieval support, go back to the original text data to interpret the pattern or\nto better understand an analogy or to verify whether a pattern\nis really reliable. So this is a high level introduction\nto the concept of text mining, and the relationship between\ntext mining and retrieval. Next, let's talk about text\ndata as a special kind of data. Now it's interesting to\nview text data as data generated by humans as subjective sensors. So, this slide shows an analogy\nbetween text data and non-text data. And between humans as\nsubjective sensors and physical sensors,\nsuch as a network sensor or a thermometer. So in general a sensor would\nmonitor the real world in some way. It would sense some signal\nfrom the real world, and then would report the signal as data,\nin various forms. For example, a thermometer would watch\nthe temperature of real world and then we report the temperature\nbeing a particular format. Similarly, a geo sensor would sense\nthe location and then report. The location specification, for example, in the form of longitude\nvalue and latitude value. A network sends over\nthe monitor network traffic, or activities in the network and\nare reported. Some digital format of data. Similarly we can think of\nhumans as subjective sensors. That will observe the real world and\nfrom some perspective. And then humans will express what they\nhave observed in the form of text data. So, in this sense, human is actually\na subjective sensor that would also sense what's happening in the world and then express what's observed in the form\nof data, in this case, text data. Now, looking at the text data in\nthis way has an advantage of being able to integrate all\ntypes of data together. And that's indeed needed in\nmost data mining problems. So here we are looking at\nthe general problem of data mining. And in general we would Be\ndealing with a lot of data about our world that\nare related to a problem. And in general it will be dealing with\nboth non-text data and text data. And of course the non-text data\nare usually produced by physical senses. And those non-text data can\nbe also of different formats. Numerical data, categorical,\nor relational data, or multi-media data like video or speech. So, these non text data are often\nvery important in some problems. But text data is also very important, mostly because they contain\na lot of symmetrical content. And they often contain\nknowledge about the users, especially preferences and\nopinions of users. So, but by treating text data as\nthe data observed from human sensors, we can treat all this data\ntogether in the same framework. So the data mining problem is\nbasically to turn such data, turn all the data in your actionable\nknowledge to that we can take advantage of it to change the real\nworld of course for better. So this means the data mining problem is basically taking a lot of data as input\nand giving actionable knowledge as output. Inside of the data mining module,\nyou can also see we have a number of different\nkind of mining algorithms. And this is because, for\ndifferent kinds of data, we generally need different algorithms for\nmining the data. For example, video data might require computer\nvision to understand video content. And that would facilitate\nthe more effective mining. And we also have a lot of general\nalgorithms that are applicable to all kinds of data and those algorithms,\nof course, are very useful. Although, for a particular kind of data, we generally want to also\ndevelop a special algorithm. So this course will cover\nspecialized algorithms that are particularly useful for\nmining text data. [MUSIC]",
 "02_1-2-overview-text-mining-and-analytics-part-2.en.txt": "[SOUND]\nSo, looking at the text mining problem more\nclosely, we see that the problem is similar to general data mining, except\nthat we'll be focusing more on text data. And we're going to have text mining\nalgorithms to help us to turn text data into actionable knowledge that\nwe can use in real world, especially for decision making, or for completing whatever tasks that\nrequire text data to support. Because, in general,\nin many real world problems of data mining we also tend to have other kinds\nof data that are non-textual. So a more general picture would be\nto include non-text data as well. And for this reason we might be\nconcerned with joint mining of text and non-text data. And so in this course we're\ngoing to focus more on text mining, but we're also going to also touch how do\nto joint analysis of both text data and non-text data. With this problem definition we\ncan now look at the landscape of the topics in text mining and analytics. Now this slide shows the process of\ngenerating text data in more detail. More specifically, a human sensor or human observer would look at\nthe word from some perspective. Different people would be looking at\nthe world from different angles and they'll pay attention to different things. The same person at different times might\nalso pay attention to different aspects of the observed world. And so the humans are able to perceive\nthe world from some perspective. And that human, the sensor,\nwould then form a view of the world. And that can be called the Observed World. Of course, this would be different from\nthe Real World because of the perspective that the person has taken\ncan often be biased also. Now the Observed World can be\nrepresented as, for example, entity-relation graphs or\nin a more general way, using knowledge representation language. But in general, this is basically what\na person has in mind about the world. And we don't really know what\nexactly it looks like, of course. But then the human would\nexpress what the person has observed using a natural language,\nsuch as English. And the result is text data. Of course a person could have used\na different language to express what he or she has observed. In that case we might have text data of\nmixed languages or different languages. The main goal of text mining\nIs actually to revert this process of generating text data. We hope to be able to uncover\nsome aspect in this process. Specifically, we can think about mining,\nfor example, knowledge about the language. And that means by looking at text data\nin English, we may be able to discover something about English, some usage\nof English, some patterns of English. So this is one type of mining problems,\nwhere the result is some knowledge about language which\nmay be useful in various ways. If you look at the picture, we can also then mine knowledge\nabout the observed world. And so this has much to do with\nmining the content of text data. We're going to look at what the text\ndata are about, and then try to get the essence of it or\nextracting high quality information about a particular aspect of\nthe world that we're interested in. For example, everything that has been\nsaid about a particular person or a particular entity. And this can be regarded as mining content to describe the observed world in\nthe user's mind or the person's mind. If you look further,\nthen you can also imagine we can mine knowledge about this observer,\nhimself or herself. So this has also to do with\nusing text data to infer some properties of this person. And these properties could\ninclude the mood of the person or sentiment of the person. And note that we distinguish\nthe observed word from the person because text data can't describe what the\nperson has observed in an objective way. But the description can be also\nsubjected with sentiment and so, in general, you can imagine the text\ndata would contain some factual descriptions of the world plus\nsome subjective comments. So that's why it's also possible to do text mining to mine\nknowledge about the observer. Finally, if you look at the picture\nto the left side of this picture, then you can see we can certainly also\nsay something about the real world. Right? So indeed we can do text mining to\ninfer other real world variables. And this is often called\na predictive analytics. And we want to predict the value\nof certain interesting variable. So, this picture basically covered multiple types of knowledge that\nwe can mine from text in general. When we infer other\nreal world variables we could also use some of the results from mining text data as intermediate\nresults to help the prediction. For example, after we mine the content of text data we\nmight generate some summary of content. And that summary could be then used to help us predict the variables\nof the real world. Now of course this is still generated\nfrom the original text data, but I want to emphasize here that\noften the processing of text data to generate some features that can help\nwith the prediction is very important. And that's why here we show the results of some other mining tasks, including\nmining the content of text data and mining knowledge about the observer,\ncan all be very helpful for prediction. In fact, when we have non-text data,\nwe could also use the non-text data to help prediction, and\nof course it depends on the problem. In general, non-text data can be very\nimportant for such prediction tasks. For example,\nif you want to predict stock prices or changes of stock prices based on\ndiscussion in the news articles or in social media, then this is an example of using text data to predict\nsome other real world variables. But in this case, obviously, the historical stock price data would\nbe very important for this prediction. And so that's an example of\nnon-text data that would be very useful for the prediction. And we're going to combine both kinds\nof data to make the prediction. Now non-text data can be also used for\nanalyzing text by supplying context. When we look at the text data alone, we'll be mostly looking at the content\nand/or opinions expressed in the text. But text data generally also\nhas context associated. For example, the time and the location\nthat associated are with the text data. And these are useful context information. And the context can provide interesting\nangles for analyzing text data. For example, we might partition text\ndata into different time periods because of the availability of the time. Now we can analyze text data in each\ntime period and then make a comparison. Similarly we can partition text\ndata based on locations or any meta data that's associated to\nform interesting comparisons in areas. So, in this sense,\nnon-text data can actually provide interesting angles or\nperspectives for text data analysis. And it can help us make context-sensitive analysis of content or\nthe language usage or the opinions about the observer or\nthe authors of text data. We could analyze the sentiment\nin different contexts. So this is a fairly general landscape of\nthe topics in text mining and analytics. In this course we're going to\nselectively cover some of those topics. We actually hope to cover\nmost of these general topics. First we're going to cover\nnatural language processing very briefly because this has to do\nwith understanding text data and this determines how we can represent\ntext data for text mining. Second, we're going to talk about how to\nmine word associations from text data. And word associations is a form of use for\nlexical knowledge about a language. Third, we're going to talk about\ntopic mining and analysis. And this is only one way to\nanalyze content of text, but it's a very useful ways\nof analyzing content. It's also one of the most useful\ntechniques in text mining. Then we're going to talk about\nopinion mining and sentiment analysis. So this can be regarded as one example\nof mining knowledge about the observer. And finally we're going to\ncover text-based prediction problems where we try to predict some\nreal world variable based on text data. So this slide also serves as\na road map for this course. And we're going to use\nthis as an outline for the topics that we'll cover\nin the rest of this course. [MUSIC]",
 "03_1-3-natural-language-content-analysis-part-1.en.txt": "[SOUND] This lecture is about natural language content analysis. Natural language content analysis\nis the foundation of text mining. So we're going to first talk about this. And in particular, natural language processing with\na factor how we can present text data. And this determines what algorithms can\nbe used to analyze and mine text data. We're going to take a look at the basic\nconcepts in natural language first. And I'm going to explain these concepts using a similar example\nthat you've all seen here. A dog is chasing a boy on the playground. Now this is a very simple sentence. When we read such a sentence\nwe don't have to think about it to get the meaning of it. But when a computer has to\nunderstand the sentence, the computer has to go\nthrough several steps. First, the computer needs\nto know what are the words, how to segment the words in English. And this is very easy,\nwe can just look at the space. And then the computer will need\nthe know the categories of these words, syntactical categories. So for example, dog is a noun,\nchasing's a verb, boy is another noun etc. And this is called a Lexical analysis. In particular, tagging these words\nwith these syntactic categories is called a part-of-speech tagging. After that the computer also needs to\nfigure out the relationship between these words. So a and dog would form a noun phrase. On the playground would be\na prepositional phrase, etc. And there is certain way for\nthem to be connected together in order for them to create meaning. Some other combinations\nmay not make sense. And this is called syntactical parsing, or syntactical analysis,\nparsing of a natural language sentence. The outcome is a parse tree\nthat you are seeing here. That tells us the structure\nof the sentence, so that we know how we can\ninterpret this sentence. But this is not semantics yet. So in order to get the meaning we\nwould have to map these phrases and these structures into some real world\nantithesis that we have in our mind. So dog is a concept that we know,\nand boy is a concept that we know. So connecting these phrases\nthat we know is understanding. Now for a computer, would have to formally\nrepresent these entities by using symbols. So dog, d1 means d1 is a dog. Boy, b1 means b1 refers to a boy etc. And also represents the chasing\naction as a predicate. So, chasing is a predicate here with three arguments, d1, b1, and p1. Which is playground. So this formal rendition of\nthe semantics of this sentence. Once we reach that level of understanding,\nwe might also make inferences. For example, if we assume there's a rule\nthat says if someone's being chased then the person can get scared, then we\ncan infer this boy might be scared. This is the inferred meaning,\nbased on additional knowledge. And finally, we might even further infer what this sentence is requesting, or why the person who say it in\na sentence, is saying the sentence. And so, this has to do with\npurpose of saying the sentence. This is called speech act analysis or\npragmatic analysis. Which first to the use of language. So, in this case a person saying this\nmay be reminding another person to bring back the dog. So this means when saying a sentence,\nthe person actually takes an action. So the action here is to make a request. Now, this slide clearly shows that\nin order to really understand a sentence there are a lot of\nthings that a computer has to do. Now, in general it's very hard for\na computer will do everything, especially if you would want\nit to do everything correctly. This is very difficult. Now, the main reason why natural\nlanguage processing is very difficult, it's because it's designed it will\nmake human communications efficient. As a result, for example,\nwith only a lot of common sense knowledge. Because we assume all of\nus have this knowledge, there's no need to encode this knowledge. That makes communication efficient. We also keep a lot of ambiguities,\nlike, ambiguities of words. And this is again, because we assume we\nhave the ability to disambiguate the word. So, there's no problem with\nhaving the same word to mean possibly different things\nin different context. Yet for\na computer this would be very difficult because a computer does not have\nthe common sense knowledge that we do. So the computer will be confused indeed. And this makes it hard for\nnatural language processing. Indeed, it makes it very hard for every step in the slide\nthat I showed you earlier. Ambiguity is a main killer. Meaning that in every step\nthere are multiple choices, and the computer would have to\ndecide whats the right choice and that decision can be very difficult\nas you will see also in a moment. And in general, we need common sense reasoning in order\nto fully understand the natural language. And computers today don't yet have that. That's why it's very hard for computers to precisely understand\nthe natural language at this point. So here are some specific\nexamples of challenges. Think about the world-level ambiguity. A word like design can be a noun or\na verb, so we've got ambiguous part of speech tag. Root also has multiple meanings,\nit can be of mathematical sense, like in the square of, or\ncan be root of a plant. Syntactic ambiguity refers\nto different interpretations of a sentence in terms structures. So for example, natural language processing can\nactually be interpreted in two ways. So one is the ordinary meaning that we will be getting as we're\ntalking about this topic. So, it's processing of natural language. But there's is also another\npossible interpretation which is to say language\nprocessing is natural. Now we don't generally have this problem,\nbut imagine for the computer to determine the structure, the computer would have\nto make a choice between the two. Another classic example is a man\nsaw a boy with a telescope. And this ambiguity lies in\nthe question who had the telescope? This is called a prepositional\nphrase attachment ambiguity. Meaning where to attach this\nprepositional phrase with the telescope. Should it modify the boy? Or should it be modifying, saw, the verb. Another problem is anaphora resolution. In John persuaded Bill to buy a TV for\nhimself. Does himself refer to John or Bill? Presupposition is another difficulty. He has quit smoking implies\nthat he smoked before, and we need to have such a knowledge in\norder to understand the languages. Because of these problems, the state\nof the art natural language processing techniques can not do anything perfectly. Even for\nthe simplest part of speech tagging, we still can not solve the whole problem. The accuracy that are listed here,\nwhich is about 97%, was just taken from some studies earlier. And these studies obviously have to\nbe using particular data sets so the numbers here are not\nreally meaningful if you take it out of the context of the data\nset that are used for evaluation. But I show these numbers mainly to give\nyou some sense about the accuracy, or how well we can do things like this. It doesn't mean any data set\naccuracy would be precisely 97%. But, in general, we can do parsing speech\ntagging fairly well although not perfect. Parsing would be more difficult, but for\npartial parsing, meaning to get some phrases correct, we can probably\nachieve 90% or better accuracy. But to get the complete parse tree\ncorrectly is still very, very difficult. For semantic analysis, we can also do\nsome aspects of semantic analysis, particularly, extraction of entities and\nrelations. For example, recognizing this is\nthe person, that's a location, and this person and\nthat person met in some place etc. We can also do word sense to some extent. The occurrence of root in this sentence\nrefers to the mathematical sense etc. Sentiment analysis is another aspect\nof semantic analysis that we can do. That means we can tag the senses\nas generally positive when it's talking about the product or\ntalking about the person. Inference, however, is very hard,\nand we generally cannot do that for any big domain and if it's only\nfeasible for a very limited domain. And that's a generally difficult\nproblem in artificial intelligence. Speech act analysis is\nalso very difficult and we can only do this probably for\nvery specialized cases. And with a lot of help from humans\nto annotate enough data for the computers to learn from. So the slide also shows that computers are far from being able to\nunderstand natural language precisely. And that also explains why the text\nmining problem is difficult. Because we cannot rely on\nmechanical approaches or computational methods to\nunderstand the language precisely. Therefore, we have to use\nwhatever we have today. A particular statistical machine learning\nmethod of statistical analysis methods to try to get as much meaning\nout from the text as possible. And, later you will see\nthat there are actually many such algorithms\nthat can indeed extract interesting model from text even though\nwe cannot really fully understand it. Meaning of all the natural\nlanguage sentences precisely. [MUSIC]",
 "04_1-4-natural-language-content-analysis-part-2.en.txt": "[SOUND] So here are some specific examples of what we can't do today and part of speech tagging is still\nnot easy to do 100% correctly. So in the example, he turned off the\nhighway verses he turned off the fan and the two offs actually have somewhat\na differentness in their active categories and also its very difficult\nto get a complete the parsing correct. Again, the example, a man saw a boy\nwith a telescope can actually be very difficult to parse\ndepending on the context. Precise deep semantic\nanalysis is also very hard. For example, to define the meaning of own, precisely is very difficult in\nthe sentence, like John owns a restaurant. So the state of the off can\nbe summarized as follows. Robust and general NLP tends to be shallow while\na deep understanding does not scale up. For this reason in this course,\nthe techniques that we cover are in general, shallow techniques for\nanalyzing text data and mining text data and they are generally\nbased on statistical analysis. So there are robust and\ngeneral and they are in the in category of shallow analysis. So such techniques have\nthe advantage of being able to be applied to any text data in\nany natural about any topic. But the downside is that, they don't\ngive use a deeper understanding of text. For that, we have to rely on\ndeeper natural language analysis. That typically would require\na human effort to annotate a lot of examples of analysis that would\nlike to do and then computers can use machine learning techniques and learn from\nthese training examples to do the task. So in practical applications, we generally\ncombine the two kinds of techniques with the general statistical and\nmethods as a backbone as the basis. These can be applied to any text data. And on top of that, we're going to use\nhumans to, and you take more data and to use supervised machine learning\nto do some tasks as well as we can, especially for those important\ntasks to bring humans into the loop to analyze text data more precisely. But this course will cover\nthe general statistical approaches that generally,\ndon't require much human effort. So they're practically,\nmore useful that some of the deeper analysis techniques that require a lot of\nhuman effort to annotate the text today. So to summarize,\nthe main points we take are first NLP is the foundation for text mining. So obviously, the better we\ncan understand the text data, the better we can do text mining. Computers today are far from being able\nto understand the natural language. Deep NLP requires common sense\nknowledge and inferences. Thus, only working for\nvery limited domains not feasible for large scale text mining. Shallow NLP based on statistical\nmethods can be done in large scale and is the main topic of this course and they are generally applicable\nto a lot of applications. They are in some sense also,\nmore useful techniques. In practice,\nwe use statistical NLP as the basis and we'll have humans for\nhelp as needed in various ways. [MUSIC]",
 "05_1-5-text-representation-part-1.en.txt": "This lecture is about the\ntextual representation. In this lecture, we are going to discuss textual\nrepresentation, and discuss how natural\nlanguage processing can allow us to represent text\nin many different ways. Let's take a look at this\nexample sentence again. We can represent this sentence\nin many different ways. First, we can always represent such a sentence\nas a string of characters. This is true for\nall the languages when we store them\nin the computer. When we store a natural\nlanguage sentence as a string of characters, we have perhaps the most general\nway of representing text since we always use this approach to\nrepresent any text data. But unfortunately, using\nsuch a representation will not help us to do semantic analysis, which is often needed for many applications\nof text mining. The reason is because we're\nnot even recognizing words. So as a string, we're going to keep\nall the spaces and these ASCII symbols. We can perhaps count what's the most frequent character\nin English text, or the correlation\nbetween those characters, but we can't really\nanalyze semantics. Yet, this is the most\ngeneral way of representing text because we can use this to represent any\nnatural language text. If we try to do a little bit more natural\nlanguage processing by doing word segmentation, then we can obtain a\nrepresentation of the same text, but in the form of a\nsequence of words. So here we see that\nwe can identify words like a dog is chasing etc. Now with this level\nof representation, we certainly can do\na lot of things, and this is mainly because\nwords are the basic units of human communication\nin natural language, so they are very powerful. By identifying words, we can for example easily count what are the most frequent words in this document or in\nthe whole collection etc. These words can be used to form topics when we combine\nrelated words together, and some words are positive, some words negative, so we can also do sentiment analysis. So representing text data\nas a sequence of words opens up a lot of interesting\nanalysis possibilities. However, this level of representation is slightly\nless general than string of characters because in\nsome languages such as Chinese, it's actually not\nthat easy to identify all the word boundaries\nbecause in such a language, you see text as a sequence of characters with\nno space in between. So you'll have to rely on some special techniques\nto identify words. In such a language,\nof course then, we might make mistakes\nin segmenting words. So the sequence of\nwords representation is not as robust as\nstring of characters. But in English, it's very easy to obtain this level\nof representation, so we can do that all the time. Now, if we go further to do naturally\nlanguage processing, we can add a part of speech tags. Now once we do that, we can count, for example, the most frequent\nnouns or what kind of nouns are associated with\nwhat kind of verbs etc. So this opens up a little bit more\ninteresting opportunities for further analysis. Note that I use a plus sign\nhere because by representing text as a sequence\nof part of speech tags, we don't necessarily replace the original word\nsequence written. Instead, we add this as an additional way of\nrepresenting text data, so that now the data is\nrepresented as both a sequence of words and a sequence\nof part of speech tags. This enriches the\nrepresentation of text data, and thus also enables\nmore interesting analysis. If we go further, then we'll\nbe pausing the sentence often to obtain\na syntactic structure. Now this of course, further open up\na more interesting analysis of, for example, the writing styles or\ncorrecting grammar mistakes. If we go further for\nsemantic analysis, then we might be able to\nrecognize dog as an animal, and we also can recognize\na boy as a person, and playground as a location. We can further analyze\ntheir relations, for example, dog is chasing the boy and\nthe boy is on the playground. Now this will add\nmore entities and relations through\nentity relation recreation. At this level, then we can do even more\ninteresting things. For example, now we\ncan count easily the most frequent person that's mentioning this whole collection\nof news articles, or whenever you\nmention this person, you also tend to see mentioning\nof another person etc. So this is a very\nuseful representation, and it's also related to the knowledge graph that\nsome of you may have heard of that Google is doing as a more semantic way of\nrepresenting text data. However, it's also less robust\nthan sequence of words or even syntactical analysis\nbecause it's not always easy to identify all the entities with\nthe right types, and we might make mistakes, and relations are\neven harder to find, and we might make mistakes. So this makes this level of\nrepresentation less robust, yet it's very useful. Now if we move further\nto logical condition, then we can have predicates\nand even inference rules. With inference rules, we can infer interesting derived\nfacts from the text, so that's very useful. But unfortunately,\nat this level of representation is even less robust and we can make mistakes and we can't do that all the time for\nall kinds of sentences. Finally, speech acts would\nadd a yet another level of repetition of the intent\nof saying this sentence. So in this case, it might be a request. So knowing that would\nallow us to analyze even more interesting\nthings about this observer or the author\nof this sentence. What's the intention\nof saying that? What's scenarios? What kind\nof actions would be made? So this is another level of analysis that would\nbe very interesting. So this picture shows\nthat if we move down, we generally see\nmore sophisticated natural language processing\ntechniques to be used. Unfortunately,\nsuch techniques would require more human effort, and they are less accurate. That means there are mistakes. So if we add an texts that are at the levels that are representing deeper\nanalysis of language, then we have to\ntolerate the errors. So that also means it's\nstill necessary to combine such deep analysis with\nshallow analysis based on, for example, sequence of words. On the right side, you'll see the arrow points\ndown to indicate that. As we go down, we are representation\nof text is closer to knowledge representation\nin our mind, and need for solving\na lot of problems. Now this is desirable because as we can represent text at\nthe level of knowledge, we can easily extract\nthe knowledge. That's the purpose\nof text-mining. So there is a trade-off here between doing\na deeper analysis that might have errors but would give us direct knowledge that\ncan be extracted from text. Doing shallow analysis, which is more robust but\nwouldn't actually give us the necessary deeper\nrepresentation of knowledge. I should also say that\ntext data are generated by humans and are meant to\nbe consumed by humans. So as a result, in\ntext data analysis, text-mining humans play\na very important role, they are always in the loop. Meaning that we should optimize the collaboration of\nhumans and computers. So in that sense, it's okay that computers\nmay not be able to have compute accurately\nrepresentation of text data, and the patterns\nthat are extracted from text data can be\ninterpreted by humans, and humans can\nguide the computers to do more accurate analysis\nby annotating more data, by providing features\nto guide a machine learning programs to make\nthem work more effectively.",
 "06_1-6-text-representation-part-2.en.txt": "[SOUND]. So, as we explained the different text representation tends to\nenable different analysis. In particular,\nwe can gradually add more and more deeper analysis results\nto represent text data. And that would open up a more\ninteresting representation opportunities and\nalso analysis capacities. So, this table summarizes\nwhat we have just seen. So the first column shows\nthe text representation. The second visualizes the generality\nof such a representation. Meaning whether we can do this\nkind of representation accurately for all the text data or only some of them. And the third column shows\nthe enabled analysis techniques. And the final column shows some\nexamples of application that can be achieved through this\nlevel of representation. So let's take a look at them. So as a stream text can only be processed\nby stream processing algorithms. It's very robust, it's general. And there was still some interesting\napplications that can be down at this level. For example, compression of text. Doesn't necessarily need to\nknow the word boundaries. Although knowing word boundaries\nmight actually also help. Word base repetition is a very\nimportant level of representation. It's quite general and relatively robust, indicating they\nwere a lot of analysis techniques. Such as word relation analysis,\ntopic analysis and sentiment analysis. And there are many applications that can\nbe enabled by this kind of analysis. For example, thesaurus discovery has\nto do with discovering related words. And topic and\nopinion related applications are abounded. And there are, for example, people might be interesting in knowing the major\ntopics covered in the collection of texts. And this can be the case\nin research literature. And scientists want to know what are the\nmost important research topics today. Or customer service people might want to\nknow all our major complaints from their customers by mining their e-mail messages. And business intelligence\npeople might be interested in understanding consumers' opinions about\ntheir products and the competitors' products to figure out what are the\nwinning features of their products. And, in general, there are many applications that can be enabled by\nthe representation at this level. Now, moving down, we'll see we can\ngradually add additional representations. By adding syntactical structures,\nwe can enable, of course, syntactical graph analysis. We can use graph mining algorithms\nto analyze syntactic graphs. And some applications are related\nto this kind of representation. For example, stylistic analysis generally requires\nsyntactical structure representation. We can also generate\nthe structure based features. And those are features that might help us\nclassify the text objects into different categories by looking at the structures\nsometimes in the classification. It can be more accurate. For example,\nif you want to classify articles into different categories corresponding\nto different authors. You want to figure out which of\nthe k authors has actually written this article, then you generally need\nto look at the syntactic structures. When we add entities and relations, then we can enable other techniques\nsuch as knowledge graph and answers, or information network and\nanswers in general. And this analysis enable\napplications about entities. For example, discovery of all the knowledge and\nopinions about real world entities. You can also use this level representation to integrate everything about\nanything from scaled resources. Finally, when we add logical predicates, that would enable large inference,\nof course. And this can be very useful for integrating analysis of\nscattered knowledge. For example,\nwe can also add ontology on top of the, extracted the information from text,\nto make inferences. A good of example of application in this\nenabled by this level of representation, is a knowledge assistant for biologists. And this program that can help a biologist\nmanage all the relevant knowledge from literature about a research problem such\nas understanding functions of genes. And the computer can make inferences about some of the hypothesis that\nthe biologist might be interesting. For example,\nwhether a gene has a certain function, and then the intelligent program can read the\nliterature to extract the relevant facts, doing compiling and\ninformation extracting. And then using a logic system to\nactually track that's the answers to researchers questioning about what\ngenes are related to what functions. So in order to support\nthis level of application we need to go as far as\nlogical representation. Now, this course is covering techniques\nmainly based on word based representation. And these techniques are general and robust and that's more widely\nused in various applications. In fact, in virtually all the text mining\napplications you need this level of representation and then techniques that\nsupport analysis of text in this level. But obviously all these other\nlevels can be combined and should be combined in order to support\nthe sophisticated applications. So to summarize,\nhere are the major takeaway points. Text representation determines what\nkind of mining algorithms can be applied. And there are multiple ways to\nrepresent the text, strings, words, syntactic structures, entity-relation\ngraphs, knowledge predicates, etc. And these different\nrepresentations should in general be combined in real applications\nto the extent we can. For example, even if we cannot\ndo accurate representations of syntactic structures, we can state\nthat partial structures strictly. And if we can recognize some entities,\nthat would be great. So in general we want to\ndo as much as we can. And when different levels\nare combined together, we can enable a richer analysis,\nmore powerful analysis. This course however focuses\non word-based representation. Such techniques have also several\nadvantage, first of they are general and robust, so they are applicable\nto any natural language. That's a big advantage over\nother approaches that rely on more fragile natural language\nprocessing techniques. Secondly, it does not require\nmuch manual effort, or sometimes, it does not\nrequire any manual effort. So that's, again, an important benefit, because that means that you can apply\nit directly to any application. Third, these techniques are actually\nsurprisingly powerful and effective form in implications. Although not all of course\nas I just explained. Now they are very effective\npartly because the words are invented by humans as basically\nunits for communications. So they are actually quite sufficient for\nrepresenting all kinds of semantics. So that makes this kind of word-based\nrepresentation all so powerful. And finally, such a word-based\nrepresentation and the techniques enable by such a representation can be combined\nwith many other sophisticated approaches. So they're not competing with each other. [MUSIC]",
 "07_1-7-word-association-mining-and-analysis.en.txt": "[SOUND] This lecture is about the word association mining and analysis. In this lecture,\nwe're going to talk about how to mine associations of words from text. Now this is an example of knowledge\nabout the natural language that we can mine from text data. Here's the outline. We're going to first talk about\nwhat is word association and then explain why discovering such\nrelations is useful and finally we're going to talk about some general\nideas about how to mine word associations. In general there are two word\nrelations and these are quite basic. One is called a paradigmatic relation. The other is syntagmatic relation. A and B have paradigmatic relation if they can be substituted for each other. That means the two words that\nhave paradigmatic relation would be in the same semantic class,\nor syntactic class. And we can in general\nreplace one by the other without affecting\nthe understanding of the sentence. That means we would still\nhave a valid sentence. For example, cat and dog, these two\nwords have a paradigmatic relation because they are in\nthe same class of animal. And in general,\nif you replace cat with dog in a sentence, the sentence would still be a valid\nsentence that you can make sense of. Similarly Monday and\nTuesday have paradigmatical relation. The second kind of relation is\ncalled syntagmatical relation. In this case, the two words that have this\nrelation, can be combined with each other. So A and B have syntagmatic relation if\nthey can be combined with each other in a sentence, that means these two\nwords are semantically related. So for example, cat and sit are related\nbecause a cat can sit somewhere. Similarly, car and\ndrive are related semantically and they can be combined with\neach other to convey meaning. However, in general, we can not\nreplace cat with sit in a sentence or car with drive in the sentence\nto still get a valid sentence, meaning that if we do that, the sentence\nwill become somewhat meaningless. So this is different from\nparadigmatic relation. And these two relations are in fact so\nfundamental that they can be generalized to capture basic relations\nbetween units in arbitrary sequences. And definitely they can be\ngeneralized to describe relations of any items in a language. So, A and B don't have to be words and\nthey can be phrases, for example. And they can even be more complex\nphrases than just a non-phrase. If you think about the general\nproblem of the sequence mining then we can think about the units\nbeing and the sequence data. Then we think of paradigmatic\nrelation as relations that are applied to units that tend to occur\nin a singular locations in a sentence, or in a sequence of data\nelements in general. So they occur in similar locations\nrelative to the neighbors in the sequence. Syntagmatical relation on\nthe other hand is related to co-occurrent elements that tend\nto show up in the same sequence. So these two are complimentary and\nare basic relations of words. And we're interested in discovering\nthem automatically from text data. Discovering such worded\nrelations has many applications. First, such relations can be directly\nuseful for improving accuracy of many NLP tasks, and this is because this is part\nof our knowledge about a language. So if you know these two words\nare synonyms, for example, and then you can help a lot of tasks. And grammar learning can be also\ndone by using such techniques. Because if we can learn\nparadigmatic relations, then we form classes of words,\nsyntactic classes for example. And if we learn syntagmatic relations,\nthen we would be able to know the rules for putting together a larger\nexpression based on component expressions. So we learn the structure and\nwhat can go with what else. Word relations can be also very useful for many applications in text retrieval and\nmining. For example, in search and\ntext retrieval, we can use word associations to modify a query,\nand this can be used to introduce additional related words into\na query and make the query more effective. It's often called a query expansion. Or you can use related words to\nsuggest related queries to the user to explore the information space. Another application is to\nuse word associations to automatically construct the top\nof the map for browsing. We can have words as nodes and\nassociations as edges. A user could navigate from\none word to another to find information in the information space. Finally, such word associations can also\nbe used to compare and summarize opinions. For example, we might be interested\nin understanding positive and negative opinions about the iPhone 6. In order to do that, we can look at what\nwords are most strongly associated with a feature word like battery in\npositive versus negative reviews. Such a syntagmatical\nrelations would help us show the detailed opinions\nabout the product. So, how can we discover such\nassociations automatically? Now, here are some intuitions\nabout how to do that. Now let's first look at\nthe paradigmatic relation. Here we essentially can take\nadvantage of similar context. So here you see some simple\nsentences about cat and dog. You can see they generally\noccur in similar context, and that after all is the definition\nof paradigmatic relation. On the right side you can kind\nof see I extracted expressly the context of cat and\ndog from this small sample of text data. I've taken away cat and\ndog from these sentences, so that you can see just the context. Now, of course we can have different\nperspectives to look at the context. For example, we can look at\nwhat words occur in the left part of this context. So we can call this left context. What words occur before we see cat or dog? So, you can see in this case, clearly\ndog and cat have similar left context. You generally say his cat or my cat and\nyou say also, my dog and his dog. So that makes them similar\nin the left context. Similarly, if you look at the words\nthat occur after cat and dog, which we can call right context,\nthey are also very similar in this case. Of course, it's an extreme case,\nwhere you only see eats. And in general,\nyou'll see many other words, of course, that can't follow cat and dog. You can also even look\nat the general context. And that might include all\nthe words in the sentence or in sentences around this word. And even in the general context, you also\nsee similarity between the two words. So this was just a suggestion\nthat we can discover paradigmatic relation by looking at\nthe similarity of context of words. So, for example,\nif we think about the following questions. How similar are context of cat and\ncontext of dog? In contrast how similar are context\nof cat and context of computer? Now, intuitively,\nwe're to imagine the context of cat and the context of dog would\nbe more similar than the context of cat and\ncontext of the computer. That means, in the first case\nthe similarity value would be high, between the context of cat and\ndog, where as in the second, the similarity between context of cat and\ncomputer would be low because they all not having a paradigmatic relationship and imagine what words\noccur after computer in general. It would be very different from\nwhat words occur after cat. So this is the basic idea of what\nthis covering, paradigmatic relation. What about the syntagmatic relation? Well, here we're going to explore\nthe correlated occurrences, again based on the definition\nof syntagmatic relation. Here you see the same sample of text. But here we're interested in knowing\nwhat other words are correlated with the verb eats and\nwhat words can go with eats. And if you look at the right\nside of this slide and you see,\nI've taken away the two words around eats. I've taken away the word to its left and also the word to its\nright in each sentence. And then we ask the question, what words\ntend to occur to the left of eats? And what words tend to\noccur to the right of eats? Now thinking about this question\nwould help us discover syntagmatic relations because syntagmatic relations\nessentially captures such correlations. So the important question to ask for\nsyntagmatical relation is, whenever eats occurs,\nwhat other words also tend to occur? So the question here has\nto do with whether there are some other words that tend\nto co-occur together with each. Meaning that whenever you see eats\nyou tend to see the other words. And if you don't see eats, probably,\nyou don't see other words often either. So this intuition can help\ndiscover syntagmatic relations. Now again, consider example. How helpful is occurrence of eats for\npredicting occurrence of meat? Right.\nAll right, so knowing whether eats occurs in a sentence would generally help us\npredict whether meat also occurs indeed. And if we see eats occur in the sentence,\nand that should increase the chance\nthat meat would also occur. In contrast,\nif you look at the question in the bottom, how helpful is the occurrence of eats for\npredicting of occurrence of text? Because eats and\ntext are not really related, so knowing whether eats occurred\nin the sentence doesn't really help us predict the weather,\ntext also occurs in the sentence. So this is in contrast to\nthe question about eats and meat. This also helps explain that intuition behind the methods of what\ndiscovering syntagmatic relations. Mainly we need to capture the correlation\nbetween the occurrences of two words. So to summarize the general ideas for discovering word associations\nare the following. For paradigmatic relation,\nwe present each word by its context. And then compute its context similarity. We're going to assume the words\nthat have high context similarity to have paradigmatic relation. For syntagmatic relation, we will count\nhow many times two words occur together in a context, which can be a sentence,\na paragraph, or a document even. And we're going to compare their co-occurrences with\ntheir individual occurrences. We're going to assume words\nwith high co-occurrences but relatively low individual occurrences\nto have syntagmatic relations because they attempt to occur together and\nthey don't usually occur alone. Note that the paradigmatic relation and\nthe syntagmatic relation are actually closely related\nin that paradigmatically related words tend to have syntagmatic\nrelation with the same word. They tend to be associated\nwith the same word, and that suggests that we can also do join\nthe discovery of the two relations. So these general ideas can be\nimplemented in many different ways. And the course won't cover all of them,\nbut we will cover at least some of\nthe methods that are effective for discovering these relations. [MUSIC]",
 "08_1-8-paradigmatic-relation-discovery-part-1.en.txt": "[SOUND]\nThis lecture is about\nthe Paradigmatics Relation Discovery. In this lecture we are going to talk about\nhow to discover a particular kind of word association called\na paradigmatical relation. By definition,\ntwo words are paradigmatically related if they share a similar context. Namely, they occur in\nsimilar positions in text. So naturally our idea of discovering such\na relation is to look at the context of each word and then try to compute\nthe similarity of those contexts. So here is an example of\ncontext of a word, cat. Here I have taken the word\ncat out of the context and you can see we are seeing some remaining\nwords in the sentences that contain cat. Now, we can do the same thing for\nanother word like dog. So in general we would like to capture\nsuch a context and then try to assess the similarity of the context of cat and\nthe context of a word like dog. So now the question is how can we\nformally represent the context and then define the similarity function. So first, we note that the context\nactually contains a lot of words. So, they can be regarded as\na pseudo document, a imagine document, but there are also different\nways of looking at the context. For example, we can look at the word\nthat occurs before the word cat. We can call this context Left1 context. All right, so in this case you\nwill see words like my, his, or big, a, the, et cetera. These are the words that can\noccur to left of the word cat. So we say my cat, his cat,\nbig cat, a cat, et cetera. Similarly, we can also collect the words\nthat occur right after the word cat. We can call this context Right1, and here we see words like eats,\nate, is, has, et cetera. Or, more generally, we can look at all the words in\nthe window of text around the word cat. Here, let's say we can take a window\nof 8 words around the word cat. We call this context Window8. Now, of course, you can see all\nthe words from left or from right, and so we'll have a bag of words in\ngeneral to represent the context. Now, such a word based representation\nwould actually give us an interesting way to define the\nperspective of measuring the similarity. Because if you look at just\nthe similarity of Left1, then we'll see words that share\njust the words in the left context, and we kind of ignored the other words\nthat are also in the general context. So that gives us one perspective to\nmeasure the similarity, and similarly, if we only use the Right1 context, we will capture this narrative\nfrom another perspective. Using both the Left1 and\nRight1 of course would allow us to capture the similarity with even\nmore strict criteria. So in general, context may contain\nadjacent words, like eats and my, that you see here, or\nnon-adjacent words, like Saturday, Tuesday, or\nsome other words in the context. And this flexibility also allows us\nto match the similarity in somewhat different ways. Sometimes this is useful, as we might want to capture\nsimilarity base on general content. That would give us loosely\nrelated paradigmatical relations. Whereas if you use only the words\nimmediately to the left and to the right of the word, then you\nlikely will capture words that are very much related by their syntactical\ncategories and semantics. So the general idea of discovering\nparadigmatical relations is to compute the similarity\nof context of two words. So here, for example,\nwe can measure the similarity of cat and dog based on the similarity\nof their context. In general, we can combine all\nkinds of views of the context. And so the similarity function is,\nin general, a combination of similarities\non different context. And of course, we can also assign\nweights to these different similarities to allow us to focus\nmore on a particular kind of context. And this would be naturally\napplication specific, but again, here the main idea for discovering\npardigmatically related words is to computer the similarity\nof their context. So next let's see how we exactly\ncompute these similarity functions. Now to answer this question,\nit is useful to think of bag of words representation as vectors\nin a vector space model. Now those of you who have been\nfamiliar with information retrieval or textual retrieval techniques would\nrealize that vector space model has been used frequently for\nmodeling documents and queries for search. But here we also find it convenient\nto model the context of a word for paradigmatic relation discovery. So the idea of this\napproach is to view each word in our vocabulary as defining one\ndimension in a high dimensional space. So we have N words in\ntotal in the vocabulary, then we have N dimensions,\nas illustrated here. And on the bottom, you can see a frequency\nvector representing a context, and here we see where eats\noccurred 5 times in this context, ate occurred 3 times, et cetera. So this vector can then be placed\nin this vector space model. So in general,\nwe can represent a pseudo document or context of cat as one vector,\nd1, and another word, dog, might give us a different context,\nso d2. And then we can measure\nthe similarity of these two vectors. So by viewing context in\nthe vector space model, we convert the problem of\nparadigmatical relation discovery into the problem of computing\nthe vectors and their similarity. So the two questions that we\nhave to address are first, how to compute each vector, and\nthat is how to compute xi or yi. And the other question is how\ndo you compute the similarity. Now in general, there are many approaches\nthat can be used to solve the problem, and most of them are developed for\ninformation retrieval. And they have been shown to work well for matching a query vector and\na document vector. But we can adapt many of\nthe ideas to compute a similarity of context documents for our purpose here. So let's first look at\nthe one plausible approach, where we try to match\nthe similarity of context based on the expected overlap of words,\nand we call this EOWC. So the idea here is to represent\na context by a word vector where each word has a weight\nthat's equal to the probability that a randomly picked word from\nthis document vector, is this word. So in other words,\nxi is defined as the normalized account of word wi in the context, and this can be interpreted as\nthe probability that you would actually pick this word from d1\nif you randomly picked a word. Now, of course these xi's would sum to one\nbecause they are normalized frequencies, and this means the vector is actually probability of\nthe distribution over words. So, the vector d2 can be also\ncomputed in the same way, and this would give us then two probability\ndistributions representing two contexts. So, that addresses the problem\nhow to compute the vectors, and next let's see how we can define\nsimilarity in this approach. Well, here, we simply define\nthe similarity as a dot product of two vectors, and\nthis is defined as a sum of the products of the corresponding\nelements of the two vectors. Now, it's interesting to see\nthat this similarity function actually has a nice interpretation,\nand that is this. Dot product, in fact that gives\nus the probability that two randomly picked words from\nthe two contexts are identical. That means if we try to pick a word\nfrom one context and try to pick another word from another context, we can then\nask the question, are they identical? If the two contexts are very similar,\nthen we should expect we frequently will see the two words picked from\nthe two contexts are identical. If they are very different,\nthen the chance of seeing identical words being picked from\nthe two contexts would be small. So this intuitively makes sense, right,\nfor measuring similarity of contexts. Now you might want to also take\na look at the exact formulas and see why this can be interpreted\nas the probability that two randomly picked words are identical. So if you just stare at the formula\nto check what's inside this sum, then you will see basically in each\ncase it gives us the probability that we will see an overlap on\na particular word, wi. And where xi gives us a probability that\nwe will pick this particular word from d1, and yi gives us the probability\nof picking this word from d2. And when we pick the same\nword from the two contexts, then we have an identical pick, right so. That's one possible approach, EOWC,\nextracted overlap of words in context. Now as always, we would like to assess\nwhether this approach it would work well. Now of course, ultimately we have to\ntest the approach with real data and see if it gives us really\nsemantically related words. Really give us paradigmatical relations,\nbut analytically we can also analyze\nthis formula a little bit. So first, as I said,\nit does make sense, right, because this formula will give a higher score if there\nis more overlap between the two contexts. So that's exactly what we want. But if you analyze\nthe formula more carefully, then you also see there might\nbe some potential problems, and specifically there\nare two potential problems. First, it might favor matching\none frequent term very well, over matching more distinct terms. And that is because in the dot product,\nif one element has a high value and this element is shared by both contexts and\nit contributes a lot to the overall sum, it might indeed make the score\nhigher than in another case, where the two vectors actually have\na lot of overlap in different terms. But each term has a relatively low\nfrequency, so this may not be desirable. Of course, this might be\ndesirable in some other cases. But in our case, we should intuitively\nprefer a case where we match more different terms in the context,\nso that we have more confidence in saying that the two words\nindeed occur in similar context. If you only rely on one term and that's a little bit questionable,\nit may not be robust. Now the second problem is that it\ntreats every word equally, right. So if you match a word like the and it will be the same as\nmatching a word like eats, but intuitively we know\nmatching the isn't really surprising because the occurs everywhere. So matching the is not as such\nstrong evidence as matching what a word like eats,\nwhich doesn't occur frequently. So this is another\nproblem of this approach. In the next chapter we are going to talk\nabout how to address these problems. [MUSIC]",
 "09_1-9-paradigmatic-relation-discovery-part-2.en.txt": "In this lecture, we continue discussing Paradigmatical\nRelation Discovery. Earlier we introduced\na method called Expected Overlap of\nWords in Context. In this method, we\nrepresent each context by a word vector that represents the probability of a\nword in the context. We measure the similarity\nby using the.product, which can be interpreted as\nthe probability that two randomly picked words from the two contexts are identical. We also discussed\nthe two problems of this method. The first is that\nit favors matching one frequent term very well over matching more distinct terms. It put too much emphasis on\nmatching one term very well. The second is that it\ntreats every word equally. Even a common word like\nthe will contribute equally as content\nword like eats. So now we are going to talk about how\nto solve these problems. More specifically, we're\ngoing to introduce some retrieval heuristics\nused in text retrieval. These heuristics can effectively\nsolve these problems, as these problems also\noccur in text retrieval when we match a query that\nthough with a document vector. So to address the first problem, we can use a sublinear\ntransformation of tone frequency. That is, we don't have to use the raw frequency count of a term to represent the context. We can transform\nit into some form that wouldn't emphasize so\nmuch on the raw frequency. To address the\nsynchronous problem, we can put more weight\non rare terms. That is we can reward\nmatching a real-world. This heuristic is called the IDF term weighting in text retrieval. IDF stands for\nInverse Document Frequency. So now, we're going to talk about the two heuristics\nin more detail. First let's talk about\nthe TF Transformation. That is to convert\nthe raw count of a word in the document\ninto some weight that reflects our belief about how important\nthis word in the document. So that will be\ndenoted by TF of w,d. That's shown in the y-axis. Now, in general, there are\nmany ways to map that. Let's first look at\nthe simple way of mapping. In this case, we're\ngoing to say, well, any non-zero counts\nwill be mapped to one and the zero count\nwill be mapped to zero. So with this mapping all the frequencies will be mapped to only two\nvalues; zero or one. The mapping function is shown\nhere as a flat line here. Now, this is naive because it's not\nthe frequency of words. However, this actually\nhas the advantage of emphasizing matching all\nthe words in the context. So it does not allow a frequency of word to\ndominate the matching. Now, the approach\nthat we have taken earlier in the expected\noverlap count approach, is a linear transformation. We basically, take\ny as the same as x. So we use the raw count\nas a representation. That created the problem that we just talked about namely; it emphasize too much on just\nmatching one frequent term. Matching one frequent term\ncan contribute a lot. So we can have a lot of other interesting\ntransformations in between the two extremes, and they generally form\na sublinear transformation. So for example,\none possibility is to take logarithm of the raw count, and this will give us curve\nthat looks like this, that you are seeing here. In this case, you can see\nthe high frequency counts. The high counts are\npenalize a little bit, so the curve is a sublinear\ncurve and it brings down the weight of\nthose really high counts. This is what we want,\nbecause it prevents that terms from dominating\nthe scoring function. Now, there is also another interesting\ntransformation called a BM25 transformation which has been shown to be very\neffective for retrieval. In this transformation, we have a form that looks like this. So it's k plus one multiplied\nby x divided by x plus k, where k is a parameter, x is the count, the raw count of a word. Now, the transformation\nis very interesting in that it can actually go from one extreme to the other\nextreme by varying k. It also interesting\nthat it has upper bound, k plus one in this case. So this puts\na very strict constraint on high frequency terms, because their weight would\nnever exceed k plus one. As we vary k, if we can\nsimulate the two extremes. So when k is set to zero, we roughly have the 0,1 vector. Whereas when we set k\nto a very large value, it will behave more like\nthe linear transformation. So this transformation\nfunction is by far the most effective\ntransformation function for text retrieval and it also makes sense for our problem setup. So we just talked about how\nto solve the problem of overemphasizing a frequency term Now let's look at\nthe second problem, and that is how we can\npenalize popular terms. Matching \"the\" is not surprising, because \"the\" occurs everywhere. But matching \"eats\"\nwould count a lot. So how can we address\nthat problem? Now in this case, we can\nuse the IDF weighting. That's commonly\nused in retrieval. IDF stands for\nInverse Document Frequency. Document frequency\nmeans the count of the total number of documents that contain\na particular word. So here we show that the IDF\nmeasure is defined as a logarithm function\nof the number of documents that match a\nterm or document frequency. So K is the number of\ndocuments containing word or document frequency and M here is the total number of\ndocuments in the collection. The IDF function is giving\na higher value for a lower K, meaning that it\nrewards rare term. The maximum value is\nlog of M plus one. That's when the word occurred\njust once in a context. So that's a very rare term, the rare is term in\nthe whole collection. The lowest value you can\nsee here is when K reaches its maximum which would be M. So that would be\na very low value, close to zero in fact. So this of course measure is used in search where we\nnaturally have a collection. In our case, what would\nbe our collection? Well, we can also\nuse the context that we can collect all the words\nas our collection. That is to say, a word that's popular in\nthe collection in general, would also have a low IDF. Because depending on the dataset, we can construct the context\nvectors in different ways. But in the end if a term is very frequent in\nthe original dataset, then it will still be frequent in the collective\ncontext documents. So how can we add\nthese heuristics to improve our similarity function? Well, here's one way\nand there are many other ways\nthat are possible. But this is a reasonable way, where we can adapt\nthe BM25 retrieval model for paradigmatical\nrelation mining. In this case, we define the\ndocument vector as containing elements representing\nnormalized BM25 values. So in this\nnormalization function, we take sum over all\nthe words and we normalize the weight of\neach word by the sum of the weights of all the words. This is to again ensure all the xi's will sum to\none in this vector. So this would be very similar\nto what we had before, in that this vector is actually something similar\nto a word distribution, all the xi's will sum to one. Now, the weight of BM25 for\neach word is defined here. If you compare this with\nour old definition where we just have a normalized count\nof this one, right? So we only have this one\nand the document lens or the total counts of words in\nthat context to document, and that's what we had before. But now with the BM25\ntransformation, we introduced something else. First, of course,\nthis extra occurrence of this count is just to achieve the sub-linear\nnormalization. But we also see we introduced\nthe parameter, k, here, and this parameter is\ngenerally a non-active number, although zero is also possible. But this controls\nthe upper bound, and also controls to what extent it simulates the\nlinear transformation. So this is one parameter, but we also see there is\nanother parameter here, b, and this would be\nwithin zero and one. This is a parameter to\ncontrol lens normalization. In this case, the normalization formula has a average document lens here. This is computed up\nby taking the average of the lenses of all the\ndocuments in the collection. In this case, all the lenses of all the context of documents\nthat we're considering. So this average documents will be a constant for\nany given collection. So it actually is only affecting the effect\nof the parameter, b, here because\nthis is a constant. But I kept it here because\nit's a constant that's used for in retrieval where it would give us a stabilized\ninterpretation of parameter, b. But for our purpose, this will be a constant so\nit would only be affecting the lens normalization\ntogether with parameter, b. Now, with this definition then, we have a new way to define\nour document of vectors, and we can compute\nthe vector d2 in the same way. The difference is that the high-frequency terms will now have a somewhat lower weights. This would help us control the inference of\nthese high-frequency terms. Now, the idea can be added\nhere in the scoring function. That means we'll\nintroduce a weight for matching each term. So you may recall\nthis sum indicates all the possible words\nthat can be overlap between the two contexts. The x_i and the y_i\nare probabilities of picking the word\nfrom both contexts. Therefore, it\nindicates how likely we'll see a match on this word. Now, IDF would give us the importance of\nmatching this word. A common word will be worth\nless than a rare word. So we emphasize more on\nmatching rare words now. So with this modification, then the new function will likely address\nthose two problems. Now, interestingly\nwe can also use this approach to discover\nsyntagmatic relations. In general, when we re-brand a context with a term vector, we would likely see some terms have high weights and other terms have low weights. Depending on how we assign\nweights to these terms, we might be able to\nuse these weights to discover the words that are strongly associated with the candidate word\nin the context. So let's take a look at the term vector in\nmore detail here. We have each x_i defined as the normalized\nweight of BM25. Now, this weight alone only reflects how frequent the word\noccurs in the context. But we can't just say any frequent term in the context that would\nbe correlated with the candidate word because many common words like 'the' will occur frequently in\nall the context. But if we apply IDF\nweighting as you see here, we can then re-weight\nthese terms based on IDF. That means the words that are common like 'the'\nwill get penalized. So now the highest\nweighted terms will not be those common terms because\nthey have lower IDFs. Instead, those terms would be the terms that are\nfrequent in the context, but not frequent\nin the collection. So those are clearly the words\nthat tend to occur in the context of the candidate\nword, for example, cat. So for this reason, the highly weighted terms in\nthis idea of weighted vector can also be assumed to be candidates for\nsyntagmatic relations. Now, of course, this is\nonly a by-product of our approach for discovering\nparadigmatic relations. In the next lecture, we're\ngoing to talk more about how to discover\nsyntagmatic relations. But it clearly shows the relation between discovering\nthe two relations. Indeed they can be discovered in a joint manner by leveraging\nsuch associations. So to summarize,\nthe main idea for discovering paradigmatic relations is to collect the context of a candidate word to\nform a pseudo document. This is typically represented\nas a bag of words. Then compute the similarity of the corresponding\ncontext documents of two candidate words. Then we can take\nthe highly similar word pairs, and treat them as having\nparadigmatic relations. These are the words that\nshare similar contexts. There are many different ways to implement this general idea. We just talked about\nsome of the approaches. More specifically, we\ntalked about using text retrieval models to help us design effective\nsimilarity function to compute the\nparadigmatic relations. More specifically, we have used the BM25 and IDF weighting to discover\nparadigmatic relation. These approaches also represent the state of the art in\ntext retrieval techniques. Finally, syntagmatic relations\ncan also be discovered as a by-product when we discover\nparadigmatic relations.",
 "01_2-1-syntagmatic-relation-discovery-entropy.en.txt": "[SOUND]. This lecture is about the syntagmatic\nrelation discovery, and entropy. In this lecture, we're going to continue\ntalking about word association mining. In particular, we're going to talk about\nhow to discover syntagmatic relations. And we're going to start with\nthe introduction of entropy, which is the basis for designing some\nmeasures for discovering such relations. By definition, syntagmatic relations hold between words\nthat have correlated co-occurrences. That means,\nwhen we see one word occurs in context, we tend to see the occurrence\nof the other word. So, take a more specific example, here. We can ask the question, whenever eats occurs,\nwhat other words also tend to occur? Looking at the sentences on the left,\nwe see some words that might occur together with eats, like cat,\ndog, or fish is right. But if I take them out and\nif you look at the right side where we only show eats and some other words,\nthe question then is. Can you predict what other words\noccur to the left or to the right? Right so\nthis would force us to think about what other words are associated with eats. If they are associated with eats,\nthey tend to occur in the context of eats. More specifically our\nprediction problem is to take any text segment which can be a sentence,\na paragraph, or a document. And then ask I the question,\nis a particular word present or absent in this segment? Right here we ask about the word W. Is W present or absent in this segment? Now what's interesting is that some words are actually easier\nto predict than other words. If you take a look at the three\nwords shown here, meat, the, and unicorn, which one do you\nthink is easier to predict? Now if you think about it for\na moment you might conclude that the is easier to predict because\nit tends to occur everywhere. So I can just say,\nwell that would be in the sentence. Unicorn is also relatively easy\nbecause unicorn is rare, is very rare. And I can bet that it doesn't\noccur in this sentence. But meat is somewhere in\nbetween in terms of frequency. And it makes it harder to predict because\nit's possible that it occurs in a sentence or the segment, more accurately. But it may also not occur in the sentence,\nso now let's study this\nproblem more formally. So the problem can be formally defined as predicting the value of\na binary random variable. Here we denote it by X sub w,\nw denotes a word, so this random variable is associated\nwith precisely one word. When the value of the variable is 1,\nit means this word is present. When it's 0, it means the word is absent. And naturally, the probabilities for\n1 and 0 should sum to 1, because a word is either present or\nabsent in a segment. There's no other choice. So the intuition with this concept earlier\ncan be formally stated as follows. The more random this random variable is,\nthe more difficult the prediction will be. Now the question is how does one\nquantitatively measure the randomness of a random variable like X sub w? How in general, can we quantify\nthe randomness of a variable and that's why we need a measure\ncalled entropy and this measure introduced in information\ntheory to measure the randomness of X. There is also some connection\nwith information here but that is beyond the scope of this course. So for\nour purpose we just treat entropy function as a function defined\non a random variable. In this case, it is a binary random\nvariable, although the definition can be easily generalized for\na random variable with multiple values. Now the function form looks like this, there's the sum of all the possible\nvalues for this random variable. Inside the sum for each value we\nhave a product of the probability that the random variable equals this\nvalue and log of this probability. And note that there is also\na negative sign there. Now entropy in general is non-negative. And that can be mathematically proved. So if we expand this sum, we'll see that\nthe equation looks like the second one. Where I explicitly plugged\nin the two values, 0 and 1. And sometimes when we have 0 log of 0, we would generally define that as 0,\nbecause log of 0 is undefined. So this is the entropy function. And this function will\ngive a different value for different distributions\nof this random variable. And it clearly depends on the probability that the random variable\ntaking value of 1 or 0. If we plot this function against the probability that the random\nvariable is equal to 1. And then the function looks like this. At the two ends,\nthat means when the probability of X equals 1 is very small or very large,\nthen the entropy function has a low value. When it's 0.5 in the middle\nthen it reaches the maximum. Now if we plot the function\nagainst the probability that X is taking a value of 0 and the function would show exactly the same curve here,\nand you can imagine why. And so that's because the two probabilities are symmetric,\nand completely symmetric. So an interesting question you\ncan think about in general is for what kind of X does entropy\nreach maximum or minimum. And we can in particular think\nabout some special cases. For example, in one case,\nwe might have a random variable that always takes a value of 1. The probability is 1. Or there's a random variable that is equally likely taking a value of one or\nzero. So in this case the probability\nthat X equals 1 is 0.5. Now which one has a higher entropy? It's easier to look at the problem\nby thinking of a simple example using coin tossing. So when we think about random\nexperiments like tossing a coin, it gives us a random variable,\nthat can represent the result. It can be head or tail. So we can define a random variable\nX sub coin, so that it's 1 when the coin shows up as head,\nit's 0 when the coin shows up as tail. So now we can compute the entropy\nof this random variable. And this entropy indicates how\ndifficult it is to predict the outcome of a coin toss. So we can think about the two cases. One is a fair coin, it's completely fair. The coin shows up as head or\ntail equally likely. So the two probabilities would be a half. Right?\nSo both are equal to one half. Another extreme case is\ncompletely biased coin, where the coin always shows up as heads. So it's a completely biased coin. Now let's think about\nthe entropies in the two cases. And if you plug in these values you can\nsee the entropies would be as follows. For a fair coin we see the entropy\nreaches its maximum, that's 1. For the completely biased coin,\nwe see it's 0. And that intuitively makes a lot of sense. Because a fair coin is\nmost difficult to predict. Whereas a completely biased\ncoin is very easy to predict. We can always say, well, it's a head. Because it is a head all the time. So they can be shown on\nthe curve as follows. So the fair coin corresponds to the middle\npoint where it's very uncertain. The completely biased coin\ncorresponds to the end point where we have a probability\nof 1.0 and the entropy is 0. So, now let's see how we can use\nentropy for word prediction. Let's think about our problem is\nto predict whether W is present or absent in this segment. Again, think about the three words,\nparticularly think about their entropies. Now we can assume high entropy\nwords are harder to predict. And so we now have a quantitative way to\ntell us which word is harder to predict. Now if you look at the three words meat,\nthe, unicorn, again, and we clearly would expect meat to have\na higher entropy than the unicorn. In fact if you look at the entropy of the,\nit's close to zero. Because it occurs everywhere. So it's like a completely biased coin. Therefore the entropy is zero. [MUSIC]",
 "02_2-2-syntagmatic-relation-discovery-conditional-entropy.en.txt": "[SOUND] This lecture is\nabout the syntagmatic relation discovery and\nconditional entropy. In this lecture, we're going to continue the discussion\nof word association mining and analysis. We're going to talk about the conditional\nentropy, which is useful for discovering syntagmatic relations. Earlier, we talked about\nusing entropy to capture how easy it is to predict the presence or\nabsence of a word. Now, we'll address\na different scenario where we assume that we know something\nabout the text segment. So now the question is, suppose we know\nthat eats occurred in the segment. How would that help us\npredict the presence or absence of water, like in meat? And in particular, we want to\nknow whether the presence of eats has helped us predict\nthe presence of meat. And if we frame this using entrophy, that would mean we are interested\nin knowing whether knowing the presence of eats could reduce\nuncertainty about the meats. Or, reduce the entrophy\nof the random variable corresponding to the presence or\nabsence of meat. We can also ask as a question,\nwhat if we know of the absents of eats? Would that also help us predict\nthe presence or absence of meat? These questions can be\naddressed by using another concept called a conditioning entropy. So to explain this concept, let's first\nlook at the scenario we had before, when we know nothing about the segment. So we have these probabilities indicating\nwhether a word like meat occurs, or it doesn't occur in the segment. And we have an entropy function that\nlooks like what you see on the slide. Now suppose we know eats is present, so now we know the value of another\nrandom variable that denotes eats. Now, that would change all\nthese probabilities to conditional probabilities. Where we look at the presence or\nabsence of meat, given that we know eats\noccurred in the context. So as a result, if we replace these probabilities\nwith their corresponding conditional probabilities in the entropy function,\nwe'll get the conditional entropy. So this equation now here would be the conditional entropy. Conditional on the presence of eats. So, you can see this is essentially\nthe same entropy function as you have seen before, except that all\nthe probabilities now have a condition. And this then tells us\nthe entropy of meat, after we have known eats\noccurring in the segment. And of course, we can also define\nthis conditional entropy for the scenario where we don't see eats. So if we know it did not occur in\nthe segment, then this entry condition of entropy would capture the instances\nof meat in that condition. So now,\nputting different scenarios together, we have the completed definition\nof conditional entropy as follows. Basically, we're going to consider both\nscenarios of the value of eats zero, one, and this gives us a probability\nthat eats is equal to zero or one. Basically, whether eats is present or\nabsent. And this of course, is the conditional entropy of\nmeat in that particular scenario. So if you expanded this entropy, then you have the following equation. Where you see the involvement of\nthose conditional probabilities. Now in general, for any discrete\nrandom variables x and y, we have the conditional entropy is no larger\nthan the entropy of the variable x. So basically, this is upper bound for\nthe conditional entropy. That means by knowing more\ninformation about the segment, we want to be able to\nincrease uncertainty. We can only reduce uncertainty. And that intuitively makes sense\nbecause as we know more information, it should always help\nus make the prediction. And cannot hurt\nthe prediction in any case. Now, what's interesting here is also to\nthink about what's the minimum possible value of this conditional entropy? Now, we know that the maximum\nvalue is the entropy of X. But what about the minimum,\nso what do you think? I hope you can reach the conclusion that\nthe minimum possible value, would be zero. And it will be interesting to think about\nunder what situation will achieve this. So, let's see how we can use conditional\nentropy to capture syntagmatic relation. Now of course,\nthis conditional entropy gives us directly one way to measure\nthe association of two words. Because it tells us to what extent,\nwe can predict the one word given that we know the presence or\nabsence of another word. Now before we look at the intuition\nof conditional entropy in capturing syntagmatic relations, it's useful to\nthink of a very special case, listed here. That is, the conditional entropy\nof the word given itself. So here, we listed this conditional\nentropy in the middle. So, it's here. So, what is the value of this? Now, this means we know where\nthe meat occurs in the sentence. And we hope to predict whether\nthe meat occurs in the sentence. And of course, this is 0 because\nthere's no incident anymore. Once we know whether the word\noccurs in the segment, we'll already know the answer\nof the prediction. So this is zero. And that's also when this conditional\nentropy reaches the minimum. So now, let's look at some other cases. So this is a case of knowing the and\ntrying to predict the meat. And this is a case of knowing eats and\ntrying to predict the meat. Which one do you think is smaller? No doubt smaller entropy means easier for\nprediction. Which one do you think is higher? Which one is not smaller? Well, if you at the uncertainty,\nthen in the first case, the doesn't really tell\nus much about the meat. So knowing the occurrence of the doesn't\nreally help us reduce entropy that much. So it stays fairly close to\nthe original entropy of meat. Whereas in the case of eats,\neats is related to meat. So knowing presence of eats or\nabsence of eats, would help us predict whether meat occurs. So it can help us reduce entropy of meat. So we should expect the sigma term, namely\nthis one, to have a smaller entropy. And that means there is a stronger\nassociation between meat and eats. So we now also know when\nthis w is the same as this meat, then the conditional entropy\nwould reach its minimum, which is 0. And for what kind of words\nwould either reach its maximum? Well, that's when this stuff\nis not really related to meat. And like the for example,\nit would be very close to the maximum, which is the entropy of meat itself. So this suggests that when you\nuse conditional entropy for mining syntagmatic relations,\nthe hours would look as follows. For each word W1, we're going to\nenumerate the overall other words W2. And then, we can compute\nthe conditional entropy of W1 given W2. We thought all the candidate was in\nascending order of the conditional entropy because we're out of favor,\na world that has a small entropy. Meaning that it helps us predict\nthe time of the word W1. And then, we're going to take the top ring\nof the candidate words as words that have potential syntagmatic relations with W1. Note that we need to use\na threshold to find these words. The stresser can be the number\nof top candidates take, or absolute value for\nthe conditional entropy. Now, this would allow us to mine the most strongly correlated words with\na particular word, W1 here. But, this algorithm does not\nhelp us mine the strongest that K syntagmatical relations\nfrom an entire collection. Because in order to do that, we have to\nensure that these conditional entropies are comparable across different words. In this case of discovering\nthe mathematical relations for a targeted word like W1, we only need\nto compare the conditional entropies for W1, given different words. And in this case, they are comparable. All right. So, the conditional entropy of W1, given\nW2, and the conditional entropy of W1, given W3 are comparable. They all measure how hard\nit is to predict the W1. But, if we think about the two pairs, where we share W2 in the same condition,\nand we try to predict the W1 and W3. Then, the conditional entropies\nare actually not comparable. You can think of about this question. Why? So why are they not comfortable? Well, that was because they\nhave a different outer bounds. Right?\nSo those outer bounds are precisely the entropy of W1 and the entropy of W3. And they have different upper bounds. So we cannot really\ncompare them in this way. So how do we address this problem? Well later, we'll discuss, we can use\nmutual information to solve this problem. [MUSIC]",
 "03_2-3-syntagmatic-relation-discovery-mutual-information-part-1.en.txt": "[SOUND]. This lecture is about the syntagmatic\nrelation discovery and mutual information. In this lecture we are going to continue\ndiscussing syntagmatic relation discovery. In particular,\nwe are going to talk about another the concept in the information series,\nwe called it mutual information and how it can be used to discover\nsyntagmatic relations. Before we talked about the problem\nof conditional entropy and that is the conditional entropy\ncomputed different pairs of words. It is not really comparable, so\nthat makes it harder with this cover, strong synagmatic relations\nglobally from corpus. So now we are going to introduce mutual\ninformation, which is another concept in the information series\nthat allows us to, sometimes, normalize the conditional entropy to make\nit more comparable across different pairs. In particular, mutual information\nin order to find I(X:Y), matches the entropy reduction\nof X obtained from knowing Y. More specifically the question we\nare interested in here is how much of an entropy of X can\nwe obtain by knowing Y. So mathematically it can be\ndefined as the difference between the original entropy of X, and\nthe condition of Y of X given Y. And you might see,\nas you can see here it can also be defined as reduction of entropy of\nY because of knowing X. Now normally the two conditional\ninterface H of X given Y and the entropy of Y given X are not equal,\nbut interestingly, the reduction of entropy by knowing\none of them, is actually equal. So, this quantity is called a Mutual\nInformation in order to buy I here. And this function has some interesting\nproperties, first it is also non-negative. This is easy to understand because\nthe original entropy is always not going to be lower than the possibility\nreduced conditional entropy. In other words, the conditional entropy\nwill never exceed the original entropy. Knowing some information can\nalways help us potentially, but will not hurt us in predicting x. The signal property is that it\nis symmetric like additional entropy is not symmetrical,\nmutual information is, and the third property is that It\nreaches its minimum, zero, if and only if the two random variables\nare completely independent. That means knowing one of them does not\ntell us anything about the other and this last property can be verified by\nsimply looking at the equation above and it reaches 0 if and\nonly the conditional entropy of X [INAUDIBLE] Y is exactly the same\nas original entropy of X. So that means knowing why it did not\nhelp at all and that is when X and a Y are completely independent. Now when we fix X to rank different\nYs using conditional entropy would give the same order as\nranking based on mutual information because in the function here,\nH(X) is fixed because X is fixed. So ranking based on mutual entropy is\nexactly the same as ranking based on the conditional entropy of X given Y, but the mutual information allows us to\ncompare different pairs of x and y. So, that is why mutual information is\nmore general and in general, more useful. So, let us examine the intuition\nof using mutual information for Syntagmatical Relation Mining. Now, the question we ask forcing\nthat relation mining is, whenever \"eats\" occurs,\nwhat other words also tend to occur? So this question can be framed as\na mutual information question, that is, which words have high mutual\ninformation was eats, so computer the missing information\nbetween eats and other words. And if we do that, and it is basically\na base on the same as conditional we will see that words that\nare strongly associated with eats, will have a high point. Whereas words that are not related\nwill have lower mutual information. For this, I will give some example here. The mutual information between \"eats\" and\n\"meats\", which is the same as between \"meats\" and\n\"eats,\" because the information is symmetrical is expected to be higher than\nthe mutual information between eats and the, because knowing the does not\nreally help us as a predictor. It is similar, and\nknowing eats does not help us predicting, the as well. And you also can easily\nsee that the mutual information between a word and\nitself is the largest, which is equal to\nthe entropy of this word and so, because in this case the reduction is maximum because knowing one allows\nus to predict the other completely. So the conditional entropy is zero, therefore the mutual information\nreaches its maximum. It is going to be larger, then are equal\nto the machine volume eats in other words. In other words picking any other word and the computer picking between eats and\nthat word. You will not get any information larger\nthe computation from eats and itself. So now let us look at how to\ncompute the mute information. Now in order to do that, we often use a different form of mutual\ninformation, and we can mathematically rewrite the mutual information\ninto the form shown on this slide. Where we essentially see\na formula that computes what is called a KL-divergence or divergence. This is another term\nin information theory. It measures the divergence\nbetween two distributions. Now, if you look at the formula,\nit is also sum over many combinations of different values of the two random\nvariables but inside the sum, mainly we are doing a comparison\nbetween two joint distributions. The numerator has the joint, actual observed the joint distribution\nof the two random variables. The bottom part or the denominator can be interpreted as the expected joint\ndistribution of the two random variables, if they were independent because when\ntwo random variables are independent, they are joined distribution is equal to\nthe product of the two probabilities. So this comparison will tell us whether\nthe two variables are indeed independent. If they are indeed independent then we\nwould expect that the two are the same, but if the numerator is different\nfrom the denominator, that would mean the two variables are not independent and\nthat helps measure the association. The sum is simply to take into\nconsideration of all of the combinations of the values of these\ntwo random variables. In our case, each random variable\ncan choose one of the two values, zero or one, so\nwe have four combinations here. If we look at this form of mutual\ninformation, it shows that the mutual information matches the divergence\nof the actual joint distribution from the expected distribution\nunder the independence assumption. The larger this divergence is, the higher\nthe mutual information would be. So now let us further look at what\nare exactly the probabilities, involved in this formula\nof mutual information. And here, this is all the probabilities\ninvolve, and it is easy for you to verify that. Basically, we have first to\n[INAUDIBLE] probabilities corresponding to the presence or\nabsence of each word. So, for w1,\nwe have two probabilities shown here. They should sum to one, because a word\ncan either be present or absent. In the segment, and similarly for the second word, we also have two\nprobabilities representing presence or absences of this word, and\nthere is some to y as well. And finally, we have a lot of\njoined probabilities that represent the scenarios of co-occurrences of\nthe two words, and they are shown here. And they sum to one because the two\nwords can only have these four possible scenarios. Either they both occur, so in that case both variables will have\na value of one, or one of them occurs. There are two scenarios. In these two cases one of the random\nvariables will be equal to one and the other will be zero and finally we have\nthe scenario when none of them occurs. This is when the two variables\ntaking a value of zero. So these are the probabilities involved\nin the calculation of mutual information, over here. Once we know how to calculate\nthese probabilities, we can easily calculate\nthe mutual information. It is also interesting to know that\nthere are actually some relations or constraint among these probabilities,\nand we already saw two of them, right? So in the previous slide, that you have seen that\nthe marginal probabilities of these words sum to one and\nwe also have seen this constraint, that says the two words have these\nfour scenarios of co-occurrency, but we also have some additional\nconstraints listed in the bottom. For example, this one means if we add up the probabilities that we observe\nthe two words occur together and the probabilities when the first word\noccurs and the second word does not occur. We get exactly the probability\nthat the first word is observed. In other words, when the word is observed. When the first word is observed, and there are only two scenarios, depending on\nwhether the second word is also observed. So, this probability captures the first\nscenario when the second word actually is also observed, and this captures the second scenario\nwhen the second word is not observed. So, we only see the first word, and it is easy to see the other equations\nalso follow the same reasoning. Now these equations allow us to\ncompute some probabilities based on other probabilities, and\nthis can simplify the computation. So more specifically,\nif we know the probability that a word is present, like in this case,\nso if we know this, and if we know the probability of\nthe presence of the second word, then we can easily compute\nthe absence probability, right? It is very easy to use this\nequation to do that, and so we take care of the computation of\nthese probabilities of presence and absence of each word. Now let's look at\nthe [INAUDIBLE] distribution. Let us assume that we also have available the probability that\nthey occurred together. Now it is easy to see that we can\nactually compute all the rest of these probabilities based on these. Specifically for\nexample using this equation we can compute the probability that the first word\noccurred and the second word did not, because we know these probabilities in\nthe boxes, and similarly using this equation we can compute the probability\nthat we observe only the second word. Word. And then finally,\nthis probability can be calculated by using this equation because\nnow this is known, and this is also known, and\nthis is already known, right. So this can be easier to calculate. So now this can be calculated. So this slide shows that we only\nneed to know how to compute these three probabilities\nthat are shown in the boxes, naming the presence of each word and the\nco-occurence of both words, in a segment. [MUSIC]",
 "04_2-4-syntagmatic-relation-discovery-mutual-information-part-2.en.txt": "[SOUND] In general, we can use the empirical count of events in the observed data\nto estimate the probabilities. And a commonly used technique is\ncalled a maximum likelihood estimate, where we simply normalize\nthe observe accounts. So if we do that, we can see, we can\ncompute these probabilities as follows. For estimating the probability that\nwe see a water current in a segment, we simply normalize the count of\nsegments that contain this word. So let's first take\na look at the data here. On the right side, you see a list of some,\nhypothesizes the data. These are segments. And in some segments you see both words\noccur, they are indicated as ones for both columns. In some other cases only one will occur,\nso only that column has one and the other column has zero. And in all, of course, in some other\ncases none of the words occur, so they are both zeros. And for estimating these probabilities, we\nsimply need to collect the three counts. So the three counts are first,\nthe count of W1. And that's the total number of\nsegments that contain word W1. It's just as the ones in the column of W1. We can count how many\nones we have seen there. The segment count is for word 2, and we\njust count the ones in the second column. And these will give us the total\nnumber of segments that contain W2. The third count is when both words occur. So this time, we're going to count\nthe sentence where both columns have ones. And then, so this would give us\nthe total number of segments where we have seen both W1 and W2. Once we have these counts,\nwe can just normalize these counts by N, which is the total number of segments, and this will give us the probabilities that\nwe need to compute original information. Now, there is a small problem,\nwhen we have zero counts sometimes. And in this case, we don't want a zero\nprobability because our data may be a small sample and in general, we would\nbelieve that it's potentially possible for a [INAUDIBLE] to avoid any context. So, to address this problem,\nwe can use a technique called smoothing. And that's basically to add some\nsmall constant to these counts, and so that we don't get\nthe zero probability in any case. Now, the best way to understand smoothing\nis imagine that we actually observed more data than we actually have, because we'll\npretend we observed some pseudo-segments. I illustrated on the top,\non the right side on the slide. And these pseudo-segments would\ncontribute additional counts of these words so\nthat no event will have zero probability. Now, in particular we introduce\nthe four pseudo-segments. Each is weighted at one quarter. And these represent the four different\ncombinations of occurrences of this word. So now each event,\neach combination will have at least one count or at least a non-zero\ncount from this pseudo-segment. So, in the actual segments\nthat we'll observe, it's okay if we haven't observed\nall of the combinations. So more specifically, you can see\nthe 0.5 here after it comes from the two ones in the two pseudo-segments,\nbecause each is weighted at one quarter. We add them up, we get 0.5. And similar to this,\n0.05 comes from one single pseudo-segment that indicates\nthe two words occur together. And of course in the denominator we add\nthe total number of pseudo-segments that we add, in this case,\nwe added a four pseudo-segments. Each is weighed at one quarter so\nthe total of the sum is, after the one. So, that's why in the denominator\nyou'll see a one there. So, this basically concludes\nthe discussion of how to compute a these four syntagmatic relation discoveries. Now, so to summarize,\nsyntagmatic relation can generally be discovered by measuring correlations\nbetween occurrences of two words. We've introduced the three\nconcepts from information theory. Entropy, which measures the uncertainty\nof a random variable X. Conditional entropy, which measures\nthe entropy of X given we know Y. And mutual information of X and Y,\nwhich matches the entropy reduction of X due to knowing Y, or\nentropy reduction of Y due to knowing X. They are the same. So these three concepts are actually very\nuseful for other applications as well. That's why we spent some time\nto explain this in detail. But in particular,\nthey are also very useful for discovering syntagmatic relations. In particular,\nmutual information is a principal way for discovering such a relation. It allows us to have values\ncomputed on different pairs of words that are comparable and\nso we can rank these pairs and discover the strongest syntagmatic\nfrom a collection of documents. Now, note that there is some relation\nbetween syntagmatic relation discovery and [INAUDIBLE] relation discovery. So we already discussed the possibility\nof using BM25 to achieve waiting for terms in the context to potentially\nalso suggest the candidates that have syntagmatic relations\nwith the candidate word. But here, once we use mutual information\nto discover syntagmatic relations, we can also represent the context with\nthis mutual information as weights. So this would give us\nanother way to represent the context of a word, like a cat. And if we do the same for all the words,\nthen we can cluster these words or compare the similarity between these\nwords based on their context similarity. So this provides yet\nanother way to do term weighting for paradigmatic relation discovery. And so to summarize this whole part\nabout word association mining. We introduce two basic associations,\ncalled a paradigmatic and a syntagmatic relations. These are fairly general, they apply\nto any items in any language, so the units don't have to be words,\nthey can be phrases or entities. We introduced multiple statistical\napproaches for discovering them, mainly showing that pure\nstatistical approaches are visible, are variable for\ndiscovering both kind of relations. And they can be combined to\nperform joint analysis, as well. These approaches can be applied\nto any text with no human effort, mostly because they are based\non counting of words, yet they can actually discover\ninteresting relations of words. We can also use different ways with\ndefining context and segment, and this would lead us to some interesting\nvariations of applications. For example, the context can be very\nnarrow like a few words, around a word, or a sentence, or maybe paragraphs,\nas using differing contexts would allows to discover different flavors\nof paradigmatical relations. And similarly,\ncounting co-occurrences using let's say, visual information to discover\nsyntagmatical relations. We also have to define the segment, and\nthe segment can be defined as a narrow text window or a longer text article. And this would give us different\nkinds of associations. These discovery associations can\nsupport many other applications, in both information retrieval and\ntext and data mining. So here are some recommended readings,\nif you want to know more about the topic. The first is a book with\na chapter on collocations, which is quite relevant to\nthe topic of these lectures. The second is an article\nabout using various statistical measures to\ndiscover lexical atoms. Those are phrases that\nare non-compositional. For example,\nhot dog is not really a dog that's hot, blue chip is not a chip that's blue. And the paper has a discussion about some\ntechniques for discovering such phrases. The third one is a new paper on a unified\nway to discover both paradigmatical relations and a syntagmatical relations,\nusing random works on word graphs. [SOUND]",
 "05_2-5-topic-mining-and-analysis-motivation-and-task-definition.en.txt": "[SOUND]\n>> This lecture is about topic mining and\nanalysis. We're going to talk about its\nmotivation and task definition. In this lecture we're going to talk\nabout different kind of mining task. As you see on this road map,\nwe have just covered mining knowledge about language,\nnamely discovery of word associations such as paradigmatic and\nrelations and syntagmatic relations. Now, starting from this lecture, we're\ngoing to talk about mining another kind of knowledge, which is content mining, and trying to discover knowledge about\nthe main topics in the text. And we call that topic mining and\nanalysis. In this lecture, we're going to talk about\nits motivation and the task definition. So first of all,\nlet's look at the concept of topic. So topic is something that we\nall understand, I think, but it's actually not that\neasy to formally define. Roughly speaking, topic is the main\nidea discussed in text data. And you can think of this as a theme or\nsubject of a discussion or conversation. It can also have different granularities. For example,\nwe can talk about the topic of a sentence. A topic of article,\naa topic of paragraph or the topic of all the research articles\nin the research library, right, so different grand narratives of topics\nobviously have different applications. Indeed, there are many applications that\nrequire discovery of topics in text, and they're analyzed then. Here are some examples. For example, we might be interested\nin knowing about what are Twitter users are talking about today? Are they talking about NBA sports, or are they talking about some\ninternational events, etc.? Or we are interested in\nknowing about research topics. For example, one might be interested in\nknowing what are the current research topics in data mining, and how are they\ndifferent from those five years ago? Now this involves discovery of topics\nin data mining literatures and also we want to discover topics in\ntoday's literature and those in the past. And then we can make a comparison. We might also be also interested in\nknowing what do people like about some products like the iPhone 6,\nand what do they dislike? And this involves discovering\ntopics in positive opinions about iPhone 6 and\nalso negative reviews about it. Or perhaps we're interested in knowing\nwhat are the major topics debated in 2012 presidential election? And all these have to do with discovering\ntopics in text and analyzing them, and we're going to talk about a lot\nof techniques for doing this. In general we can view a topic as\nsome knowledge about the world. So from text data we expect to\ndiscover a number of topics, and then these topics generally provide\na description about the world. And it tells us something about the world. About a product, about a person etc. Now when we have some non-text data, then we can have more context for\nanalyzing the topics. For example, we might know the time\nassociated with the text data, or locations where the text\ndata were produced, or the authors of the text, or\nthe sources of the text, etc. All such meta data, or context variables can be associated\nwith the topics that we discover, and then we can use these context variables\nhelp us analyze patterns of topics. For example, looking at topics over time,\nwe would be able to discover whether there's a trending topic, or\nsome topics might be fading away. Soon you are looking at topics\nin different locations. We might know some insights about\npeople's opinions in different locations. So that's why mining\ntopics is very important. Now, let's look at the tasks\nof topic mining and analysis. In general, it would involve first\ndiscovering a lot of topics, in this case, k topics. And then we also would like to know, which\ntopics are covered in which documents, to what extent. So for example, in document one, we\nmight see that Topic 1 is covered a lot, Topic 2 and\nTopic k are covered with a small portion. And other topics,\nperhaps, are not covered. Document two, on the other hand,\ncovered Topic 2 very well, but it did not cover Topic 1 at all, and it also covers Topic k to some extent,\netc., right? So now you can see there\nare generally two different tasks, or sub-tasks, the first is to discover k\ntopics from a collection of text laid out. What are these k topics? Okay, major topics in the text they are. The second task is to figure out\nwhich documents cover which topics to what extent. So more formally,\nwe can define the problem as follows. First, we have, as input,\na collection of N text documents. Here we can denote the text\ncollection as C, and denote text article as d i. And, we generally also need to have\nas input the number of topics, k. But there may be techniques that can\nautomatically suggest a number of topics. But in the techniques that we will\ndiscuss, which are also the most useful techniques, we often need to\nspecify a number of topics. Now the output would then be the k\ntopics that we would like to discover, in order as theta sub\none through theta sub k. Also we want to generate the coverage of\ntopics in each document of d sub i And this is denoted by pi sub i j. And pi sub ij is the probability\nof document d sub i covering topic theta sub j. So obviously for each document, we have\na set of such values to indicate to what extent the document covers,\neach topic. And we can assume that these\nprobabilities sum to one. Because a document won't be able to cover other topics outside of the topics\nthat we discussed, that we discovered. So now, the question is, how do we define\ntheta sub i, how do we define the topic? Now this problem has not\nbeen completely defined until we define what is exactly theta. So in the next few lectures, we're going to talk about\ndifferent ways to define theta. [MUSIC]",
 "06_2-6-topic-mining-and-analysis-term-as-topic.en.txt": "[MUSIC] This lecture is about topic mining and\nanalysis. We're going to talk about\nusing a term as topic. This is a slide that you have\nseen in a earlier lecture where we define the task of\ntopic mining and analysis. We also raised the question, how do\nwe exactly define the topic of theta? So in this lecture, we're going to\noffer one way to define it, and that's our initial idea. Our idea here is defining\na topic simply as a term. A term can be a word or a phrase. And in general,\nwe can use these terms to describe topics. So our first thought is just\nto define a topic as one term. For example, we might have terms\nlike sports, travel, or science, as you see here. Now if we define a topic in this way, we can then analyze the coverage\nof such topics in each document. Here for example, we might want to discover to what\nextent document one covers sports. And we found that 30% of the content\nof document one is about sports. And 12% is about the travel, etc. We might also discover document\ntwo does not cover sports at all. So the coverage is zero, etc. So now, of course,\nas we discussed in the task definition for topic mining and analysis,\nwe have two tasks. One is to discover the topics. And the second is to analyze coverage. So let's first think\nabout how we can discover topics if we represent\neach topic by a term. So that means we need to mine k\ntopical terms from a collection. Now there are, of course,\nmany different ways of doing that. And we're going to talk about\na natural way of doing that, which is also likely effective. So first of all, we're going to parse the text data in\nthe collection to obtain candidate terms. Here candidate terms can be words or\nphrases. Let's say the simplest solution is\nto just take each word as a term. These words then become candidate topics. Then we're going to design a scoring\nfunction to match how good each term is as a topic. So how can we design such a function? Well there are many things\nthat we can consider. For example, we can use pure statistics\nto design such a scoring function. Intuitively, we would like to\nfavor representative terms, meaning terms that can represent\na lot of content in the collection. So that would mean we want\nto favor a frequent term. However, if we simply use the frequency\nto design the scoring function, then the highest scored terms\nwould be general terms or functional terms like the, etc. Those terms occur very frequently English. So we also want to avoid having\nsuch words on the top so we want to penalize such words. But in general, we would like to favor\nterms that are fairly frequent but not so frequent. So a particular approach could be based\non TF-IDF weighting from retrieval. And TF stands for term frequency. IDF stands for inverse document frequency. We talked about some of these ideas in the lectures about\nthe discovery of word associations. So these are statistical methods, meaning that the function is\ndefined mostly based on statistics. So the scoring function\nwould be very general. It can be applied to any language,\nany text. But when we apply such a approach\nto a particular problem, we might also be able to leverage\nsome domain-specific heuristics. For example, in news we might favor\ntitle words actually general. We might want to favor title\nwords because the authors tend to use the title to describe\nthe topic of an article. If we're dealing with tweets,\nwe could also favor hashtags, which are invented to denote topics. So naturally, hashtags can be good\ncandidates for representing topics. Anyway, after we have this design\nscoring function, then we can discover the k topical terms by simply picking\nk terms with the highest scores. Now, of course, we might encounter situation where the\nhighest scored terms are all very similar. They're semantically similar, or\nclosely related, or even synonyms. So that's not desirable. So we also want to have coverage over\nall the content in the collection. So we would like to remove redundancy. And one way to do that is\nto do a greedy algorithm, which is sometimes called a maximal\nmarginal relevance ranking. Basically, the idea is to go down\nthe list based on our scoring function and gradually take terms\nto collect the k topical terms. The first term, of course, will be picked. When we pick the next term, we're\ngoing to look at what terms have already been picked and try to avoid\npicking a term that's too similar. So while we are considering\nthe ranking of a term in the list, we are also considering\nthe redundancy of the candidate term with respect to the terms\nthat we already picked. And with some thresholding,\nthen we can get a balance of the redundancy removal and\nalso high score of a term. Okay, so\nafter this that will get k topical terms. And those can be regarded as the topics\nthat we discovered from the connection. Next, let's think about how we're going\nto compute the topic coverage pi sub ij. So looking at this picture,\nwe have sports, travel and science and these topics. And now suppose you are give a document. How should we pick out coverage\nof each topic in the document? Well, one approach can be to simply\ncount occurrences of these terms. So for example, sports might have occurred\nfour times in this this document and travel occurred twice, etc. And then we can just normalize these\ncounts as our estimate of the coverage probability for each topic. So in general, the formula would\nbe to collect the counts of all the terms that represent the topics. And then simply normalize them so\nthat the coverage of each topic in the document would add to one. This forms a distribution of the topics\nfor the document to characterize coverage of different topics in the document. Now, as always,\nwhen we think about idea for solving problem, we have to ask\nthe question, how good is this one? Or is this the best way\nof solving problem? So now let's examine this approach. In general,\nwe have to do some empirical evaluation by using actual data sets and\nto see how well it works. Well, in this case let's take\na look at a simple example here. And we have a text document that's\nabout a NBA basketball game. So in terms of the content,\nit's about sports. But if we simply count these\nwords that represent our topics, we will find that the word sports\nactually did not occur in the article, even though the content\nis about the sports. So the count of sports is zero. That means the coverage of sports\nwould be estimated as zero. Now of course,\nthe term science also did not occur in the document and\nit's estimate is also zero. And that's okay. But sports certainly is not okay because\nwe know the content is about sports. So this estimate has problem. What's worse, the term travel\nactually occurred in the document. So when we estimate the coverage\nof the topic travel, we have got a non-zero count. So its estimated coverage\nwill be non-zero. So this obviously is also not desirable. So this simple example illustrates\nsome problems of this approach. First, when we count what\nwords belong to to the topic, we also need to consider related words. We can't simply just count\nthe topic word sports. In this case, it did not occur at all. But there are many related words\nlike basketball, game, etc. So we need to count\nthe related words also. The second problem is that a word\nlike star can be actually ambiguous. So here it probably means\na basketball star, but we can imagine it might also\nmean a star on the sky. So in that case, the star might actually\nsuggest, perhaps, a topic of science. So we need to deal with that as well. Finally, a main restriction of this\napproach is that we have only one term to describe the topic, so it cannot\nreally describe complicated topics. For example, a very specialized\ntopic in sports would be harder to describe by using just a word or\none phrase. We need to use more words. So this example illustrates\nsome general problems with this approach of treating a term as topic. First, it lacks expressive power. Meaning that it can only represent\nthe simple general topics, but it cannot represent the complicated topics\nthat might require more words to describe. Second, it's incomplete\nin vocabulary coverage, meaning that the topic itself\nis only represented as one term. It does not suggest what other\nterms are related to the topic. Even if we're talking about sports,\nthere are many terms that are related. So it does not allow us to easily\ncount related terms to order, conversion to coverage of this topic. Finally, there is this problem\nof word sense disintegration. A topical term or\nrelated term can be ambiguous. For example,\nbasketball star versus star in the sky. So in the next lecture,\nwe're going to talk about how to solve\nthe problem with of a topic. [MUSIC]",
 "07_2-7-topic-mining-and-analysis-probabilistic-topic-models.en.txt": "This lecture is about Probabilistic Topic\nModels for topic mining and analysis. In this lecture, we're going to continue talking\nabout the topic mining and analysis. We're going to introduce\nprobabilistic topic models. So this is a slide that\nyou have seen earlier, where we discussed the problems\nwith using a term as a topic. So, to solve these problems\nintuitively we need to use more words to describe the topic. And this will address the problem\nof lack of expressive power. When we have more words that we\ncan use to describe the topic, that we can describe complicated topics. To address the second problem we\nneed to introduce weights on words. This is what allows you to distinguish\nsubtle differences in topics, and to introduce semantically\nrelated words in a fuzzy manner. Finally, to solve the problem of\nword ambiguity, we need to split ambiguous word, so\nthat we can disambiguate its topic. It turns out that all these can be done\nby using a probabilistic topic model. And that's why we're going to spend a lot\nof lectures to talk about this topic. So the basic idea here is that, improve the replantation of\ntopic as one distribution. So what you see now is\nthe older replantation. Where we replanted each topic, it was just\none word, or one term, or one phrase. But now we're going to use a word\ndistribution to describe the topic. So here you see that for sports. We're going to use\nthe word distribution over theoretical speaking all\nthe words in our vocabulary. So for example, the high\nprobability words here are sports, game, basketball,\nfootball, play, star, etc. These are sports related terms. And of course it would also give\na non-zero probability to some other word like Trouble which might be\nrelated to sports in general, not so much related to topic. In general we can imagine a non\nzero probability for all the words. And some words that are not read and\nwould have very, very small probabilities. And these probabilities will sum to one. So that it forms a distribution\nof all the words. Now intuitively, this distribution\nrepresents a topic in that if we assemble words from the distribution, we tended\nto see words that are ready to dispose. You can also see, as a very special case,\nif the probability of the mass is concentrated in entirely on\njust one word, it's sports. And this basically degenerates\nto the symbol foundation of a topic was just one word. But as a distribution,\nthis topic of representation can, in general,\ninvolve many words to describe a topic and can model several differences\nin semantics of a topic. Similarly we can model Travel and Science\nwith their respective distributions. In the distribution for Travel we see top\nwords like attraction, trip, flight etc. Whereas in Science we see scientist,\nspaceship, telescope, or genomics, and, you know,\nscience related terms. Now that doesn't mean sports related terms will necessarily have zero\nprobabilities for science. In general we can imagine all of these\nwords we have now zero probabilities. It's just that for a particular\ntopic in some words we have very, very small probabilities. Now you can also see there are some\nwords that are shared by these topics. When I say shared it just means even\nwith some probability threshold, you can still see one word\noccurring much more topics. In this case I mark them in black. So you can see travel, for example,\noccurred in all the three topics here, but with different probabilities. It has the highest probability for\nthe Travel topic, 0.05. But with much smaller probabilities for\nSports and Science, which makes sense. And similarly, you can see a Star\nalso occurred in Sports and Science with reasonably\nhigh probabilities. Because they might be actually\nrelated to the two topics. So with this replantation it addresses the\nthree problems that I mentioned earlier. First, it now uses multiple\nwords to describe a topic. So it allows us to describe\na fairly complicated topics. Second, it assigns weights to terms. So now we can model several\ndifferences of semantics. And you can bring in related\nwords together to model a topic. Third, because we have probabilities for\nthe same word in different topics, we can disintegrate the sense of word. In the text to decode\nit's underlying topic, to address all these three problems with\nthis new way of representing a topic. So now of course our problem definition\nhas been refined just slightly. The slight is very similar to what\nyou've seen before except we have added refinement for what our topic is. Now each topic is word distribution,\nand for each word distribution we know that all the probabilities should sum to\none with all the words in the vocabulary. So you see a constraint here. And we still have another constraint\non the topic coverage, namely pis. So all the Pi sub ij's must sum to one for\nthe same document. So how do we solve this problem? Well, let's look at this problem\nas a computation problem. So we clearly specify it's input and output and\nillustrate it here on this side. Input of course is our text data. C is our collection but we also generally\nassume we know the number of topics, k. Or we hypothesize a number and\nthen try to bind k topics, even though we don't know the exact\ntopics that exist in the collection. And V is the vocabulary that has\na set of words that determines what units would be treated as\nthe basic units for analysis. In most cases we'll use words\nas the basis for analysis. And that means each word is a unique. Now the output would consist of as first\na set of topics represented by theta I's. Each theta I is a word distribution. And we also want to know the coverage\nof topics in each document. So that's. That the same pi ijs\nthat we have seen before. So given a set of text data we would\nlike compute all these distributions and all these coverages as you\nhave seen on this slide. Now of course there may be many\ndifferent ways of solving this problem. In theory, you can write the [INAUDIBLE]\nprogram to solve this problem, but here we're going to introduce a general way of solving this\nproblem called a generative model. And this is, in fact,\na very general idea and it's a principle way of using statistical\nmodeling to solve text mining problems. And here I dimmed the picture\nthat you have seen before in order to show the generation process. So the idea of this approach is actually\nto first design a model for our data. So we design a probabilistic model\nto model how the data are generated. Of course,\nthis is based on our assumption. The actual data aren't\nnecessarily generating this way. So that gave us a probability\ndistribution of the data that you are seeing on this slide. Given a particular model and\nparameters that are denoted by lambda. So this template of actually consists of all the parameters that\nwe're interested in. And these parameters in general\nwill control the behavior of the probability risk model. Meaning that if you set these\nparameters with different values and it will give some data points\nhigher probabilities than others. Now in this case of course,\nfor our text mining problem or more precisely topic mining problem\nwe have the following plans. First of all we have theta i's which\nis a word distribution snd then we have a set of pis for each document. And since we have n documents, so we have\nn sets of pis, and each set the pi up. The pi values will sum to one. So this is to say that we\nfirst would pretend we already have these word distributions and\nthe coverage numbers. And then we can see how we can generate\ndata by using such distributions. So how do we model the data in this way? And we assume that the data\nare actual symbols drawn from such a model that\ndepends on these parameters. Now one interesting question here is to think about how many\nparameters are there in total? Now obviously we can already see\nn multiplied by K parameters. For pi's. We also see k theta i's. But each theta i is actually a set\nof probability values, right? It's a distribution of words. So I leave this as an exercise for you to figure out exactly how\nmany parameters there are here. Now once we set up the model then\nwe can fit the model to our data. Meaning that we can\nestimate the parameters or infer the parameters based on the data. In other words we would like to\nadjust these parameter values. Until we give our data set\nthe maximum probability. I just said,\ndepending on the parameter values, some data points will have higher\nprobabilities than others. What we're interested in, here, is what parameter values will give\nour data set the highest probability? So I also illustrate the problem\nwith a picture that you see here. On the X axis I just illustrate lambda,\nthe parameters, as a one dimensional variable. It's oversimplification, obviously,\nbut it suffices to show the idea. And the Y axis shows the probability\nof the data, observe. This probability obviously depends\non this setting of lambda. So that's why it varies as you\nchange the value of lambda. What we're interested here\nis to find the lambda star. That would maximize the probability\nof the observed data. So this would be, then,\nour estimate of the parameters. And these parameters, note that are precisely what we\nhoped to discover from text data. So we'd treat these parameters\nas actually the outcome or the output of the data mining algorithm. So this is the general idea of using a generative model for text mining. First, we design a model with\nsome parameter values to fit the data as well as we can. After we have fit the data,\nwe will recover some parameter value. We will use the specific\nparameter value And those would be the output\nof the algorithm. And we'll treat those as actually\nthe discovered knowledge from text data. By varying the model of course we\ncan discover different knowledge. So to summarize, we introduced\na new way of representing topic, namely representing as word distribution\nand this has the advantage of using multiple words to describe a complicated\ntopic.It also allow us to assign weights on words so we have more than\nseveral variations of semantics. We talked about the task of topic mining,\nand answers. When we define a topic as distribution. So the importer is a clashing of text\narticles and a number of topics and a vocabulary set and\nthe output is a set of topics. Each is a word distribution and also the coverage of all\nthe topics in each document. And these are formally represented\nby theta i's and pi i's. And we have two constraints here for\nthese parameters. The first is the constraints\non the worded distributions. In each worded distribution\nthe probability of all the words must sum to 1,\nall the words in the vocabulary. The second constraint is on\nthe topic coverage in each document. A document is not allowed to recover\na topic outside of the set of topics that we are discovering. So, the coverage of each of these k\ntopics would sum to one for a document. We also introduce a general idea of using\na generative model for text mining. And the idea here is, first we're design\na model to model the generation of data. We simply assume that they\nare generative in this way. And inside the model we embed some\nparameters that we're interested in denoted by lambda. And then we can infer the most\nlikely parameter values lambda star, given a particular data set. And we can then take the lambda star as\nknowledge discovered from the text for our problem. And we can adjust\nthe design of the model and the parameters to discover various\nkinds of knowledge from text. As you will see later\nin the other lectures. [MUSIC]",
 "08_2-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1.en.txt": "[SOUND]\n>> This lecture is about the Overview\nof Statistical Language Models, which cover proper\nmodels as special cases. In this lecture we're going to give a overview of Statical Language Models. These models are general models that cover probabilistic topic models\nas a special cases. So first off,\nwhat is a Statistical Language Model? A Statistical Language Model is\nbasically a probability distribution over word sequences. So, for example,\nwe might have a distribution that gives, today is Wednesday a probability of .001. It might give today Wednesday is, which is a non-grammatical sentence, a very,\nvery small probability as shown here. And similarly another sentence, the eigenvalue is positive might\nget the probability of .00001. So as you can see such a distribution\nclearly is Context Dependent. It depends on the Context of Discussion. Some Word Sequences might have higher\nprobabilities than others but the same Sequence of Words might have different\nprobability in different context. And so this suggests that such a\ndistribution can actually categorize topic such a model can also be regarded\nas Probabilistic Mechanism for generating text. And that just means we can view text\ndata as data observed from such a model. For this reason,\nwe call such a model as Generating Model. So, now given a model we can then\nassemble sequences of words. So, for example, based on the distribution\nthat I have shown here on this slide, when matter it say assemble\na sequence like today is Wednesday because it has a relative\nhigh probability. We might often get such a sequence. We might also get the item\nvalue as positive sometimes with a smaller probability and\nvery, very occasionally we might get today is Wednesday because\nit's probability is so small. So in general, in order to categorize such\na distribution we must specify probability values for\nall these different sequences of words. Obviously, it's impossible\nto specify that because it's impossible to enumerate all of\nthe possible sequences of words. So in practice, we will have to\nsimplify the model in some way. So, the simplest language model is\ncalled the Unigram Language Model. In such a case, it was simply a the text is generated by generating\neach word independently. But in general, the words may\nnot be generated independently. But after we make this assumption, we can\nsignificantly simplify the language more. Basically, now the probability of\na sequence of words, w1 through wn, will be just the product of\nthe probability of each word. So for such a model, we have as many parameters as\nthe number of words in our vocabulary. So here we assume we have n words,\nso we have n probabilities. One for each word. And then some to 1. So, now we assume that\nour text is a sample drawn according to this word distribution. That just means,\nwe're going to draw a word each time and then eventually we'll get a text. So for example, now again, we can try to assemble words\naccording to a distribution. We might get Wednesday often or\ntoday often. And some other words like eigenvalue\nmight have a small probability, etcetera. But with this, we actually can\nalso compute the probability of every sequence, even though our model\nonly specify the probabilities of words. And this is because of the independence. So specifically, we can compute\nthe probability of today is Wednesday. Because it's just a product\nof the probability of today, the probability of is, and\nprobability of Wednesday. For example,\nI show some fake numbers here and when you multiply these numbers together you get\nthe probability that today's Wednesday. So as you can see, with N probabilities,\none for each word, we actually can characterize the probability situation\nover all kinds of sequences of words. And so, this is a very simple model. Ignore the word order. So it may not be, in fact, in some\nproblems, such as for speech recognition, where you may care about\nthe order of words. But it turns out to be\nquite sufficient for many tasks that involve topic analysis. And that's also what\nwe're interested in here. So when we have a model, we generally have\ntwo problems that we can think about. One is, given a model, how likely are we\nto observe a certain kind of data points? That is,\nwe are interested in the Sampling Process. The other is the Estimation Process. And that, is to think of\nthe parameters of a model given, some observe the data and we're\ngoing to talk about that in a moment. Let's first talk about the sampling. So, here I show two examples of Water\nDistributions or Unigram Language Models. The first one has higher probabilities for words like a text mining association,\nit's separate. Now this signals a topic about text mining\nbecause when we assemble words from such a distribution, we tend to see words\nthat often occur in text mining contest. So in this case,\nif we ask the question about what is the probability of\ngenerating a particular document. Then, we likely will see text that\nlooks like a text mining paper. Of course, the text that we\ngenerate by drawing words. This distribution is unlikely coherent. Although, the probability\nof generating attacks mine [INAUDIBLE] publishing\nin the top conference is non-zero assuming that no word has\na zero probability in the distribution. And that just means,\nwe can essentially generate all kinds of text documents including very\nmeaningful text documents. Now, the second distribution show, on the bottom, has different than\nwhat was high probabilities. So food [INAUDIBLE] healthy [INAUDIBLE],\netcetera. So this clearly indicates\na different topic. In this case it's probably about health. So if we sample a word\nfrom such a distribution, then the probability of observing a text\nmining paper would be very, very small. On the other hand, the probability of\nobserving a text that looks like a food nutrition paper would be high,\nrelatively higher. So that just means, given a particular\ndistribution, different than the text. Now let's look at\nthe estimation problem now. In this case, we're going to assume\nthat we have observed the data. I will know exactly what\nthe text data looks like. In this case,\nlet's assume we have a text mining paper. In fact, it's abstract of the paper,\nso the total number of words is 100. And I've shown some counts\nof individual words here. Now, if we ask the question,\nwhat is the most likely Language Model that has been\nused to generate this text data? Assuming that the text is observed\nfrom some Language Model, what's our best guess\nof this Language Model? Okay, so the problem now is just to\nestimate the probabilities of these words. As I've shown here. So what do you think? What would be your guess? Would you guess text has\na very small probability, or a relatively large probability? What about query? Well, your guess probably\nwould be dependent on how many times we have observed\nthis word in the text data, right? And if you think about it for a moment. And if you are like many others,\nyou would have guessed that, well, text has a probability of 10\nout of 100 because I've observed the text 10 times in the text\nthat has a total of 100 words. And similarly, mining has 5 out of 100. And query has a relatively small\nprobability, just observed for once. So it's 1 out of 100. Right, so that, intuitively,\nis a reasonable guess. But the question is, is this our best\nguess or best estimate of the parameters? Of course,\nin order to answer this question, we have to define what do we mean by best,\nin this case, it turns out that our\nguesses are indeed the best. In some sense and this is called\nMaximum Likelihood Estimate. And it's the best thing that, it will give\nthe observer data our maximum probability. Meaning that, if you change\nthe estimate somehow, even slightly, then the probability of the observed\ntext data will be somewhat smaller. And this is called\na Maximum Likelihood Estimate. [MUSIC]",
 "09_2-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2.en.txt": "[MUSIC] So now let's talk about the problem\na little bit more, and specifically let's talk about the two different ways\nof estimating the parameters. One is called the Maximum Likelihood\nestimate that I already just mentioned. The other is Bayesian estimation. So in maximum likelihood estimation,\nwe define best as meaning the data likelihood\nhas reached the maximum. So formally it's given\nby this expression here, where we define the estimate as a arg\nmax of the probability of x given theta. So, arg max here just means its\nactually a function that will turn. The argument that gives the function\nmaximum value, adds the value. So the value of arg max is not\nthe value of this function. But rather, the argument that has\nmade it the function reaches maximum. So in this case the value\nof arg max is theta. It's the theta that makes the probability\nof X, given theta, reach it's maximum. So this estimate that in due it also\nmakes sense and it's often very useful, and it seeks the premise\nthat best explains the data. But it has a problem, when the data\nis too small because when the data points are too small,\nthere are very few data points. The sample is small,\nthen if we trust data in entirely and try to fit the data and\nthen we'll be biased. So in the case of text data,\nlet's say, all observed 100 words did not contain another\nword related to text mining. Now, our maximum likelihood estimator\nwill give that word a zero probability. Because giving the non-zero probability would take away probability\nmass from some observer word. Which obviously is not optimal in\nterms of maximizing the likelihood of the observer data. But this zero probability for all the unseen words may not\nbe reasonable sometimes. Especially, if we want the distribution\nto characterize the topic of text mining. So one way to address this problem is\nactually to use Bayesian estimation, where we actually would look\nat the both the data, and our prior knowledge about the parameters. We assume that we have some prior\nbelief about the parameters. Now in this case of course, so we are not going to look at just the data,\nbut also look at the prior. So the prior here is\ndefined by P of theta, and this means, we will impose some\npreference on certain theta's of others. And by using Bayes Rule,\nthat I have shown here, we can then combine\nthe likelihood function. With the prior to give us this posterior probability of the parameter. Now, a full explanation of Bayes rule,\nand some of these things related to Bayesian reasoning,\nwould be outside the scope of this course. But I just gave a brief\nintroduction because this is general knowledge that\nmight be useful to you. The Bayes Rule is basically defined here,\nand allows us to write down one\nconditional probability of X given Y in terms of the conditional\nprobability of Y given X. And you can see the two probabilities are different in the order\nof the two variables. But often the rule is used for\nmaking inferences of the variable, so\nlet's take a look at it again. We can assume that p(X) Encodes\nour prior belief about X. That means before we observe any other\ndata, that's our belief about X, what we believe some X values have\nhigher probability than others. And this probability of X given Y is a conditional probability, and\nthis is our posterior belief about X. Because this is our belief about X\nvalues after we have observed the Y. Given that we have observed the Y,\nnow what do we believe about X? Now, do we believe some values have\nhigher probabilities than others? Now the two probabilities\nare related through this one, this can be regarded as the probability of the observed evidence Y,\ngiven a particular X. So you can think about X\nas our hypothesis, and we have some prior belief about\nwhich hypothesis to choose. And after we have observed Y,\nwe will update our belief and this updating formula is based\non the combination of our prior. And the likelihood of observing\nthis Y if X is indeed true, so much for detour about Bayes Rule. In our case, what we are interested\nin is inferring the theta values. So, we have a prior here that includes\nour prior knowledge about the parameters. And then we have the data likelihood here, that would tell us which parameter\nvalue can explain the data well. The posterior probability\ncombines both of them, so it represents a compromise\nof the the two preferences. And in such a case, we can maximize\nthis posterior probability. To find this theta that would\nmaximize this posterior probability, and this estimator is called a Maximum\na Posteriori, or MAP estimate. And this estimator is a more general estimator than\nthe maximum likelihood estimator. Because if we define our prior\nas a noninformative prior, meaning that it's uniform\nover all the theta values. No preference, then we basically would go\nback to the maximum likelihood estimated. Because in such a case,\nit's mainly going to be determined by this likelihood value, the same as here. But if we have some not informative prior,\nsome bias towards the different values then map estimator\ncan allow us to incorporate that. But the problem here of course,\nis how to define the prior. There is no free lunch and if you want to\nsolve the problem with more knowledge, we have to have that knowledge. And that knowledge,\nideally, should be reliable. Otherwise, your estimate may not\nnecessarily be more accurate than that maximum likelihood estimate. So, now let's look at the Bayesian\nestimation in more detail. So, I show the theta values as just a one dimension value and\nthat's a simplification of course. And so, we're interested in which\nvariable of theta is optimal. So now, first we have the Prior. The Prior tells us that\nsome of the variables are more likely the others would believe. For example, these values are more\nlikely than the values over here, or here, or other places. So this is our Prior, and then we have our theta likelihood. And in this case, the theta also tells us\nwhich values of theta are more likely. And that just means loose syllables\ncan best expand our theta. And then when we combine the two\nwe get the posterior distribution, and that's just a compromise of the two. It would say that it's\nsomewhere in-between. So, we can now look at some\ninteresting point that is made of. This point represents the mode of prior,\nthat means the most likely parameter value according to our prior,\nbefore we observe any data. This point is the maximum\nlikelihood estimator, it represents the theta that gives\nthe theta of maximum probability. Now this point is interesting,\nit's the posterior mode. It's the most likely value of the theta\ngiven by the posterior of this. And it represents a good\ncompromise of the prior mode and the maximum likelihood estimate. Now in general in Bayesian inference,\nwe are interested in the distribution of all these\nparameter additives as you see here. If there's a distribution over\nsee how values that you can see. Here, P of theta given X. So the problem of Bayesian inference is to infer this posterior, this regime, and also to infer other interesting\nquantities that might depend on theta. So, I show f of theta here as an interesting variable\nthat we want to compute. But in order to compute this value,\nwe need to know the value of theta. In Bayesian inference,\nwe treat theta as an uncertain variable. So we think about all\nthe possible variables of theta. Therefore, we can estimate the value of\nthis function f as extracted value of f, according to the posterior distribution\nof theta, given the observed evidence X. As a special case, we can assume f\nof theta is just equal to theta. In this case,\nwe get the expected value of the theta, that's basically the posterior mean. That gives us also one point of theta, and it's sometimes the same as posterior mode,\nbut it's not always the same. So, it gives us another way\nto estimate the parameter. So, this is a general illustration of\nBayesian estimation and its an influence. And later,\nyou will see this can be useful for topic mining where we want to inject\nthe sum prior knowledge about the topics. So to summarize,\nwe've used the language model which is basically probability\ndistribution over text. It's also called a generative model for\ntext data. The simplest language model\nis Unigram Language Model, it's basically a word distribution. We introduced the concept\nof likelihood function, which is the probability of\nthe a data given some model. And this function is very important, given a particular set of parameter\nvalues this function can tell us which X, which data point has a higher likelihood,\nhigher probability. Given a data sample X,\nwe can use this function to determine which parameter values would maximize\nthe probability of the observed data, and this is the maximum\nlivelihood estimate. We also talk about the Bayesian\nestimation or inference. In this case we, must define a prior\non the parameters p of theta. And then we're interested in computing the\nposterior distribution of the parameters, which is proportional to the prior and\nthe likelihood. And this distribution would allow us then\nto infer any derive that is from theta. [MUSIC]",
 "10_2-10-probabilistic-topic-models-mining-one-topic.en.txt": "[SOUND] This lecture is a continued discussion of probabilistic topic models. In this lecture, we're going to continue\ndiscussing probabilistic models. We're going to talk about\na very simple case where we are interested in just mining\none topic from one document. So in this simple setup,\nwe are interested in analyzing one document and\ntrying to discover just one topic. So this is the simplest\ncase of topic model. The input now no longer has k,\nwhich is the number of topics because we know there is only one topic and the\ncollection has only one document, also. In the output,\nwe also no longer have coverage because we assumed that the document\ncovers this topic 100%. So the main goal is just to discover\nthe world of probabilities for this single topic, as shown here. As always, when we think about using a\ngenerating model to solve such a problem, we start with thinking about what\nkind of data we are going to model or from what perspective we're going to\nmodel the data or data representation. And then we're going to\ndesign a specific model for the generating of the data,\nfrom our perspective. Where our perspective just means we want\nto take a particular angle of looking at the data, so that the model will\nhave the right parameters for discovering the knowledge that we want. And then we'll be thinking\nabout the microfunction or write down the microfunction to\ncapture more formally how likely a data point will be\nobtained from this model. And the likelihood function will have\nsome parameters in the function. And then we argue our interest in\nestimating those parameters for example, by maximizing the likelihood which will\nlead to maximum likelihood estimated. These estimator parameters\nwill then become the output of the mining hours,\nwhich means we'll take the estimating parameters as the knowledge\nthat we discover from the text. So let's look at these steps for\nthis very simple case. Later we'll look at this procedure for\nsome more complicated cases. So our data, in this case is, just\na document which is a sequence of words. Each word here is denoted by x sub i. Our model is a Unigram language model. A word distribution that we hope to\ndenote a topic and that's our goal. So we will have as many parameters as many\nwords in our vocabulary, in this case M. And for convenience we're\ngoing to use theta sub i to denote the probability of word w sub i. And obviously these theta\nsub i's will sum to 1. Now what does a likelihood\nfunction look like? Well, this is just the probability\nof generating this whole document, that given such a model. Because we assume the independence in\ngenerating each word so the probability of the document will be just a product\nof the probability of each word. And since some word might\nhave repeated occurrences. So we can also rewrite this\nproduct in a different form. So in this line, we have rewritten\nthe formula into a product over all the unique words in\nthe vocabulary, w sub 1 through w sub M. Now this is different\nfrom the previous line. Well, the product is over different\npositions of words in the document. Now when we do this transformation,\nwe then would need to introduce a counter function here. This denotes the count of\nword one in document and similarly this is the count\nof words of n in the document because these words might\nhave repeated occurrences. You can also see if a word did\nnot occur in the document. It will have a zero count, therefore\nthat corresponding term will disappear. So this is a very useful form of writing down the likelihood function\nthat we will often use later. So I want you to pay attention to this,\njust get familiar with this notation. It's just to change the product over all\nthe different words in the vocabulary. So in the end, of course, we'll use\ntheta sub i to express this likelihood function and it would look like this. Next, we're going to find\nthe theta values or probabilities of these words that would maximize\nthis likelihood function. So now lets take a look at the maximum\nlikelihood estimate problem more closely. This line is copied from\nthe previous slide. It's just our likelihood function. So our goal is to maximize\nthis likelihood function. We will find it often easy to maximize the local likelihood\ninstead of the original likelihood. And this is purely for\nmathematical convenience because after the logarithm transformation our function\nwill becomes a sum instead of product. And we also have constraints\nover these these probabilities. The sum makes it easier to take\nderivative, which is often needed for finding the optimal\nsolution of this function. So please take a look at this sum again,\nhere. And this is a form of\na function that you will often see later also,\nthe more general topic models. So it's a sum over all\nthe words in the vocabulary. And inside the sum there is\na count of a word in the document. And this is macroed by\nthe logarithm of a probability. So let's see how we can\nsolve this problem. Now at this point the problem is purely a\nmathematical problem because we are going to just the find the optimal solution\nof a constrained maximization problem. The objective function is\nthe likelihood function and the constraint is that all these\nprobabilities must sum to one. So, one way to solve the problem is\nto use Lagrange multiplier approace. Now this command is beyond\nthe scope of this course but since Lagrange multiplier is a very\nuseful approach, I also would like to just give a brief introduction to this,\nfor those of you who are interested. So in this approach we will\nconstruct a Lagrange function, here. And this function will combine\nour objective function with another term that\nencodes our constraint and we introduce Lagrange multiplier here, lambda, so it's an additional parameter. Now, the idea of this approach is just to\nturn the constraint optimization into, in some sense,\nan unconstrained optimizing problem. Now we are just interested in\noptimizing this Lagrange function. As you may recall from calculus,\nan optimal point would be achieved when\nthe derivative is set to zero. This is a necessary condition. It's not sufficient, though. So if we do that you will\nsee the partial derivative, with respect to theta i\nhere ,is equal to this. And this part comes from the derivative\nof the logarithm function and this lambda is simply taken from here. And when we set it to zero we can easily see theta sub i is\nrelated to lambda in this way. Since we know all the theta\ni's must a sum to one we can plug this into this constraint,\nhere. And this will allow us to solve for\nlambda. And this is just a net\nsum of all the counts. And this further allows us to then\nsolve the optimization problem, eventually, to find the optimal\nsetting for theta sub i. And if you look at this formula it turns\nout that it's actually very intuitive because this is just the normalized\ncount of these words by the document ns, which is also a sum of all\nthe counts of words in the document. So, after all this mess, after all, we have just obtained something\nthat's very intuitive and this will be just our\nintuition where we want to maximize the data by\nassigning as much probability mass as possible to all\nthe observed the words here. And you might also notice that this is\nthe general result of maximum likelihood raised estimator. In general, the estimator would be to\nnormalize counts and it's just sometimes the counts have to be done in a particular\nway, as you will also see later. So this is basically an analytical\nsolution to our optimization problem. In general though, when the likelihood\nfunction is very complicated, we're not going to be able to solve the optimization\nproblem by having a closed form formula. Instead we have to use some\nnumerical algorithms and we're going to see such cases later, also. So if you imagine what would we\nget if we use such a maximum likelihood estimator to estimate one\ntopic for a single document d here? Let's imagine this document\nis a text mining paper. Now, what you might see is\nsomething that looks like this. On the top, you will see the high\nprobability words tend to be those very common words,\noften functional words in English. And this will be followed by\nsome content words that really characterize the topic well like text,\nmining, etc. And then in the end,\nyou also see there is more probability of words that are not really\nrelated to the topic but they might be extraneously\nmentioned in the document. As a topic representation,\nyou will see this is not ideal, right? That because the high probability\nwords are functional words, they are not really\ncharacterizing the topic. So my question is how can we\nget rid of such common words? Now this is the topic of the next module. We're going to talk about how to use\nprobabilistic models to somehow get rid of these common words. [MUSIC]",
 "01_3-1-probabilistic-topic-models-mixture-of-unigram-language-models.en.txt": "[MUSIC] This lecture is about the mixture\nof unigram language models. In this lecture we will continue\ndiscussing probabilistic topic models. In particular, what we introduce\na mixture of unigram language models. This is a slide that\nyou have seen earlier. Where we talked about how to\nget rid of the background words that we have on top of for\none document. So if you want to solve the problem, it would be useful to think about\nwhy we end up having this problem. Well, this obviously because these\nwords are very frequent in our data and we are using a maximum\nlikelihood to estimate. Then the estimate obviously would\nhave to assign high probability for these words in order to\nmaximize the likelihood. So, in order to get rid of them that\nwould mean we'd have to do something differently here. In particular we'll have\nto say this distribution doesn't have to explain all\nthe words in the tax data. What were going to say is that, these common words should not be\nexplained by this distribution. So one natural way to solve the problem is\nto think about using another distribution to account for just these common words. This way, the two distributions can be\nmixed together to generate the text data. And we'll let the other model which\nwe'll call background topic model to generate the common words. This way our target topic theta\nhere will be only generating the common handle words that are\ncharacterised the content of the document. So, how does this work? Well, it is just a small\nmodification of the previous setup where we have just one distribution. Since we now have two distributions, we have to decide which distribution\nto use when we generate the word. Each word will still be a sample\nfrom one of the two distributions. Text data is still\ngenerating the same way. Namely, look at the generating\nof the one word at each time and eventually we generate a lot of words. When we generate the word, however, we're going to first decide\nwhich of the two distributions to use. And this is controlled by another\nprobability, the probability of theta sub d and\nthe probability of theta sub B here. So this is a probability of enacting\nthe topic word of distribution. This is the probability of\nenacting the background word of distribution denoted by theta sub B. On this case I just give example\nwhere we can set both to 0.5. So you're going to basically flip a coin,\na fair coin, to decide what you want to use. But in general these probabilities\ndon't have to be equal. So you might bias toward using\none topic more than the other. So now the process of generating a word\nwould be to first we flip a coin. Based on these probabilities choosing\neach model and if let's say the coin shows up as head, which means we're going\nto use the topic two word distribution. Then we're going to use this word\ndistribution to generate a word. Otherwise we might be\ngoing slow this path. And we're going to use the background\nword distribution to generate a word. So in such a case,\nwe have a model that has some uncertainty associated with the use\nof a word distribution. But we can still think of this as\na model for generating text data. And such a model is\ncalled a mixture model. So now let's see. In this case, what's the probability\nof observing a word w? Now here I showed some words. like \"the\" and \"text\". So as in all cases, once we setup a model we are interested\nin computing the likelihood function. The basic question is, so what's the probability of\nobserving a specific word here? Now we know that the word can be observed\nfrom each of the two distributions, so we have to consider two cases. Therefore it's a sum over these two cases. The first case is to use the topic for\nthe distribution to generate the word. And in such a case then\nthe probably would be theta sub d, which is the probability\nof choosing the model multiplied by the probability of actually\nobserving the word from that model. Both events must happen\nin order to observe. We first must have choosing\nthe topic theta sub d and then, we also have to actually have sampled\nthe word the from the distribution. And similarly,\nthe second part accounts for a different way of generally\nthe word from the background. Now obviously the probability of\ntext the same is all similar, right? So we also can see the two\nways of generating the text. And in each case, it's a product of the\nprobability of choosing a particular word is multiplied by the probability of\nobserving the word from that distribution. Now whether you will see,\nthis is actually a general form. So might want to make sure that you have\nreally understood this expression here. And you should convince yourself that\nthis is indeed the probability of obsolete text. So to summarize what we observed here. The probability of a word from\na mixture model is a general sum of different ways of generating the word. In each case, it's a product of the probability\nof selecting that component model. Multiplied by the probability of\nactually observing the data point from that component of the model. And this is something quite general and\nyou will see this occurring often later. So the basic idea of a mixture\nmodel is just to retrieve thesetwo distributions\ntogether as one model. So I used a box to bring all\nthese components together. So if you view this\nwhole box as one model, it's just like any other generative model. It would just give us\nthe probability of a word. But the way that determines this\nprobability is quite the different from when we have just one distribution. And this is basically a more\ncomplicated mixture model. So the more complicated is more\nthan just one distribution. And it's called a mixture model. So as I just said we can treat\nthis as a generative model. And it's often useful to think of\njust as a likelihood function. The illustration that\nyou have seen before, which is dimmer now, is just\nthe illustration of this generated model. So mathematically,\nthis model is nothing but to just define the following\ngenerative model. Where the probability of a word is\nassumed to be a sum over two cases of generating the word. And the form you are seeing now\nis a more general form that what you have seen in\nthe calculation earlier. Well I just use the symbol\nw to denote any water but you can still see this is\nbasically first a sum. Right? And this sum is due to the fact that the\nwater can be generated in much more ways, two ways in this case. And inside a sum,\neach term is a product of two terms. And the two terms are first\nthe probability of selecting a component like of D Second, the probability of actually observing\nthe word from this component of the model. So this is a very general description\nof all the mixture models. I just want to make sure\nthat you understand this because this is really the basis for\nunderstanding all kinds of on top models. So now once we setup model. We can write down that like\nfunctioning as we see here. The next question is,\nhow can we estimate the parameter, or what to do with the parameters. Given the data. Well, in general, we can use some of the text data\nto estimate the model parameters. And this estimation would allow us to discover the interesting\nknowledge about the text. So you, in this case, what do we discover? Well, these are presented\nby our parameters and we will have two kinds of parameters. One is the two worded distributions,\nthat result in topics, and the other is the coverage\nof each topic in each. The coverage of each topic. And this is determined by\nprobability of C less of D and probability of theta, so this is to one. Now, what's interesting is\nalso to think about special cases like when we send one of\nthem to want what would happen? Well with the other, with the zero right? And if you look at\nthe likelihood function, it will then degenerate to the special\ncase of just one distribution. Okay so you can easily verify that by\nassuming one of these two is 1.0 and the other is Zero. So in this sense,\nthe mixture model is more general than the previous model where we\nhave just one distribution. It can cover that as a special case. So to summarize, we talked about the\nmixture of two Unigram Language Models and the data we're considering\nhere is just One document. And the model is a mixture\nmodel with two components, two unigram LM models,\nspecifically theta sub d, which is intended to denote the topic of\ndocument d, and theta sub B, which is representing a background topic that\nwe can set to attract the common words because common words would be\nassigned a high probability in this model. So the parameters can\nbe collectively called Lambda which I show here you can again think about the question about how many\nparameters are we talking about exactly. This is usually a good exercise to do\nbecause it allows you to see the model in depth and to have a complete understanding\nof what's going on this model. And we have mixing weights,\nof course, also. So what does a likelihood\nfunction look like? Well, it looks very similar\nto what we had before. So for the document, first it's a product over all the words in\nthe document exactly the same as before. The only difference is that inside here\nnow it's a sum instead of just one. So you might have recalled before\nwe just had this one there. But now we have this sum\nbecause of the mixture model. And because of the mixture model we\nalso have to introduce a probability of choosing that particular\ncomponent of distribution. And so\nthis is just another way of writing, and by using a product over all the unique\nwords in our vocabulary instead of having that product over all\nthe positions in the document. And this form where we look at\nthe different and unique words is a commutative that formed for computing\nthe maximum likelihood estimate later. And the maximum likelihood estimator is,\nas usual, just to find the parameters that would\nmaximize the likelihood function. And the constraints here\nare of course two kinds. One is what are probabilities in each [INAUDIBLE] must sum to 1 the other is the choice of each\n[INAUDIBLE] must sum to 1. [MUSIC]",
 "02_3-2-probabilistic-topic-models-mixture-model-estimation-part-1.en.txt": "This lecture is about\nthe mixture model estimation. In this lecture, we're\ngoing to continue discussing probabilistic\ntopic models. In particular, we're going\nto talk about the how to estimate the parameters\nof a mixture model. So let's first look\nat our motivation for using a mixture model, and we hope to effect out the background words from\nthe topic word distribution. So the idea is to assume that the text data actually\ncontain two kinds of words. One kind is from\nthe background here, so the \"is\", \"we\" etc. The other kind is from our topic word distribution\nthat we're interested in. So in order to solve\nthis problem of factoring out background words, we can set up our mixture\nmodel as follows. We are going to assume that we already know the parameters of all the values for all the parameters in\nthe mixture model except for the word distribution of\nTheta sub d which is our target. So this is a case of\ncustomizing probably some model so that we embedded the unknown variables\nthat we are interested in, but we're going to\nsimplify other things. We're going to assume\nwe have knowledge about others and this is a powerful way of customizing a model\nfor a particular need. Now you can imagine, we\ncould have assumed that we also don't know the\nbackground word distribution, but in this case,\nour goal is to affect out precisely those high probability\nin the background words. So we assume the background\nmodel is already fixed. The problem here is, how can we adjust the Theta sub\nd in order to maximize the probability of\nthe observed document here and we assume all the\nother parameters are known? Now, although we\ndesigned the modal heuristically to try to factor out these\nbackground words, it's unclear whether if we use maximum\nlikelihood estimator, we will actually end up having\na word distribution where the common words like \"the\" will be indeed having smaller\nprobabilities than before. So now, in this case, it turns out that\nthe answer is yes. When we set up the probabilistic\nmodeling this way, when we use maximum\nlikelihood estimator, we will end up having\na word distribution where the common words\nwould be factored out by the use of\nthe background distribution. So to understand why this is so, it's useful to examine\nthe behavior of a mixture model. So we're going to look\nat a very simple case. In order to understand some interesting behaviors\nof a mixture model, the observed patterns\nhere actually are generalizable to mixture\nmodel in general, but it's much easier to\nunderstand this behavior when we use a very simple case\nlike what we're seeing here. So specifically in this case, let's assume that\nthe probability of choosing each of the two models\nis exactly the same. So we're going to flip a fair coin to decide\nwhich model to use. Furthermore, we are going\nto assume there are precisely to words,\n\"the\" and \"text.\" Obviously, this is\na very naive oversimplification of the actual text, but again, it is useful to examine the behavior\nin such a special case. So we further assume that, the background model gives\nprobability of 0.9 to the word \"the\" and \"text\" 0.1. Now, let's also assume that\nour data is extremely simple. The document has just two words\n\"text\" and then \"the.\" So now, let's write down the likelihood function\nin such a case. First, what's the probability of \"text\" and what's the\nprobability of \"the\"? I hope by this point, you will be able\nto write it down. So the probability of \"text\" is basically a sum of\ntwo cases where each case corresponds to each of the water distribution and it accounts for the two ways\nof generating text. Inside each case, we have the probability of choosing\nthe model which is 0.5 multiplied by the probability of observing \"text\"\nfrom that model. Similarly, \"the\" would\nhave a probability of the same form just as it was different exactly\nprobabilities. So naturally,\nour likelihood function is just the product of the two. So it's very easy to see that, once you understand\nwhat's the probability of each word and which\nis also why it's so important to understand what's exactly the probability of observing each word from\nsuch a mixture model. Now, the interesting\nquestion now is, how can we then optimize\nthis likelihood? Well, you will notice that, there are only two variables. They are precisely\nthe two probabilities of the two words\n\"text\" and \"the\" given by Theta sub d. This is\nbecause we have assumed that, all the other\nparameters are known. So now, the question is\na very simple algebra question. So we have a simple expression with two variables and we hope to choose the values of these two variables to\nmaximize this function. It's exercises that we have seen some simple\nalgebra problems, and note that the two\nprobabilities must sum to one. So there's some constraint. If there were\nno constraint of course, we will set both probabilities to their maximum value which\nwould be one to maximize this, but we can't do that because \"text\" and\n\"the\" must sum to one. We can't give those a\nprobability of one. So now the question is, how should we allocate\nthe probability in the mass between the two words?\nWhat do you think? Now, it will be useful to look at this formula for\nmoment and to see intuitively what\nwe do in order to set these probabilities to maximize the value\nof this function. If we look into this further, then we'll see\nsome interesting behavior of the two component\nmodels in that, they will be\ncollaborating to maximize the probability of\nthe observed data which is dictated by the maximum\nlikelihood estimator, but they're also\ncompeting in some way. In particular, they\nwould be competing on the words and they\nwill tend to bet high probabilities on\ndifferent words to avoid this competition in some sense or to gain advantage\nin this competition. So again, looking at this\nobjective function and we have a constraint on\nthe two probabilities, now if you look at\nthe formula intuitively, you might feel that\nyou want to set the probability of \"text\" to be somewhat larger than \"the\". This intuition can\nbe well-supported by mathematical fact which is, when the sum of\ntwo variables is a constant then the product of them which is maximum\nthen they are equal, and this is a fact that\nwe know from algebra. Now, if we plug that in, we will would mean\nthat we have to make the two probabilities equal. When we make them equal\nand then if we consider the constraint that we can\neasily solve this problem, and the solution is the\nprobability of \"text\" would be 0.9 and probability\nof \"the\" is 0.1. As you can see indeed, the probability of text\nis not much larger than probability of \"the\" and this is not the case when we\nhave just one distribution. This is clearly because of the use of the\nbackground model which assign a very high probability to \"the\" low\nprobability to \"text\". If you look at the equation, you will see obviously some interaction of\nthe two distributions here. In particular, you will see in order to make them equal and then the probability assigned\nby Theta sub d must be higher for a word that has a smaller probability\ngiven by the background. This is obvious from\nexamining this equation because \"the\" background part is weak for \"text\" it's a small. So in order to\ncompensate for that, we must make the probability\nof \"text\" that's given by Theta sub d somewhat larger so that the two sides\ncan be balanced. So this is in fact a very general behavior\nof this mixture model. That is, if one distribution assigns a high probability\nto one word than another, then the other distribution would tend to do the opposite. Basically, it would discourage other distributions to do the same and this is to\nbalance them out so that, we can account for all words. This also means that, by using a background\nmodel that is fixed to assign high probabilities\nto background words, we can indeed encourage the unknown topic\nword distribution to assign smaller probabilities\nfor such common words. Instead, put more probability\nmass on the content words that cannot be explained well by the background\nmodel meaning that, they have a very\nsmall probability from the background\nmodel like \"text\" here.",
 "03_3-3-probabilistic-topic-models-mixture-model-estimation-part-2.en.txt": "[SOUND] Now lets look at another\nbehaviour of the Mixed Model and in this case lets look at\nthe response to data frequencies. So what you are seeing now is basically\nthe likelihood of function for the two word document and\nwe now in this case the solution is text. A probability of 0.9 and\nthe a probability of 0.1. Now it's interesting to think about a scenario where we start\nadding more words to the document. So what would happen if we add\nmany the's to the document? Now this would change the game, right? So, how? Well, picture, what would\nthe likelihood function look like now? Well, it start with the likelihood\nfunction for the two words, right? As we add more words, we know that. But we have to just multiply\nthe likelihood function by additional terms to account for\nthe additional. occurrences of that. Since in this case, all the additional terms are the,\nwe're going to just multiply by this term. Right?\nFor the probability of the. And if we have another occurrence of the,\nwe'd multiply again by the same term, and so on and forth. Add as many terms as the number of\nthe's that we add to the document, d'. Now this obviously changes\nthe likelihood function. So what's interesting is now to think\nabout how would that change our solution? So what's the optimal solution now? Now, intuitively you'd know\nthe original solution, pulling the 9 versus pulling the ,will no\nlonger be optimal for this new function. Right? But, the question is how\nshould we change it. What general is to sum to one. So he know we must take away some\nprobability the mass from one word and add the probability\nmass to the other word. The question is which word to\nhave reduce the probability and which word to have a larger probability. And in particular,\nlet's think about the probability of the. Should it be increased\nto be more than 0.1? Or should we decrease it to less than 0.1? What do you think? Now you might want to pause the video\na moment to think more about. This question. Because this has to do with understanding\nof important behavior of a mixture model. And indeed,\nother maximum likelihood estimator. Now if you look at the formula for\na moment, then you will see it seems like another object Function is more\ninfluenced by the than text. Before, each computer. So now as you can imagine,\nit would make sense to actually assign a smaller probability for\ntext and lock it. To make room for\na larger probability for the. Why?\nBecause the is repeated many times. If we increase it a little bit,\nit will have more positive impact. Whereas a slight decrease of text\nwill have relatively small impact because it occurred just one, right? So this means there is another\nbehavior that we observe here. That is high frequency words\ngenerated with high probabilities from all the distributions. And, this is no surprise at all, because after all, we are maximizing\nthe likelihood of the data. So the more a word occurs, then it\nmakes more sense to give such a word a higher probability because the impact\nwould be more on the likelihood function. This is in fact a very general phenomenon\nof all the maximum likelihood estimator. But in this case, we can see as we\nsee more occurrences of a term, it also encourages the unknown\ndistribution theta sub d to assign a somewhat higher\nprobability to this word. Now it's also interesting to think about\nthe impact of probability of Theta sub B. The probability of choosing one\nof the two component models. Now we've been so far assuming\nthat each model is equally likely. And that gives us 0.5. But you can again look at this likelihood\nfunction and try to picture what would happen if we increase the probability\nof choosing a background model. Now you will see these terms for the, we have a different form where\nthe probability that would be even larger because the background has\na high probability for the word and the coefficient in front of 0.9 which\nis now 0.5 would be even larger. When this is larger,\nthe overall result would be larger. And that also makes this\nthe less important for theta sub d to increase\nthe probability before the. Because it's already very large. So the impact here of increasing\nthe probability of the is somewhat regulated by this coefficient,\nthe point of i. If it's larger on the background, then it becomes less important\nto increase the value. So this means the behavior here, which is high frequency words tend to get\nthe high probabilities, are effected or regularized somewhat by the probability\nof choosing each component. The more likely a component\nis being chosen. It's more important that to have higher\nvalues for these frequent words. If you have a various small probability of\nbeing chosen, then the incentive is less. So to summarize,\nwe have just discussed the mixture model. And we discussed that the estimation\nproblem of the mixture model and particular with this discussed some\ngeneral behavior of the estimator and that means we can expect our\nestimator to capture these infusions. First every component model attempts to assign high probabilities to\nhigh frequent their words in the data. And this is to collaboratively\nmaximize likelihood. Second, different component models tend to\nbet high probabilities on different words. And this is to avoid a competition or\nwaste of probability. And this would allow them to collaborate\nmore efficiently to maximize the likelihood. So, the probability of choosing each\ncomponent regulates the collaboration and the competition between component models. It would allow some component models\nto respond more to the change, for example, of frequency of\nthe theta point in the data. We also talked about the special case\nof fixing one component to a background word distribution, right? And this distribution can be estimated\nby using a collection of documents, a large collection of English documents,\nby using just one distribution and then we'll just have normalized\nfrequencies of terms to give us the probabilities\nof all these words. Now when we use such\na specialized mixture model, we show that we can effectively get rid\nof that one word in the other component. And that would make this cover\ntopic more discriminative. This is also an example of imposing\na prior on the model parameter and the prior here basically means one model\nmust be exactly the same as the background language model and if you recall what we\ntalked about in Bayesian estimation, and this prior will allow us to favor a model\nthat is consistent with our prior. In fact, if it's not consistent we're\ngoing to say the model is impossible. So it has a zero prior probability. That effectively excludes such a scenario. This is also issue that\nwe'll talk more later. [MUSIC]",
 "04_3-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1.en.txt": "This lecture is about the expectation-maximization\nalgorithm, also called the EM algorithm. In this lecture, we're\ngoing to continue the discussion of\nprobabilistic topic models. In particular, we're going to\nintroduce the EM algorithm, which is a family of\nuseful algorithms for computing the maximum likelihood estimate\nof mixture models. So this is now familiar scenario of\nusing a two component, the mixture model, to try to factor out\nthe background words from one topic word\nof distribution here. So we're interested in\ncomputing this estimate, and we're going to try to adjust these probability values to maximize the probability\nof the observed document. Note that we assume that all the other parameters are known. So the only thing unknown is the word probabilities\ngiven by theta sub. In this lecture, we're\ngoing to look into how to compute this maximum\nlikelihood estimate. Now, let's start with the idea of separating the words in\nthe text data into two groups. One group would be explained\nby the background model. The other group would\nbe explained by the unknown topic\nword distribution. After all, this is\nthe basic idea of mixture model. But suppose we actually know which word is from\nwhich distribution? So that would mean, for example, these words the, is, and we are known to be from this background\nword distribution. On the other hand, the\nother words text, mining, clustering etc are known to be from the topic word distribution. If you can see the color, then these are shown in blue. These blue words are then assumed that to be from\nthe topic word distribution. If we already know how\nto separate these words, then the problem of estimating the word distribution\nwould be extremely simple. If you think about\nthis for a moment, you'll realize that, well, we can simply take\nall these words that are known to be from this word distribution theta sub d\nand normalize them. So indeed this problem would be very easy to solve if we had known which words are from which a distribution precisely, and this is in fact making this model no\nlonger a mixture model because we can already observe which distribution has been used to generate\nwhich part of the data. So we actually go back to the single word\ndistribution problem. In this case let's call these words that are\nknown to be from theta d, a pseudo document of d prime, and now all we need to\ndo is just normalize these words counts\nfor each word w_i. That's fairly straightforward. It's just dictated by the\nmaximum likelihood estimator. Now, this idea however\ndoesn't work because we in practice don't really know which word is from\nwhich distribution, but this gives us\nthe idea of perhaps we can guess which word is\nfrom which it is written. Specifically given\nall the parameters, can we infer the distribution\na word is from. So let's assume that we actually know tentative probabilities for these words in theta sub d. So now all the parameters are known for this mixture model, and now let's consider\na word like a \"text\". So the question is, do you\nthink \"text\" is more likely having been generated from theta sub d or from\ntheta sub of b? So in other words,\nwe want to infer which distribution has been\nused to generate this text. Now, this inference process is a typical Bayesian inference\nsituation where we have some prior about\nthese two distributions. So can you see what\nis our prior here? Well, the prior here is the probability of\neach distribution. So the prior is given by\nthese two probabilities. In this case, the prior is saying that each model\nis equally likely, but we can imagine perhaps a\ndifferent prior is possible. So this is called a prior\nbecause this is our guess of which distribution has\nbeen used to generate a word before we even\noff reserve the word. So that's why we\ncall it the prior. So if we don't observe the word, we don't know what word\nhas been observed. Our best guess is to say\nwell, they're equally likely. All right. So it's\njust flipping a coin. Now in Bayesian inference we\ntypically learn with update our belief after we have\nobserved the evidence. So what is the evidence here? Well, the evidence\nhere is the word text. Now that we know we're\ninterested in the word text. So text that can be\nregarded as evidence, and if we use Bayes rule to combine the\nprior and the data likelihood, what we will end up\nwith is to combine the prior with the likelihood\nthat you see here, which is basically\nthe probability of the word text from\neach distribution. We see that in both cases\nthe text is possible. Note that even in the background\nit is still possible, it just has a very\nsmall probability. So intuitively what would\nbe your guess in this case. Now if you're like many others, you are guess text\nis probably from theta sub d. It's more likely\nfrom theta sub d. Why? You will probably see that\nit's because text that has a much higher probability\nhere by the theta sub d, then by the background model which has a very\nsmall probability. By this we're going to say, well, text is more likely from theta sub d. So you see our guess of which\ndistribution has been used to generate\nthe text would depend on how high the probability of the text is in\neach word distribution. We can do, tend to guess the distribution that gives us a word a higher probability, and this is likely to\nmaximize the likelihood. So we're going to choose a word that has\na higher likelihood. So in other words,\nwe're going to compare these two probabilities of the word given by\neach distributions. But our guess must also\nbe affected by the prior. So we also need to\ncompare these two priors. Why? Because imagine if we\nadjust these probabilities, we're going to say\nthe probability of choosing a background model is\nalmost 100 percent. Now, if you have that kind\nof strong prior, then that would\naffect your guess. You might think,\nwell, wait a moment, maybe text could have been\nfrom the background as well. Although the probability\nis very small here, the prior is very high. So in the end, we have\nto combine the two, and the base formula provides us a solid and principled way of making this kind of\nguess to quantify that. So more specifically, let's think about\nthe probability that this word has been generated in fact from from theta sub d. Well, in order for texts\nto be generated from theta sub d two things\nmust happen. First, the theta sub d\nmust have been selected, so we have the selection\nprobability here. Secondly, we also have to actually have observed text\nfrom the distribution. So when we multiply\nthe two together, we get the probability\nthat text has in fact been generated from\ntheta sub d. Similarly, for the background model, the probability of generating text is another product\nof a similar form. Now, we also introduced the latent variable\nz here to denote whether the word is from\nthe background or the topic. When z is zero, it means it's from the topic\ntheta sub d. When it's one, it means it's from\nthe background theta sub b. So now we have the probability that text\nis generated from each. Then we can simply normalize them to have an estimate\nof the probability that the word text is from theta sub d or\nfrom theta sub b. Then equivalently, the\nprobability that z is equal to zero given that\nthe observed evidence is text. So this is application\nof Bayes rule. But this step is very\ncrucial for understanding the EM algorithm because\nif we can do this, then we would be able to first initialize the parameter values\nsomewhat randomly, and then we're going to take\na guess of these z values. Which distributing has been\nused to generate which word, and the initialized\nthe parameter values would allow us to have a complete specification of\nthe mixture model which further allows us to\napply Bayes rule to infer which distribution is more\nlikely to generate each word. This prediction\nessentially helped us to separate the words from\nthe two distributions. Although we can't\nseparate them for sure, but we can separate them\nprobabilistically as shown here.",
 "05_3-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2.en.txt": "[SOUND]\nSo this is indeed a general idea of\nthe Expectation-Maximization, or EM, Algorithm. So in all the EM algorithms we\nintroduce a hidden variable to help us solve the problem more easily. In our case the hidden variable\nis a binary variable for each occurrence of a word. And this binary variable would\nindicate whether the word has been generated from 0 sub d or 0 sub p. And here we show some possible\nvalues of these variables. For example, for the it's from background,\nthe z value is one. And text on the other hand. Is from the topic then it's zero for\nz, etc. Now, of course, we don't observe these z\nvalues, we just imagine they're all such. Values of z attaching to other words. And that's why we call\nthese hidden variables. Now, the idea that we\ntalked about before for predicting the word distribution that\nhas been used when we generate the word is it a predictor,\nthe value of this hidden variable? And, so, the EM algorithm then,\nwould work as follows. First, we'll initialize all\nthe parameters with random values. In our case,\nthe parameters are mainly the probability. of a word, given by theta sub d. So this is an initial addition stage. These initialized values would allow\nus to use base roll to take a guess of these z values, so\nwe'd guess these values. We can't say for sure whether\ntextt is from background or not. But we can have our guess. This is given by this formula. It's called an E-step. And so the algorithm would then try to\nuse the E-step to guess these z values. After that, it would then invoke\nanother that's called M-step. In this step we simply take advantage\nof the inferred z values and then just group words that are in\nthe same distribution like these from that ground including this as well. We can then normalize the count\nto estimate the probabilities or to revise our estimate of the parameters. So let me also illustrate\nthat we can group the words that are believed to have\ncome from zero sub d, and that's text, mining algorithm,\nfor example, and clustering. And we group them together to help us re-estimate the parameters\nthat we're interested in. So these will help us\nestimate these parameters. Note that before we just set\nthese parameter values randomly. But with this guess, we will have\nsomewhat improved estimate of this. Of course, we don't know exactly\nwhether it's zero or one. So we're not going to really\ndo the split in a hard way. But rather we're going to\ndo a softer split. And this is what happened here. So we're going to adjust the count by\nthe probability that would believe this word has been generated\nby using the theta sub d. And you can see this,\nwhere does this come from? Well, this has come from here, right? From the E-step. So the EM Algorithm would\niteratively improve uur initial estimate of parameters by using\nE-step first and then M-step. The E-step is to augment the data\nwith additional information, like z. And the M-step is to take advantage of the additional information\nto separate the data. To split the data accounts and\nthen collect the right data accounts to re-estimate our parameter. And then once we have a new generation of\nparameter, we're going to repeat this. We are going the E-step again. To improve our estimate\nof the hidden variables. And then that would lead to another\ngeneration of re-estimated parameters. For the word distribution\nthat we are interested in. Okay, so, as I said,\nthe bridge between the two is really the variable z, hidden variable,\nwhich indicates how likely this water is from the top water\ndistribution, theta sub p. So, this slide has a lot of content and\nyou may need to. Pause the reader to digest it. But this basically captures\nthe essence of EM Algorithm. Start with initial values that\nare often random themself. And then we invoke E-step followed\nby M-step to get an improved setting of parameters. And then we repeated this, so\nthis a Hill-Climbing algorithm that would gradually improve\nthe estimate of parameters. As I will explain later\nthere is some guarantee for reaching a local maximum of\nthe log-likelihood function. So lets take a look at the computation for\na specific case, so these formulas are the EM. Formulas that you see before, and\nyou can also see there are superscripts, here, like here, n,\nto indicate the generation of parameters. Like here for example we have n plus one. That means we have improved. From here to here we have an improvement. So in this setting we have assumed the two\nnumerals have equal probabilities and the background model is null. So what are the relevance\nof the statistics? Well these are the word counts. So assume we have just four words,\nand their counts are like this. And this is our background model that\nassigns high probabilities to common words like the. And in the first iteration,\nyou can picture what will happen. Well first we initialize all the values. So here, this probability that we're\ninterested in is normalized into a uniform distribution of all the words. And then the E-step would give us a guess\nof the distribution that has been used. That will generate each word. We can see we have different\nprobabilities for different words. Why? Well, that's because these words have\ndifferent probabilities in the background. So even though the two\ndistributions are equally likely. And then our initial audition say uniform\ndistribution because of the difference in the background of the distribution,\nwe have different guess the probability. So these words are believed to\nbe more likely from the topic. These on the other hand are less likely. Probably from background. So once we have these z values, we know in the M-step these probabilities\nwill be used to adjust the counts. So four must be multiplied by this 0.33 in order to get the allocated\naccounts toward the topic. And this is done by this multiplication. Note that if our guess says this\nis 100% If this is one point zero, then we just get the full count\nof this word for this topic. In general it's not going\nto be one point zero. So we're just going to get some percentage\nof this counts toward this topic. Then we simply normalize these counts to have a new generation\nof parameters estimate. So you can see, compare this with\nthe older one, which is here. So compare this with this one and\nwe'll see the probability is different. Not only that, we also see some words that are believed to have come from\nthe topic will have a higher probability. Like this one, text. And of course, this new generation of\nparameters would allow us to further adjust the inferred latent variable or\nhidden variable values. So we have a new generation of values, because of the E-step based on\nthe new generation of parameters. And these new inferred values\nof Zs will give us then another generation of the estimate\nof probabilities of the word. And so on and so forth so this is what\nwould actually happen when we compute these probabilities\nusing the EM Algorithm. As you can see in the last row\nwhere we show the log-likelihood, and the likelihood is increasing\nas we do the iteration. And note that these log-likelihood is\nnegative because the probability is between 0 and 1 when you take a logarithm,\nit becomes a negative value. Now what's also interesting is,\nyou'll note the last column. And these are the inverted word split. And these are the probabilities\nthat a word is believed to have come from one distribution, in this\ncase the topical distribution, all right. And you might wonder whether\nthis would be also useful. Because our main goal is to\nestimate these word distributions. So this is our primary goal. We hope to have a more discriminative\norder of distribution. But the last column is also bi-product. This also can actually be very useful. You can think about that. We want to use, is to for example is to estimate to what extent this\ndocument has covered background words. And this, when we add this up or take the average we will kind of know to\nwhat extent it has covered background versus content was that are not\nexplained well by the background. [MUSIC]",
 "06_3-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3.en.txt": "So, I just showed you that empirically\nthe likelihood will converge, but theoretically it can also\nbe proved that EM algorithm will converge to a local maximum. So here's just an illustration of what\nhappened and a detailed explanation. This required more knowledge about that, some of that inequalities,\nthat we haven't really covered yet. So here what you see is on the X\ndimension, we have a c0 value. This is a parameter that we have. On the y axis we see\nthe likelihood function. So this curve is the original\nlikelihood function, and this is the one that\nwe hope to maximize. And we hope to find a c0 value\nat this point to maximize this. But in the case of Mitsumoto we can\nnot easily find an analytic solution to the problem. So, we have to resolve\nthe numerical errors, and the EM algorithm is such an algorithm. It's a Hill-Climb algorithm. That would mean you start\nwith some random guess. Let's say you start from here,\nthat's your starting point. And then you try to improve\nthis by moving this to another point where you can\nhave a higher likelihood. So that's the ideal hill climbing. And in the EM algorithm, the way we\nachieve this is to do two things. First, we'll fix a lower\nbound of likelihood function. So this is the lower bound. See here. And once we fit the lower bound,\nwe can then maximize the lower bound. And of course, the reason why this works, is because the lower bound\nis much easier to optimize. So we know our current guess is here. And by maximizing the lower bound,\nwe'll move this point to the top. To here. Right? And we can then map to the original\nlikelihood function, we find this point. Because it's a lower bound, we are\nguaranteed to improve this guess, right? Because we improve our lower bound and\nthen the original likelihood curve which is above this lower bound\nwill definitely be improved as well. So we already know it's\nimproving the lower bound. So we definitely improve this\noriginal likelihood function, which is above this lower bound. So, in our example, the current guess is parameter value\ngiven by the current generation. And then the next guess is\nthe re-estimated parameter values. From this illustration you\ncan see the next guess is always better than the current guess. Unless it has reached the maximum,\nwhere it will be stuck there. So the two would be equal. So, the E-step is basically to compute this lower bound. We don't directly just compute\nthis likelihood function but we compute the length of\nthe variable values and these are basically a part\nof this lower bound. This helps determine the lower bound. The M-step on the other hand is\nto maximize the lower bound. It allows us to move\nparameters to a new point. And that's why EM algorithm is guaranteed\nto converge to a local maximum. Now, as you can imagine,\nwhen we have many local maxima, we also have to repeat the EM\nalgorithm multiple times. In order to figure out which one\nis the actual global maximum. And this actually in general is a\ndifficult problem in numeral optimization. So here for\nexample had we started from here, then we gradually just\nclimb up to this top. So, that's not optimal, and\nwe'd like to climb up all the way to here, so the only way to climb up to this gear\nis to start from somewhere here or here. So, in the EM algorithm, we generally\nwould have to start from different points or have some other way to determine\na good initial starting point. To summarize in this lecture we\nintroduced the EM algorithm. This is a general algorithm for computing\nmaximum maximum likelihood estimate of all kinds of models, so\nnot just for our simple model. And it's a hill-climbing algorithm, so it\ncan only converge to a local maximum and it will depend on initial points. The general idea is that we will have\ntwo steps to improve the estimate of. In the E-step we roughly [INAUDIBLE]\nhow many there are by predicting values of useful hidden variables that we\nwould use to simplify the estimation. In our case, this is the distribution\nthat has been used to generate the word. In the M-step then we would exploit\nsuch augmented data which would make it easier to estimate the distribution,\nto improve the estimate of parameters. Here improve is guaranteed in\nterms of the likelihood function. Note that it's not necessary that we\nwill have a stable convergence of parameter value even though the likelihood\nfunction is ensured to increase. There are some properties that have to\nbe satisfied in order for the parameters also to convert into some stable value. Now here data augmentation\nis done probabilistically. That means, we're not going to just say exactly\nwhat's the value of a hidden variable. But we're going to have a probability\ndistribution over the possible values of these hidden variables. So this causes a split of counts\nof events probabilistically. And in our case we'll split the word\ncounts between the two distributions. [MUSIC]",
 "07_3-7-probabilistic-latent-semantic-analysis-plsa-part-1.en.txt": "[SOUND]\nThis lecture is about probabilistic and\nlatent Semantic Analysis or PLSA. In this lecture we're going to introduce\nprobabilistic latent semantic analysis, often called PLSA. This is the most basic topic model,\nalso one of the most useful topic models. Now this kind of models\ncan in general be used to mine multiple topics from text documents. And PRSA is one of the most basic\ntopic models for doing this. So let's first examine this power\nin the e-mail for more detail. Here I show a sample article which is\na blog article about Hurricane Katrina. And I show some simple topics. For example government response,\nflood of the city of New Orleans. Donation and the background. You can see in the article we use\nwords from all these distributions. So we first for example see there's\na criticism of government response and this is followed by discussion of flooding\nof the city and donation et cetera. We also see background\nwords mixed with them. So the overall of topic analysis here\nis to try to decode these topics behind the text, to segment the topics,\nto figure out which words are from which distribution and to figure out first,\nwhat are these topics? How do we know there's a topic\nabout government response. There's a topic about a flood in the city. So these are the tasks\nat the top of the model. If we had discovered these\ntopics can color these words, as you see here,\nto separate the different topics. Then you can do a lot of things,\nsuch as summarization, or segmentation, of the topics,\nclustering of the sentences etc. So the formal definition of problem of\nmining multiple topics from text is shown here. And this is after a slide that you\nhave seen in an earlier lecture. So the input is a collection, the number\nof topics, and a vocabulary set, and of course the text data. And then the output is of two kinds. One is the topic category,\ncharacterization. Theta i's. Each theta i is a word distribution. And second, it's the topic coverage for\neach document. These are pi sub i j's. And they tell us which document it covers. Which topic to what extent. So we hope to generate these as output. Because there are many useful\napplications if we can do that. So the idea of PLSA is\nactually very similar to the two component mixture model\nthat we have already introduced. The only difference is that we\nare going to have more than two topics. Otherwise, it is essentially the same. So here I illustrate how we can generate\nthe text that has multiple topics and naturally in all cases of Probabilistic modelling would want\nto figure out the likelihood function. So we would also ask the question, what's the probability of observing\na word from such a mixture model? Now if you look at this picture and compare this with the picture\nthat we have seen earlier, you will see the only difference is\nthat we have added more topics here. So, before we have just one topic,\nbesides the background topic. But now we have more topics. Specifically, we have k topics now. All these are topics that we assume\nthat exist in the text data. So the consequence is that our switch for\nchoosing a topic is now a multiway switch. Before it's just a two way switch. We can think of it as flipping a coin. But now we have multiple ways. First we can flip a coin to decide\nwhether we're talk about the background. So it's the background lambda\nsub B versus non-background. 1 minus lambda sub B gives\nus the probability of actually choosing a non-background topic. After we have made this decision, we have to make another decision to\nchoose one of these K distributions. So there are K way switch here. And this is characterized by pi,\nand this sum to one. This is just the difference of designs. Which is a little bit more complicated. But once we decide which distribution to\nuse the rest is the same we are going to just generate a word by using one of\nthese distributions as shown here. So now lets look at the question\nabout the likelihood. So what's the probability of observing\na word from such a distribution? What do you think? Now we've seen this\nproblem many times now and if you can recall, it's generally a sum. Of all the different possibilities\nof generating a word. So let's first look at how the word can\nbe generated from the background mode. Well, the probability that the word is\ngenerated from the background model is lambda multiplied by the probability\nof the word from the background mode. Model, right. Two things must happen. First, we have to have\nchosen the background model, and that's the probability of lambda,\nof sub b. Then second, we must have actually\nobtained the word w from the background, and that's probability\nof w given theta sub b. Okay, so similarly, we can figure out the probability of\nobserving the word from another topic. Like the topic theta sub k. Now notice that here's\nthe product of three terms. And that's because of the choice\nof topic theta sub k, only happens if two things happen. One is we decide not to\ntalk about background. So, that's a probability\nof 1 minus lambda sub B. Second, we also have to actually choose\ntheta sub K among these K topics. So that's probability of theta sub K,\nor pi. And similarly, the probability of\ngenerating a word from the second. The topic and the first topic\nare like what you are seeing here. And so in the end the probability of observing\nthe word is just a sum of all these cases. And I have to stress again this is a very\nimportant formula to know because this is really key to understanding all the topic\nmodels and indeed a lot of mixture models. So make sure that you really\nunderstand the probability of w is indeed the sum of these terms. So, next,\nonce we have the likelihood function, we would be interested in\nknowing the parameters. All right, so to estimate the parameters. But firstly, let's put all these together to have the\ncomplete likelihood of function for PLSA. The first line shows the probability of a\nword as illustrated on the previous slide. And this is an important\nformula as I said. So let's take a closer look at this. This actually commands all\nthe important parameters. So first of all we see lambda sub b here. This represents a percentage\nof background words that we believe exist in the text data. And this can be a known value\nthat we set empirically. Second, we see the background\nlanguage model, and typically we also assume this is known. We can use a large collection of text, or use all the text that we have available\nto estimate the world of distribution. Now next in the next stop this formula. [COUGH] Excuse me. You see two interesting\nkind of parameters, those are the most important parameters. That we are. So one is pi's. And these are the coverage\nof a topic in the document. And the other is word distributions\nthat characterize all the topics. So the next line,\nthen is simply to plug this in to calculate\nthe probability of document. This is, again, of the familiar\nform where you have a sum and you have a count of\na word in the document. And then log of a probability. Now it's a little bit more\ncomplicated than the two component. Because now we have more components,\nso the sum involves more terms. And then this line is just\nthe likelihood for the whole collection. And it's very similar, just accounting for\nmore documents in the collection. So what are the unknown parameters? I already said that there are two kinds. One is coverage,\none is word distributions. Again, it's a useful exercise for\nyou to think about. Exactly how many\nparameters there are here. How many unknown parameters are there? Now, try and think out that question will help you\nunderstand the model in more detail. And will also allow you to understand\nwhat would be the output that we generate when use PLSA to analyze text data? And these are precisely\nthe unknown parameters. So after we have obtained\nthe likelihood function shown here, the next is to worry about\nthe parameter estimation. And we can do the usual think,\nmaximum likelihood estimator. So again, it's a constrained optimization\nproblem, like what we have seen before. Only that we have a collection of text and\nwe have more parameters to estimate. And we still have two constraints,\ntwo kinds of constraints. One is the word distributions. All the words must have probabilities\nthat's sum to one for one distribution. The other is the topic\ncoverage distribution and a document will have to cover\nprecisely these k topics so the probability of covering each\ntopic that would have to sum to 1. So at this point though it's basically\na well defined applied math problem, you just need to figure out\nthe solutions to optimization problem. There's a function with many variables. and we need to just figure\nout the patterns of these variables to make the function\nreach its maximum. >> [MUSIC]",
 "08_3-8-probabilistic-latent-semantic-analysis-plsa-part-2.en.txt": "[SOUND] We can compute this maximum estimate by using the EM algorithm. So in the e step,\nwe now have to introduce more hidden variables because we have more topics,\nso our hidden variable z now, which is a topic indicator can\ntake more than two values. So specifically will\ntake a k plus one values, with b in the noting the background. And once locate,\nto denote other k topics, right. So, now the e step, as you can\nrecall is your augmented data, and by predicting the values\nof the hidden variable. So we're going to predict for a word, whether the word has come from\none of these k plus one distributions. This equation allows us to\npredict the probability that the word w in document d is\ngenerated from topic zero sub j. And the bottom one is\nthe predicted probability that this word has been generated\nfrom the background. Note that we use document\nd here to index the word. Why? Because whether a word is\nfrom a particular topic actually depends on the document. Can you see why? Well, it's through the pi's. The pi's are tied to each document. Each document can have potentially\ndifferent pi's, right. The pi's will then affect our prediction. So, the pi's are here. And this depends on the document. And that might give a different guess for a word in different documents,\nand that's desirable. In both cases we are using\nthe Baye's Rule, as I explained, basically assessing the likelihood of generating\nword from each of this division and there's normalize. What about the m step? Well, we may recall the m step is we\ntake advantage of the inferred z values. To split the counts. And then collected the right counts\nto re-estimate the parameters. So in this case, we can re-estimate\nour coverage of probability. And this is re-estimated based on\ncollecting all the words in the document. And that's why we have the count\nof the word in document. And sum over all the words. And then we're going to look at to\nwhat extent this word belongs to the topic theta sub j. And this part is our guess from each step. This tells us how likely this word\nis actually from theta sub j. And when we multiply them together, we get the discounted count that's\nlocated for topic theta sub j. And when we normalize\nthis over all the topics, we get the distribution of all\nthe topics to indicate the coverage. And similarly, the bottom one is the\nestimated probability of word for a topic. And in this case we are using exact\nthe same count, you can see this is the same discounted account,\n] it tells us to what extend we should allocate this word [INAUDIBLE] but\nthen normalization is different. Because in this case we are interested\nin the word distribution, so we simply normalize this\nover all the words. This is different, in contrast here we\nnormalize the amount all the topics. It would be useful to take\na comparison between the two. This give us different distributions. And these tells us how to\nimprove the parameters. And as I just explained,\nin both the formula is we have a maximum estimate based on allocated\nword counts [INAUDIBLE]. Now this phenomena is actually general\nphenomena in all the EM algorithms. In the m-step, you general with\nthe computer expect an account of the event based on the e-step result,\nand then you just and then count to four,\nparticular normalize it, typically. So, in terms of computation\nof this EM algorithm, we can actually just keep accounting various\nevents and then normalize them. And when we thinking this way, we also have a more concise way\nof presenting the EM Algorithm. It actually helps us better\nunderstand the formulas. So I'm going to go over\nthis in some detail. So as a algorithm we first initialize\nall the unknown perimeters randomly, all right. So, in our case, we are interested in all\nof those coverage perimeters, pi's and awarded distributions [INAUDIBLE],\nand we just randomly normalize them. This is the initialization step and then\nwe will repeat until likelihood converges. Now how do we know whether\nlikelihood converges? We can do compute\nlikelihood at each step and compare the current likelihood\nwith the previous likelihood. If it doesn't change much and\nwe're going to say it stopped, right. So, in each step we're\ngoing to do e-step and m-step. In the e-step we're going to do\naugment the data by predicting the hidden variables. In this case,\nthe hidden variable, z sub d, w, indicates whether the word w in\nd is from a topic or background. And if it's from a topic, which topic. So if you look at the e-step formulas, essentially we're actually\nnormalizing these counts, sorry, these probabilities of observing\nthe word from each distribution. So you can see,\nbasically the prediction of word from topic zero sub j is\nbased on the probability of selecting that theta sub j as a word\ndistribution to generate the word. Multiply by the probability of observing\nthe word from that distribution. And I said it's proportional to this\nbecause in the implementation of EM algorithm you can keep counter for this quantity, and\nin the end it just normalizes it. So the normalization here\nis over all the topics and then you would get a probability. Now, in the m-step, we do the same,\nand we are going to collect these. Allocated account for each topic. And we split words among the topics. And then we're going to normalize\nthem in different ways to obtain the real estimate. So for example, we can normalize among all\nthe topics to get the re-estimate of pi, the coverage. Or we can re-normalize\nbased on all the words. And that would give us\na word distribution. So it's useful to think algorithm in this\nway because when implemented, you can just use variables, but keep track of\nthese quantities in each case. And then you just normalize these\nvariables to make them distribution. Now I did not put the constraint for\nthis one. And I intentionally leave\nthis as an exercise for you. And you can see,\nwhat's the normalizer for this one? It's of a slightly different form but\nit's essentially the same as the one that you have\nseen here in this one. So in general in the envisioning of EM\nalgorithms you will see you accumulate the counts, various counts and\nthen you normalize them. So to summarize,\nwe introduced the PLSA model. Which is a mixture model with k unigram\nlanguage models representing k topics. And we also added a pre-determined\nbackground language model to help discover discriminative topics, because this background language model\ncan help attract the common terms. And we select the maximum estimate\nthat we cant discover topical knowledge from text data. In this case PLSA allows us to discover\ntwo things, one is k worded distributions, each one representing a topic and the other is the proportion of\neach topic in each document. And such detailed characterization\nof coverage of topics in documents can enable a lot of photo analysis. For example, we can aggregate\nthe documents in the particular pan period to assess the coverage of\na particular topic in a time period. That would allow us to generate\nthe temporal chains of topics. We can also aggregate topics covered in\ndocuments associated with a particular author and then we can categorize\nthe topics written by this author, etc. And in addition to this, we can also\ncluster terms and cluster documents. In fact,\neach topic can be regarded as a cluster. So we already have the term clusters. In the higher probability,\nthe words can be regarded as belonging to one cluster\nrepresented by the topic. Similarly, documents can be\nclustered in the same way. We can assign a document\nto the topic cluster that's covered most in the document. So remember, pi's indicate to what extent\neach topic is covered in the document, we can assign the document to the topical\ncluster that has the highest pi. And in general there are many useful\napplications of this technique. [MUSIC]",
 "09_3-9-latent-dirichlet-allocation-lda-part-1.en.txt": "This lecture is about that Latent Dirichlet Allocation or LDA. In this lecture, we are going to continue talking about topic models. In particular, we are going to talk about some extension of PLSA, and one of them is LDA or Latent Dirichlet Allocation. So the plan for this lecture is to cover two things. One is to extend the PLSA with prior knowledge and that would allow us to have in some sense a user-controlled PLSA, so it doesn't apply to they just listen to data, but also would listen to our needs. The second is to extend the PLSA as a generative model, a fully generative model. This has led to the development of Latent Dirichlet Allocation or LDA. So first, let's talk about the PLSA with prior knowledge. Now in practice, when we apply PLSA to analyze text data, we might have additional knowledge that we want to inject to guide the analysis. The standard PLSA is going to blindly listen to the data by using maximum [inaudible]. We are going to just fit data as much as we can and get some insight about data. This is also very useful, but sometimes a user might have some expectations about which topics to analyze. For example, we might expect to see retrieval models as a topic in information retrieval or we also may be interesting in certain aspects, such as battery and memory, when looking at opinions about a laptop because the user is particularly interested in these aspects. A user may also have knowledge about topic coverage and we may know which topic is definitely not covering which document or is covering the document. For example, we might have seen those tags, topic tags assigned to documents. And those tags could be treated as topics. If we do that then a document account will be generated using topics corresponding to the tags already assigned to the document. If the document is not assigned a tag, we're going to say there is no way for using that topic to generate document. The document must be generated by using the topics corresponding to that assigned tags. So question is how can we incorporate such knowledge into PLSA. It turns out that there is a very elegant way of doing that and that would incorporate such knowledge as priors on the models. And you may recall in Bayesian inference, we use prior together with data to estimate parameters and this is precisely what would happen. So in this case, we can use maximum a posteriori estimate also called MAP estimate and the formula is given here. Basically, this is to maximize the posteriori distribution probability. And this is a combination of the likelihood of data and the prior. So what would happen is that we are going to have an estimate that listens to the data and also listens to our prior preferences. We can use this prior which is denoted as p of lambda to encode all kinds of preferences and the constraints. So for example, we can use this to encode the need of having precise background of the topic. Now this could be encoded as a prior because we can say the prior for the parameters is only a non-zero if the parameters contain one topic that is equivalent to the background language model. In other words, in other cases if it is not like that, we are going to say the prior says it is impossible. So the probability of that kind of models I think would be zero according to our prior. So now we can also for example use the prior to force particular choice of topic to have a probability of a certain number. For example, we can force document D to choose topic one with probability of one half or we can prevent topic from being used in generating document. So we can say the third topic should not be used in generating document D, we will set to the Pi zero for that topic. We can also use the prior to favor a set of parameters with topics that assign high probability to some particular words. In this case, we are not going to say it is impossible but we can just strongly favor certain kind of distributions and you will see example later. The MAP can be computed using a similar EM algorithm as we have used for the maximum likelihood estimate. With just some modifications, most of the parameters would reflect the prior preferences and in such an estimate if we use a special form of the prior code or conjugate the prior, then the functional form of the prior will be similar to the data. As a result, we can combine the two and the consequence is that you can basically convert the inference of the prior into the inference of having additional pseudo data because the two functional forms are the same and they can be combined. So the effect is as if we had more data and this is convenient for computation. It does not mean conjugate prior is the best way to define prior. So now let us look at the specific example. Suppose the user is particularly interested in battery life of a laptop and we are analyzing reviews. So the prior says that the distribution should contain one distribution that would assign high probability to battery and life. So we could say well there is distribution that is kind of concentrated on battery life and prior says that one of distributions should be very similar to this. Now if we use MAP estimate with conjugate prior, which is the original prior, the original distribution based on this preference, then the only difference in the EM is that when we re-estimate words distributions, we are going to add additional counts to reflect our prior. So here you can see the pseudo counts are defined based on the probability of words in a prior. So battery obviously would have high pseudo counts and similarly life would have also high pseudo counts. All the other words would have zero pseudo counts because their probability is zero in the prior and we see this is also controlled by a parameter mu and we are going to add a mu much by the probability of W given prior distribution to the connected accounts when we re-estimate this word distribution. So this is the only step that is changed and the change is happening here. And before we just connect the counts of words that we believe have been generated from this topic but now we force this distribution to give more probabilities to these words by adding them to the pseudo counts. So in fact we artificially inflated their probabilities. To make this distribution, we also need to add this many pseudo counts to the denominator. This is total sum of all the pseudo counts we have added for all the words This would make this a gamma distribution. Now this is intuitively very reasonable way of modifying EM and theoretically speaking, this works and it computes the MAP estimate. It is useful to think about the two specific extreme cases of mu. Now, [inaudible] the picture. Think about what would happen if we set mu to zero. Well that essentially to remove this prior. So mu in some sense indicates our strengths on prior. Now what would happen if we set mu to positive infinity. Well that is to say that prior is so strong that we are not going to listen to the data at all. So in the end, you see in this case we are going to make one of the distributions fixed to the prior. You see why? When mu is infinitive, we basically let this one dominate. In fact we are going to set this one to precise this distribution. So in this case, it is this distribution. And that is why we said the background language model is in fact a way to impose the prior because it would force one distribution to be exactly the same as what we give, that is background distribution. So in this case, we can even force the distribution to entirely focus on battery life. But of course this would not work well because it cannot attract other words. It would affect the accuracy of counting topics about battery life. So in practice, mu is set somewhere in between of course. So this is one way to impose a prior. We can also impose some other constraints. For example, we can set any parameters that will constantly include zero as needed. For example, we may want to set one of the Pi's to zero and this would mean we do not allow that topic to participate in generating that document. And this is only reasonable of course when we have prior analogy that strongly suggests this.",
 "10_3-10-latent-dirichlet-allocation-lda-part-2.en.txt": "[SOUND] So\nnow let's talk about the exchanging of PLSA to of LDA and to motivate that, we need to talk about some\ndeficiencies of PLSA. First, it's not really a generative model\nbecause we can compute the probability of a new document. You can see why, and that's because the\npis are needed to generate the document, but the pis are tied to the document\nthat we have in the training data. So we can't compute the pis for\nfuture document. And there's some heuristic workaround,\nthough. Secondly, it has many parameters, and I've\nasked you to compute how many parameters exactly there are in PLSA, and\nyou will see there are many parameters. That means that model is very complex. And this also means that there\nare many local maxima and it's prone to overfitting. And that means it's very hard to\nalso find a good local maximum. And that we are representing\nglobal maximum. And in terms of explaining future data,\nwe might find that it will overfit the training data\nbecause of the complexity of the model. The model is so flexible to fit precisely\nwhat the training data looks like. And then it doesn't allow us to generalize\nthe model for using other data. This however is not a necessary problem\nfor text mining because here we're often only interested in hitting\nthe training documents that we have. We are not always interested in modern\nfuture data, but in other cases, or if we would care about the generality,\nwe would worry about this overfitting. So LDA is proposing to improve that,\nand basically to make PLSA a generative model by imposing\na Dirichlet prior on the model parameters. Dirichlet is just a special distribution\nthat we can use to specify product. So in this sense, LDA is just\na Bayesian version of PLSA, and the parameters are now\nmuch more regularized. You will see there are many\nfew parameters and you can achieve the same goal as PLSA for\ntext mining. It means it can compute the top coverage\nand topic word distributions as in PLSA. However, there's no. Why are the parameters for\nPLSA here are much fewer, there are fewer parameters and\nin order to compute a topic coverage and word distributions,\nwe again face a problem of influence of these variables because\nthey are not parameters of the model. So the influence part again\nface the local maximum problem. So essentially they are doing something\nvery similar, but theoretically, LDA is a more elegant way of looking\nat the top and bottom problem. So let's see how we can\ngeneralize the PLSA to LDA or a standard PLSA to have LDA. Now a full treatment of LDA is\nbeyond the scope of this course and we just don't have time to go in\ndepth on that talking about that. But here, I just want to give you\na brief idea about what's extending and what it enables, all right. So this is the picture of LDA. Now, I remove the background\nof model just for simplicity. Now, in this model, all these\nparameters are free to change and we do not impose any prior. So these word distributions are now\nrepresented as theta vectors. So these are word distributions, so here. And the other set of parameters are pis. And we would present it as a vector also. And this is more convenient\nto introduce LDA. And we have one vector for each document. And in this case, in theta,\nwe have one vector for each topic. Now, the difference between LDA and PLSA is that in LDA, we're not going\nto allow them to free the chain. Instead, we're going to force them to\nbe drawn from another distribution. So more specifically, they will be drawn from two Dirichlet\ndistributions respectively, but the Dirichlet distribution is\na distribution over vectors. So it gives us a probability of\nfour particular choice of a vector. Take, for example, pis, right. So this Dirichlet distribution tells\nus which vectors of pi is more likely. And this distribution in itself is\ncontrolled by another vector of parameters of alphas. Depending on the alphas, we can\ncharacterize the distribution in different ways but with full certain choices of\npis to be more likely than others. For example, you might favor the choice of a relatively\nuniform distribution of all the topics. Or you might favor generating\na skewed coverage of topics, and this is controlled by alpha. And similarly here, the topic or\nword distributions are drawn from another Dirichlet\ndistribution with beta parameters. And note that here,\nalpha has k parameters, corresponding to our inference on\nthe k values of pis for our document. Whereas here, beta has n values corresponding to\ncontrolling the m words in our vocabulary. Now once we impose this price, then\nthe generation process will be different. And we start with joined pis from the Dirichlet distribution and\nthis pi will tell us these probabilities. And then, we're going to use the pi\nto further choose which topic to use, and this is of course\nvery similar to the PLSA model. And similar here, we're not going\nto have these distributions free. Instead, we're going to draw one\nfrom the Dirichlet distribution. And then from this,\nthen we're going to further sample a word. And the rest is very similar to the. The likelihood function now\nis more complicated for LDA. But there's a close connection between the\nlikelihood function of LDA and the PLSA. So I'm going to illustrate\nthe difference here. So in the top, you see PLSA likelihood function\nthat you have already seen before. It's copied from previous slide. Only that I dropped the background for\nsimplicity. So in the LDA formulas you\nsee very similar things. You see the first equation\nis essentially the same. And this is the probability of generating\na word from multiple word distributions. And this formula is a sum of all\nthe possibilities of generating a word. Inside a sum is a product of\nthe probability of choosing a topic multiplied by the probability of\nobserving the word from that topic. So this is a very important formula,\nas I've stressed multiple times. And this is actually the core\nassumption in all the topic models. And you might see other topic models\nthat are extensions of LDA or PLSA. And they all rely on this. So it's very important to understand this. And this gives us a probability of\ngetting a word from a mixture model. Now, next in the probability of\na document, we see there is a PLSA component in the LDA formula, but the LDA\nformula will add a sum integral here. And that's to account for\nthe fact that the pis are not fixed. So they are drawn from the original\ndistribution, and that's shown here. That's why we have to take an integral,\nto consider all the possible pis that we could possibly draw from\nthis Dirichlet distribution. And similarly in the likelihood for\nthe whole collection, we also see further components added,\nanother integral here. Right? So basically in the area we're just\nadding this integrals to account for the uncertainties and we added of course\nthe Dirichlet distributions to cover the choice of this parameters,\npis, and theta. So this is a likelihood function for LDA. Now, next to this, let's talk about the\nparameter as estimation and inferences. Now the parameters can be now estimated\nusing exactly the same approach maximum likelihood estimate for LDA. Now you might think about how many\nparameters are there in LDA versus PLSA. You'll see there're a fewer parameters\nin LDA because in this case the only parameters are alphas and the betas. So we can use the maximum likelihood\nestimator to compute that. Of course, it's more complicated because\nthe form of likelihood function is more complicated. But what's also important\nis notice that now these parameters that we are interested\nin name and topics, and the coverage are no\nlonger parameters in LDA. In this case we have to\nuse basic inference or posterior inference to compute them based\non the parameters of alpha and the beta. Unfortunately, this\ncomputation is intractable. So we generally have to resort\nto approximate inference. And there are many methods available for\nthat and I'm sure you will see them when you use different tool kits\nfor LDA, or when you read papers about these different extensions of LDA. Now here we, of course, can't give\nin-depth instruction to that, but just know that they are computed based in inference by using\nthe parameters alphas and betas. But our math [INAUDIBLE],\nactually, in the end, in some of our math list,\nit's very similar to PLSA. And, especially when we use\nalgorithm called class assembly, then the algorithm looks very\nsimilar to the Algorithm. So in the end,\nthey are doing something very similar. So to summarize our discussion\nof properties of topic models, these models provide\na general principle or way of mining and analyzing topics\nin text with many applications. The best basic task setup is\nto take test data as input and we're going to output the k topics. Each topic is characterized\nby word distribution. And we're going to also output proportions\nof these topics covered in each document. And PLSA is the basic topic model, and\nin fact the most basic of the topic model. And this is often adequate for\nmost applications. That's why we spend a lot of\ntime to explain PLSA in detail. Now LDA improves over\nPLSA by imposing priors. This has led to theoretically\nmore appealing models. However, in practice, LDA and\nPLSA tend to give similar performance, so in practice PLSA and LDA would work\nequally well for most of the tasks. Now here are some suggested readings if\nyou want to know more about the topic. First is a nice review of\nprobabilistic topic models. The second has a discussion about how\nto automatically label a topic model. Now I've shown you some distributions and\nthey intuitively suggest a topic. But what exactly is a topic? Can we use phrases to label the topic? To make it the more easy to understand and this paper is about the techniques for\ndoing that. The third one is empirical comparison\nof LDA and the PLSA for various tasks. The conclusion is that they\ntend to perform similarly. [MUSIC]",
 "01_4-1-text-clustering-motivation.en.txt": "[SOUND]\nThis lecture is the first one\nabout the text clustering. In this lecture, we are going to\ntalk about the text clustering. This is a very important technique for\ndoing topic mining and analysis. In particular, in this lecture we're going to start with\nsome basic questions about the clustering. And that is, what is text clustering and\nwhy we are interested in text clustering. In the following lectures, we are going\nto talk about how to do text clustering. How to evaluate the clustering results? So what is text clustering? Well, clustering actually is\na very general technique for data mining as you might have\nlearned in some other courses. The idea is to discover natural\nstructures in the data. In another words,\nwe want to group similar objects together. In our case, these objects are of course,\ntext objects. For example, they can be documents,\nterms, passages, sentences, or websites, and then I'll\ngo group similar text objects together. So let's see an example, well, here\nyou don't really see text objects, but I just used some shapes to denote\nobjects that can be grouped together. Now if I ask you, what are some natural\nstructures or natural groups where you, if you look at it and you might agree that\nwe can group these objects based on chips, or their locations on this\ntwo dimensional space. So we got the three clusters in this case. And they may not be so\nmuch this agreement about these three clusters but it really depends\non the perspective to look at the objects. Maybe some of you have also seen\nthing in a different way, so we might get different clusters. And you'll see another example\nabout this ambiguity more clearly. But the main point of here is, the problem\nis actually not so well defined. And the problem lies in\nhow to define similarity. And what do you mean by similar objects? Now this problem has to be clearly defined in order to have\na well defined clustering problem. And the problem is in general that any two objects can be similar\ndepending on how you look at them. So for example, this will kept\nthe two words like car and horse. So are the two words similar? Well, it depends on how if\nwe look at the physical properties of car and\nhorse they are very different but if you look at them functionally,\na car and a horse, can both be transportation tools. So in that sense, they may be similar. So as we can see, it really depends on\nour perspective, to look at the objects. And so it ought to make\nthe clustering problem well defined. A user must define the perspective for\nassessing similarity. And we call this perspective\nthe clustering bias. And when you define a clustering problem,\nit's important to specify your perspective for\nsimilarity or for defining the similarity that will be\nused to group similar objects. because otherwise,\nthe similarity is not well defined and one can have different\nways to group objects. So let's look at the example here. You are seeing some objects,\nor some shapes, that are very similar to what you\nhave seen on the first slide, but if I ask you to group these objects,\nagain, you might feel there is more than here\nthan on the previous slide. For example, you might think, well, we can steer a group by ships, so that would\ngive us cluster that looks like this. However, you might also feel that, well, maybe the objects can be\ngrouped based on their sizes. So that would give us a different way\nto cluster the data if we look at the size and\nlook at the similarity in size. So as you can see clearly here,\ndepending on the perspective, we'll get different clustering result. So that also clearly tells us that in\norder to evaluate the clustering without, we must use perspective. Without perspective, it's very hard to\ndefine what is the best clustering result. So there are many examples\nof text clustering setup. And so for example, we can cluster\ndocuments in the whole text collection. So in this case,\ndocuments are the units to be clustered. We may be able to cluster terms. In this case, terms are objects. And a cluster of terms can be used to\ndefine concept, or theme, or a topic. In fact, there's a topic models that you\nhave seen some previous lectures can give you cluster of terms in some\nsense if you take terms with high probabilities from word distribution. Another example is just to cluster any\ntext segments, for example, passages, sentences, or any segments that you can\nextract the former larger text objects. For example, we might extract the order\ntext segments about the topic, let's say, by using a topic model. Now once we've got those\ntext objects then we can cluster the segments that we've got to discover interesting clusters that\nmight also ripple in the subtopics. So this is a case of combining text\nclustering with some other techniques. And in general you will\nsee a lot of text mining can be accurate combined in\na flexible way to achieve the goal of doing more sophisticated\nmining and analysis of text data. We can also cluster fairly\nlarge text objects and by that, I just mean text objects may\ncontain a lot of documents. So for example, we might cluster websites. Each website is actually\ncompose of multiple documents. Similarly, we can also cluster articles\nwritten by the same author, for example. So we can trigger all the articles\npublished by also as one unit for clustering. In this way, we might group authors\ntogether based on whether they're published papers or similar. For the more text clusters will be for\nthe cluster to generate a hierarchy. That's because we can in general cluster\nany text object at different levels. So more generally why is\ntext clustering interesting? Well, it's because it's a very\nuseful technique for text mining, particularly exploratory text analysis. And so a typical scenario is that\nyou were getting a lot of text data, let's say all the email messages\nfrom customers in some time period, all the literature articles, etc. And then you hope to get a sense\nabout what are the overall content of the connection, so for example,\nyou might be interested in getting a sense about major topics,\nor what are some typical or representative documents\nin the connection. And clustering to help\nus achieve this goal. We sometimes also want to link\na similar text objects together. And these objects might be\nduplicated content, for example. And in that case, such a technique can help us remove\nredundancy and remove duplicate documents. Sometimes they are about\nthe same topic and by linking them together we can have\nmore complete coverage of a topic. We may also used text clustering to create\na structure on the text data and sometimes we can create a hierarchy of structures\nand this is very useful for problems. We may also use text clustering to induce\nadditional features to represent text data when we cluster documents together,\nwe can treat each cluster as a feature. And then we can say when\na document is in this cluster and then the feature value would be one. And if a document is not in this cluster,\nthen the feature value is zero. And this helps provide additional\ndiscrimination that might be used for text classification as\nwe will discuss later. So there are, in general,\nmany applications of text clustering. And I just thought of\ntwo very specific ones. One is to cluster search results, for example, [INAUDIBLE] search engine\ncan cluster such results so that the user can see overall structure\nof the results of return the fall query. And when the query's ambiguous this\nis particularly useful because the clusters likely represent\ndifferent senses of ambiguous word. Another application is to understand the\nmajor complaints from customers based on their emails, right. So in this case,\nwe can cluster email messages and then find in the major\nclusters from there, we can understand what are the major\ncomplaints about them. [MUSIC]",
 "02_4-2-text-clustering-generative-probabilistic-models-part-1.en.txt": "[SOUND]\nThis lecture is about generating probabilistic\nmodels for text clustering. In this lecture, we're going to continue\ndiscussing text clustering, and we're going to introduce\ngenerating probabilistic models as a way to do text clustering. So this is the overall plan for\ncovering text clustering. In the previous lecture, we have talked\nabout what is text clustering and why text clustering is interesting. In this lecture, we're going to talk\nabout how to do text clustering. In general, as you see on this slide,\nthere are two kinds of approaches. One is generating probabilistic models,\nwhich is the topic of this lecture. And later, we'll also discuss\nsimilarity-based approaches. So to talk about generating models for\ntext clustering, it would be useful to revisit\nthe topic mining problem using topic models,\nbecause the two problems are very similar. This is a slide that you have seen\nearlier in the lecture on topic model. Here we show that we have input\nof a text collection C and a number of topics k, and vocabulary V. And we hope to generate\nas output two things. One is a set of topics\ndenoted by Theta i's, each is awarded distribution and\nthe other is pi i j. These are the probabilities that\neach document covers each topic. So this is a topic coverage and\nit's also visualized here on this slide. You can see that this is what we\ncan get by using a topic model. Now, the main difference between this and\nthe text clustering problem is that here, a document is assumed to\npossibly cover multiple topics. And indeed, in general,\na document will be covering more than one topic with\nnonzero probabilities. In text clustering, however,\nwe only allow a document to cover one topic,\nif we assume one topic is a cluster. So that means if we change the problem\ndefinition just slightly by assuming that each document that can only\nbe generated by using precisely one topic. Then we'll have a definition of\nthe clustering problem as you'll hear. So here the output is changed so that we no longer have the detailed\ncoverage distributions pi i j. But instead, we're going to have\na cluster assignment decisions, Ci. And Ci is a decision for the document i. And C sub i is going to take a value\nfrom 1 through k to indicate one of the k clusters. And basically tells us that\nd i is in which cluster. As illustrated here, we no longer have\nmultiple topics covered in each document. It is precisely one topic. Although which topic is still uncertain. There is also a connection with the problem of mining one topic\nthat we discussed earlier. So here again,\nit's a slide that you have seen before and here we hope to estimate a topic model or distribution based on\nprecisely one document. And that's when we assume that this\ndocument, it covers precisely one topic. But we can also consider some\nvariations of the problem. For example,\nwe can consider there are N documents, each covers a different topic, so\nthat's N documents, and topics. Of course, in this case,\nthese documents are independent, and these topics are also independent. But, we can further allow these\ndocuments with share topics, and we can also assume that we are going\nto assume there are fewer topics than the number of documents, so\nthese documents must share some topics. And if we have N documents\nthat share k topics, then we'll again have precisely\nthe document clustering problem. So because of these connections,\nnaturally we can think about how to use a probabilistically generative model\nto solve the problem of text clustering. So the question now is what generative\nmodel can be used to do clustering? As in all cases of designing a generative\nmodel, we hope the generative model would adopt the output that we hope to generate\nor the structure that we hope to model. So in this case,\nthis is a clustering structure, the topics and\neach document that covers one topic. And we hope to embed such\npreferences in the generative model. But, if you think about the main\ndifference between this problem and the topic model that we\ntalked about earlier. And you will see a main requirement\nis how can we force every document to be generated\nfrom precisely one topic, instead of k topics,\nas in the topic model? So let's revisit the topic\nmodel again in more detail. So this is a detailed view of\na two component mixture model. When we have k components,\nit looks similar. So here we see that when\nwe generate a document, we generate each word independent. And when we generate each word, but first\nmake a choice between these distributions. We decide to use one of\nthem with probability. So p of theta 1 is the probability of\nchoosing the distribution on the top. Now we first make this decision regarding\nwhich distribution should be used to generate the word. And then we're going to use this\ndistribution to sample a word. Now note that in such a generative model, the decision on which distribution\nto use for each word is independent. So that means, for example, the here could have generated from\nthe second distribution, theta 2 whereas text is more likely generated\nfrom the first one on the top. That means the words in the document that\ncould have been generated in general from multiple distributions. Now this is not what we want,\nas we said, for text clustering, for document clustering, where we hoped this document will be\ngenerated from precisely one topic. So now that means we\nneed to modify the model. But how? Well, let's first think about why this\nmodel cannot be used for clustering. And as I just said, the reason is because it has allowed multiple topics to\ncontribute a word to the document. And that causes confusion because\nwe're not going to know which cluster this document is from. And it's, more importantly\nit's violating our assumption about the partitioning of\ndocuments in the clusters. If we really have one topic to correspond\nit to one cluster of documents, then we would have a document that we\ngenerate from precisely one topic. That means all the words in the document must have been generated from\nprecisely one distribution. And this is not true for\nsuch a topic model that we're seeing here. And that's why this cannot be used for\nclustering because it did not ensure that only one distribution has been used\nto generate all the words in one document. So if you realize this problem, then we can naturally design alternative\nmixture model for doing clustering. So this is what you're seeing here. And we again have to make a decision\nregarding which distribution to use to generate this document because\nthe document could potentially be generated from any of the k\nword distributions that we have. But this time, once we have made\na decision to choose one of the topics, we're going to stay with this regime to\ngenerate all the words in the document. And that means, once we have made\na choice of the distribution in generating the first word,\nwe're going to go stay with this distribution in generating all of\nthe other words in the document. So, in other words,\nwe only make the choice once for, basically, we make the decision once for\nthis document and this state was just to\ngenerate all the words. Similarly if I had choosing the second\ndistribution, theta sub 2 here, you can see which state was this one. And then generate\nthe entire document of d. Now, if you compare this\npicture with the previous one, you will see the decision of\nusing a particular distribution is made just once for this document,\nin the case of document clustering. But in the case of topic model, we have to make as many decisions as\nthe number of words in the document. Because for each word, we can make\na potentially different decision. And that's the key difference\nbetween the two models. But this is obviously\nalso a mixed model so we can just group them together\nas one box to show that this is the model that will give us\na probability of the document. Now, inside of this model, there is also this switch of\nchoosing a different distribution. And we don't observe that so\nthat's a mixture model. And of course a main problem in\ndocument clustering is to infer which distribution has been used\nto generate a document and that would allow us to recover\nthe cluster identity of a document. So it will be useful to think about\nthe difference from the topic model as I have also mentioned multiple times. And there are mainly two differences, one is the choice of using that particular distribution is\nmade just once for document clustering. Whereas in the topic model, it's made\nit multiple times for different words. The second is that word distribution,\nhere, is going to be used to regenerate\nall the words for a document. But, in the case of one\ndistribution doesn't have to generate all the words in the document. Multiple distribution could have been used\nto generate the words in the document. Let's also think about a special case, when one of the probability of choosing\na particular distribution is equal to 1. Now that just means we\nhave no uncertainty now. We just stick with one\nparticular distribution. Now in that case, clearly, we will\nsee this is no longer mixture model, because there's no uncertainty here and we can just use precisely one of the\ndistributions for generating a document. And we're going back to\nthe case of estimating one order distribution based on one document. So that's a connection\nthat we discussed earlier. Now you can see it more clearly. So as in all cases of using\na generative model to solve a problem, we first look at data and\nthen think about how to design the model. But once we design the model, the next step is to write\ndown the likelihood function. And after that we're going to look at\nthe how to estimate the parameters. So in this case,\nwhat's the likelihood function? It's going to be very similar to what\nyou have seen before in topic models but it will be also different. Now if you still recall what\nthe likelihood function looks like in then you will realize that in general, the\nprobability of observing a data point from mixture model is going to be a sum of all\nthe possibilities of generating the data. In this case, so it's going to\nbe a sum over these k topics, because every one can be\nuser generated document. And then inside is the sum you can still\nrecall what the formula looks like, and it's going to be the product\nof two probabilities. One is the probability of choosing the\ndistribution, the other is the probability of observing a particular\ndatapoint from that distribution. So if you map this kind of\nformula to our problem here, you will see the probability\nof observing a document d is basically a sum in this\ncase of two different distributions because we have a very\nsimplified situation of just two clusters. And so in this case,\nyou can see it's a sum of two cases. In each case,\nit's indeed the probability of choosing the distribution either theta 1 or\ntheta 2. And then, the probability is\nmultiplied by the probability of observing this document from\nthis particular distribution. And if you further expanded\nthis probability of observing the whole document, we see that it's\na product of observing each word X sub i. And here we made the assumption that\neach word is generated independently, so the probability of the whole\ndocument is just a product of the probability of each\nword in the document. So this form should be very\nsimilar to the topic model. But it's also useful to think about\nthe difference and for that purpose, I am also copying the probability of\ntopic model of these two components here. So here you can see the formula looks very\nsimilar or in many ways, they are similar. But there is also some difference. And in particular,\nthe difference is on the top. You see for the mixture model for document\nclustering, we first take a product, and then take a sum. And that's corresponding\nto our assumption of first make a choice of\nchoosing one distribution and then stay with the distribution,\nit'll generate all the words. And that's why we have\nthe product inside the sum. The sum corresponds to the choice. Now, in topic model, we see that\nthe sum is actually inside the product. And that's because we generated\neach word independently. And that's why we have\nthe product outside, but when we generate each word we\nhave to make a decision regarding which distribution we use so\nwe have a sum there for each word. But in general,\nthese are all mixture models and we can estimate these models\nby using the Algorithm, as we will discuss more later. [MUSIC]",
 "03_4-3-text-clustering-generative-probabilistic-models-part-2.en.txt": "[SOUND]\nThis lecture is a continuing discussion of Generative\nProbabilistic Models for text clustering. In this lecture, we are going to continue\ntalking about the text clustering, particularly, the Generative\nProbabilistic Models. So this is a slide that you have seen\nearlier where we have written down the likelihood function for\na document with two distributions, being a two component\nmixed model for document clustering. Now in this lecture, we're going to\ngeneralize this to include the k clusters. Now if you look at the formula and think\nabout the question, how to generalize it, you'll realize that all we need is to add\nmore terms, like what you have seen here. So you can just add more thetas and\nthe probabilities of thetas and the probabilities of\ngenerating d from those thetas. So this is precisely what we\nare going to use and this is the general presentation of the mixture\nmodel for document clustering. So as more cases would follow these\nsteps in using a generating model first, think about our data. And so in this case our data\nis a collection of documents, end documents denoted by d sub i, and then we talk about the other models,\nthink of other modelling. In this case, we design a mixture\nof k unigram language models. It's a little bit different from the topic\nmodel, but we have similar parameters. We have a set of theta i's that\ndenote that our distributions corresponding to the k\nunigram language models. We have p of each theta i as\na probability of selecting each of the k distributions\nwe generate the document. Now note that although our goal\nis to find the clusters and we actually have used a more general\nnotion of a probability of each cluster and this as you will see later, will allow us to assign\na document to the cluster that has the highest probability of\nbeing able to generate the document. So as a result,\nwe can also recover some other interesting properties, as you will see later. So the model basically would make\nthe following assumption about the generation of a document. We first choose a theta i according\nto probability of theta i, and then generate all the words in\nthe document using this distribution. Note that it's important that we use this distribution all\nthe words in the document. This is very different from topic model. So the likelihood function would\nbe like what you are seeing here. So you can take a look\nat the formula here, we have used the different notation here in the second line of this equation. You are going to see now\nthe notation has been changed to use unique word in the vocabulary,\nin the product instead of particular\nposition in the document. So from X subject to W,\nis a change of notation and this change allows us to show\nthe estimation formulas more easily. And you have seen this change also\nin the topic model presentation, but it's basically still just a product of\nthe probabilities of all the words. And so with the likelihood function, now we can\ntalk about how to do parameter estimation. Here we can simply use\nthe maximum likelihood estimator. So that's just a standard\nway of doing things. So all should be familiar to you now. It's just a different model. So after we have estimated parameters, how can we then allocate\nclusters to the documents? Well, let's take a look at\nthe this situation more closely. So we just repeated the parameters\nhere for this mixture model. Now if you think about what we can\nget by estimating such a model, we can actually get more information than\nwhat we need for doing clustering, right? So theta i for example,\nrepresents the content of cluster i, this is actually a by-product, it can help\nus summarize what the cluster is about. If you look at the top\nterms in this cluster or in this word distribution and they will\ntell us what the cluster is about. p of theta i can be interpreted as\nindicating the size of cluster because it tells us how likely the cluster would\nbe used to generate the document. The more likely a cluster is\nused to generate a document, we can assume the larger\nthe cluster size is. Note that unlike in PLSA and this probability of theta\ni is not dependent on d. Now you may recall that the topic\nyou chose at each document actually depends on d. That means each document can have\na potentially different choice of topics, but here we have a generic choice\nprobability for all the documents. But of course, even a particular document\nthat we still have to infer which topic is more likely to\ngenerate the document. So in that sense, we can still have a document\ndependent probability of clusters. So now let's look at the key problem\nof assigning documents to clusters or assigning clusters to documents. So that's the computer c sub d here and\nthis will take one of the values in the range of one through k to indicate\nwhich cluster should be assigned to d. Now first you might think about\na way to use likelihood on it and that is to assign d to the cluster\ncorresponding to the topic of theta i, that most likely has\nbeen used to generate d. So that means we're going to choose\none of those distributions that gives d the highest probability. In other words, we see which distribution has the content\nthat matches our d at the [INAUDIBLE]. Intuitively that makes sense,\nhowever, this approach does not consider the size of clusters,\nwhich is also a available to us and so a better way is to use\nthe likelihood together with the prior, in this case the prior is p of theta i. And together, that is, we're going to\nuse the base formula to compute the posterior probability of theta,\ngiven d. And if we choose theta .based\non this posterior probability, we would have the following formula that\nyou see here on the bottom of this slide. And in this case, we're going to choose\nthe theta that has a large P of theta i, that means a large cluster and\nalso a high probability of generating d. So we're going to favor\na cluster that's large and also consistent with the document. And that intuitively makes\nsense because the chance of a document being a large cluster is\ngenerally higher than in a small cluster. So this means once we can estimate\nthe parameters of the model, then we can easily solve\nthe problem of document clustering. So next, we'll have to discuss how to actually compute\nthe estimate of the model. [MUSIC]",
 "04_4-4-text-clustering-generative-probabilistic-models-part-3.en.txt": "[SOUND]\nThis lecture is a continuing discussion of generative\nprobabilistic models for tax classroom. In this lecture, we're going to\ndo a finishing discussion of generative probabilistic models for\ntext crossing. So this is a slide that you have seen\nbefore and here, we show how we define the mixture model for text crossing and\nwhat the likelihood function looks like. And we can also compute\nthe maximum likelihood estimate, to estimate the parameters. In this lecture, we're going to do talk\nmore about how exactly we're going to compute the maximum likelihood estimate. As in most cases the Algorithm can be used\nto solve this problem for mixture models. So here's the detail of this Algorithm for\ndocument clustering. Now, if you have understood\nhow Algorithm works for topic models like TRSA, and\nI think here it would be very similar. And we just need to adapt a little\nbit to this new mixture model. So as you may recall Algorithm starts with\ninitialization of all the parameters. So this is the same as what\nhappened before for topic models. And then we're going to repeat\nuntil the likelihood converges and in each step we'll do E step and M step. In M step, we're going to infer which distribution\nhas been used to generate each document. So I have to introduce\na hidden variable Zd for each document and this variable could take\na value from the range of 1 through k, representing k different distributions. More specifically basically, we're going\nto apply base rules to infer which distribution is more likely to\nhave generated this document, or computing the posterior probability of\nthe distribution given the document. And we know it's proportional\nto the probability of selecting this\ndistribution p of Z the i. And the probability of generating this\nwhole document from the distribution which is the product of the probabilities of\nworld for this document as you see here. Now, as you all clear this use for\nkind of remember, the normalizer or\nthe constraint on this probability. So in this case, we know\nthe constraint on this probability in E-Step is that all the probabilities\nof Z equals i must sum to 1. Because the documented must have been\ngenerated from precisely one of these k topics. So the probability of being generated\nfrom each of them should sum to 1. And if you know this constraint, then\nyou can easily compute this distribution as long as you know what\nit is proportional to. So once you compute this product that\nyou see here, then you simply normalize these probabilities,\nto make them sum to 1 over all the topics. So that's E-Step, after E-Step we\nwant to know which distribution is more likely to have generated this\ndocument d, and which is unlikely. And then in M-Step we're going to\nre-estimate all the parameters based on the in further z values or in further\nknowledge about which distribution has been used to generate which document. So the re-estimation involves two kinds\nof parameters 1 is p of theta and this is the probability of selecting\na particular distribution. Before we observe anything, we don't have any knowledge about\nwhich cluster is more likely. But after we have observed\nthat these documents, then we can crack the evidence to\ninfer which cluster is more likely. And so this is proportional to the sum of the probability of Z\nsub d j is equal to i. And so this gives us all\nthe evidence about using topic i, theta i to generate a document. Pull them together and again,\nwe normalize them into probabilities. So this is for key of theta sub i. Now the other kind of parameters\nare the probabilities of words in each distribution, in each cluster. And this is very similar\nto the case piz and here we just report the kinds\nof words that are in documents that are inferred\nto have been generated from a particular topic of theta i here. This would allows to then\nestimate how many words have actually been generated from theta i. And then we'll normalize again these\naccounts in the probabilities so that the probabilities on all\nthe words would sum to up. Note that it's very important to\nunderstand these constraints as they are precisely the normalizing\nin all these formulas. And it's also important to know\nthat the distribution is over what? For example, the probability of\ntheta is over all the k topics, that's why these k\nprobabilities will sum to 1. Whereas the probability of a word given\ntheta is a probability distribution over all the words. So there are many probabilities and\nthey have to sum to 1. So now, let's take a look at\na simple example of two clusters. I've two clusters,\nI've assumed some initialized values for the two distributions. And let's assume we randomly\ninitialize two probability of selecting each cluster as 0.5,\nso equally likely. And then let's consider one\ndocument that you have seen here. There are two occurrences of text and\ntwo occurrences of mining. So there are four words together and\nmedical and health did not occur in this document. So let's think about the hidden variable. Now for each document then we\nmuch use a hidden variable. And before in piz,\nwe used one hidden variable for each work because that's\nthe output from one mixture model. So in our case the output\nfrom the mixture model or the observation from mixture\nmodel is a document, not a word. So now we have one hidden variable\nattached to the document. Now that hidden variable must tell us\nwhich distribution has been used to generate the document. So it's going to take two values,\none and two to indicate the two topics. So now how do we infer which\ndistribution has been used generally d? Well it's been used base rule,\nso it looks like this. In order for the first topic\ntheta 1 to generate a document, two things must happen. First, theta sub 1 must\nhave been selected. So it's given by p of theta 1. Second, it must have also be generating\nthe four words in the document. Namely, two occurrences of text and\ntwo occurrences of sub mining. And that's why you see the numerator\nhas the product of the probability of selecting theta 1 and the probability of\ngenerating the document from theta 1. So the denominator is just the sum of\ntwo possibilities of generality in this document. And you can plug in the numerical\nvalues to verify indeed in this case, the document is more likely\nto be generated from theta 1, much more likely than from theta 2. So once we have this probability, we can easily compute the probability\nof Z equals 2, given this document. How? Well, we can use the constraint. That's going to be 1 minus 100 over 101. So now it's important that you note\nthat in such a computation there is a potential problem of underflow. And that is because if you look at the\noriginal numerator and the denominator, it involves the competition of\na product of many small probabilities. Imagine if a document has many words and it's going to be a very small value here\nthat can cause the problem of underflow. So to solve the problem,\nwe can use a normalize. So here you see that we take\na average of all these two math solutions to compute average at\nthe screen called a theta bar. And this average distribution\nwould be comparable to each of these distributions in terms\nof the quantities or the magnitude. So we can then divide the numerator and the denominator both by this normalizer. So basically this normalizes\nthe probability of generating this document by using this\naverage word distribution. So you can see the normalizer is here. And since we have used exactly the same\nnormalizer for the numerator and the denominator. The whole value of this expression is not\nchanged but by doing this normalization you can see we can make the numerators and\nthe denominators more manageable in that the overall value is not\ngoing to be very small for each. And thus we can avoid\nthe underflow problem. In some other times we sometimes\nalso use logarithm of the product to convert this into a sum\nof log of probabilities. This can help preserve precision as well,\nbut in this case we cannot use\nalgorithm to solve the problem. Because there is a sum in the denominator,\nbut this kind of normalizes can be\neffective for solving this problem. So it's a technique that's sometimes\nuseful in other situations in other situations as well. Now let's look at the M-Step. So from the E-Step we can see our estimate\nof which distribution is more likely to have generated a document at d. And you can see d1's more like\ngot it from the first topic, where is d2 is more like\nfrom second topic, etc. Now, let's think about what we\nneed to compute in M-step well basically we need to\nre-estimate all the parameters. First, look at p of theta 1 and\np of theta 2. How do we estimate that? Intuitively you can just pool together\nthese z, the probabilities from E-step. So if all of these documents say,\nwell they're more likely from theta 1, then we intuitively would give\na higher probability to theta 1. In this case,\nwe can just take an average of these probabilities that you see here and\nwe've obtain a 0.6 for theta 1. So 01 is more likely and then theta 2. So you can see probability of\n02 would be natural in 0.4. What about these word of probabilities? Well we do the same, and\nintuition is the same. So we're going to see, in order to estimate the probabilities\nof words in theta 1, we're going to look at which documents\nhave been generated from theta 1. And we're going to pull together the words\nin those documents and normalize them. So this is basically what I just said. More specifically, we're going to do for\nexample, use all the kinds of text in these documents for estimating\nthe probability of text given theta 1. But we're not going to use their\nraw count or total accounts. Instead, we can do that discount them\nby the probabilities that each document is likely been generated from theta 1. So these gives us some\nfractional accounts. And then these accounts\nwould be then normalized in order to get the probability. Now, how do we normalize them? Well these probability of\nthese words must assign to 1. So to summarize our discussion of\ngenerative models for clustering. Well we show that a slight variation\nof topic model can be used for clustering documents. And this also shows the power\nof generating models in general. By changing the generation assumption and\nchanging the model slightly we can achieve different goals, and we can capture\ndifferent patterns and types of data. So in this case, each cluster is\nrepresented by unigram language model word distribution and\nthat is similar to topic model. So here you can see the word distribution\nactually generates a term cluster as a by-product. A document that is generated by first\nchoosing a unigram language model. And then generating all the words\nin the document are using just a single language model. And this is very different from again\ncopy model where we can generate the words in the document by using\nmultiple unigram language models. And then the estimated model parameters\nare given both topic characterization of each cluster and the probabilistic assignment of\neach document into a cluster. And this probabilistic assignment\nsometimes is useful for some applications. But if we want to achieve\nharder clusters mainly to partition documents into\ndisjointed clusters. Then we can just force a document into\nthe cluster corresponding to the words distribution that's most likely\nto have generated the document. We've also shown that the Algorithm can\nbe used to compute the maximum likelihood estimate. And in this case, we need to use a special number addition technique\nto avoid underflow. [MUSIC]",
 "05_4-5-text-clustering-similarity-based-approaches.en.txt": "[MUSIC] This lecture is about the similarity-based\napproaches to text clustering. In this lecture we're going to to continue\nthe discussion of how to do a text clustering. In particular, we're going to to\ncover different kinds of approaches than generative models, and\nthat is similarity-based approaches. So the general idea of similarity-based\nclustering is to explicitly specify a similarity function to measure\nthe similarity between two text objects. Now this is in contrast with a generative model where we\nimplicitly define the clustering bias by using a particular object to\nfunction like a [INAUDIBLE] function. The whole process is driven by\noptimizing the [INAUDIBLE,] but here we explicitly provide a view\nof what we think are similar. And this is often very useful\nbecause then it allows us to inject any particular view of similarity\ninto the clustering program. So once we have a similarity function,\nwe can then aim at optimally partitioning, to partitioning the data into clusters or\ninto different groups. And try to maximize\nthe inter-group similarity and minimize the inter-group similarity. That is to ensure the objects that are put\ninto the same group to be similar, but the objects that are put into\ndifferent groups to be not similar. And these are the general\ngoals of clustering, and there is often a trade off\nbetween achieving both goals. Now there are many different methods for\ndoing similarity based clustering, and in general I think we can distinguish\nthe two strategies at high level. One is to progressively construct\nthe hierarchy of clusters, and so this often leads to\nhierarchical clustering. And we can further distinguish it\ntwo ways, to construct a hierarchy depending on whether we started with\nthe collection to divide the connection. Or started with individual objectives and\ngradually group them together, so one is bottom-up that can\nbe called agglomerative. Well we gradually group a similar\nobjects into larger and larger clusters. Until we group everything together,\nthe other is top-down or divisive, in this case we gradually partition the whole data\nset into smaller and smaller clusters. The other general strategy is to start\nwith the initial tentative clustering and then iteratively improve it. And this often leads for\na flat clustering, one example is k-Means, so as I just said, there are many\ndifferent clustering methods available. And a full coverage of all\nthe clustering methods would be beyond the scope of this course. But here we are going to talk about the\ntwo representative methods, in some detail one is Hierarchical Agglomerative\nClustering or HAC, the other is k-Means. So first of it we'll get the agglomerative\nhierarchical clustering, in this case, we're given a similarity function to\nmeasure similarity between two objects. And then we can gradually group similar\nobjects together in a bottom-up fashion to form larger and larger groups. And they always form a hierarchy, and then we can stop when some\nstopping criterion is met. It could be either some number\nof clusters has been achieved or the threshold for\nsimilarity has been reached. There are different variations here, and they mainly differ in the ways\nto compute a group similarity. Based on the individual\nobjects similarity, so let's illustrate how again induced\na structure based on just similarity. So start with all the text objects and we can then measure\nthe similarity between them. Of course based on the provided\nsimilarity function, and then we can see which pair\nhas the highest similarity. And then just group them together, and then we're going to see which\npair is the next one to group. Maybe these two now have\nthe highest similarity, and then we're going to gradually\ngroup them together. And then every time we're going\nto pick the highest similarity, the similarity of pairs to group. This will give us a binary tree\neventually to group everything together. Now, depending on our applications, we can use the whole hierarchy as\na structure for browsing, for example. Or we can choose a cutoff,\nlet's say cut here to get four clusters, or we can use a threshold to cut. Or we can cut at this high level\nto get just two clusters, so this is a general idea, now if you think\nabout how to implement this algorithm. You'll realize that we have\neverything specified except for how to compute group similarity. We are only given the similarity\nfunction of two objects, but as we group groups together, we also need to\nassess the similarity between two groups. There are also different ways to do that\nand there are the three popular methods. Single-link, complete-link,\nand average-link, so given two groups and\nthe single-link algorithm. Is going to define the group similarity\nas the similarity of the closest pair of the two groups. Complete-link defines\nthe similarity of the two groups as the similarity of\nthe farthest system pair. Average-link defines the similarity\nas average of similarity of all the pairs of the two groups. So it's much easier to understand\nthe methods by illustrating them, so here are two groups, g1 and\ng2 with some objects in each group. And we know how to compute\nthe similarity between two objects, but the question now is, how can we compute\nthe similarity between the two groups? And then we can in general base this\non the similarities of the objects in the two groups. So, in terms of single-link and we're just looking at the closest pair so\nin this case, these two paired objects will defined\nthe similarities of the two groups. As long as they are very close,\nwe're going to say the two groups are very close so\nit is an optimistic view of similarity. The complete link on the other hand\nwere in some sense pessimistic, and by taking the similarity of the two farthest\npair as the similarity for the two groups. So we are going to make sure that if the two groups are having\na high similarity. Then every pair of the two groups, or the objects in the two groups will have,\nwill be ensured to have high similarity. Now average link is in between, so\nit takes the average of all these pairs. Now these different ways of computing\ngroup similarities will lead to different clustering algorithms. And they would in general\ngive different results, so it's useful to take a look at their\ndifferences and to make a comparison. First, single-link can be expected to generally the loose clusters, the reason\nis because as long as two objects are very similar in the two groups,\nit will bring the two groups together. If you think about this as similar\nto having parties with people, then it just means two groups of\npeople would be partying together. As long as in each group\nthere is a person that is well connected with the other group. So the two leaders of the two\ngroups can have a good relationship with each other and then\nthey will bring together the two groups. In this case, the cluster is loose,\nbecause there's no guarantee that other members of the two groups\nare actually very close to each other. Sometimes they may be very far away,\nnow in this case it's also based on individual decisions, so\nit could be sensitive to outliers. The complete-link is in\nthe opposite situation, where we can expect\nthe clusters to be tight. And it's also based on individual decision\nso it can be sensitive to outliers. Again to continue the analogy\nto having a party of people, then complete-link would mean\nwhen two groups come together. They want to ensure that even the people that are unlikely to talk\nto each other would be comfortable. Always talking to each other, so\nensure the whole class to be coherent. The average link of clusters in\nbetween and as group decision, so it's going to be insensitive to outliers,\nnow in practice which one is the best. Well, this would depend on the application\nand sometimes you need a lose clusters. And aggressively cluster objects\ntogether that maybe single-link is good. But other times you might\nneed a tight clusters and a complete-link might be better. But in general, you have to\nempirically evaluate these methods for your application to know\nwhich one is better. Now, next let's look at another example of\na method for similarity-based clustering. In this case,\nwhich is called k-Means clustering, we will represent each text\nobject as a term vector. And then assume a similarity function\ndefined on two objects, now we're going to start with some tentative clustering\nresults by just selecting k randomly. selected vectors as\ncentroids of k clusters and treat them as centers as if they\nrepresent, they each represent a cluster. So this gives us the initial\ntentative cluster, then we're going to\niteratively improve it. And the process goes like this, and\nonce we have these centroids Decide. We're going to assign\na vector to the cluster whose centroid is closest to the current vector. So basically we're going to measure\nthe distance between this vector, and each of the centroids, and\nsee which one is the closest to this one. And then just put this\nobject into that cluster, this is to have tentative assignment\nof objects into clusters. And we're going to\npartition all the objects into k clusters based on our\ntentative clustering and centroids. Then we can do re-compute\nthe centroid based on the locate the object in each cluster. And this is to adjust the centroid, and then we can repeat this process until\nthe similarity-based objective function. In this case, it's within cluster\nsum of squares converges, and theoretically we can show that. This process actually is going to minimize\nthe within cluster sum of squares where define object and function. Given k clusters, so it can be also shown, this process will converge\nto a local minimum. I think about this process for a moment,\nit might remind you the Algorithm for mixture model. Indeed this algorithm is very\nsimilar to the Algorithm for the mixture model for clustering. More specifically we also\ninitialize these parameters in the Algorithm so\nthe random initialization is similar. And then in the Algorithm,\nyou may recall that, we're going to repeat E-step and M-step\nto improve our parameter estimation. In this case, we're going to\nimprove the clustering result iteratively by also doing two steps. And in fact that the two steps are very\nsimilar to Algorithm, in that when we locate the vector into one of the clusters\nbased on our tentative clustering. It's very similar to inferring\nthe distribution that has been used to generate the document, the mixture model. So it is essentially similar to E-step, so what's the difference,\nwell the difference is here. We don't make a probabilistic\nallocation as in the case of E-step, the brother will make a choice. We're going to make a call if this,\nthere upon this closest to cluster two, then we're going to say\nyou are in cluster two. So there's no choice, and we're not going to say, you assume\nthe set is belonging to a cluster two. And so\nwe're not going to have a probability, but we're just going to put one object\ninto precisely one cluster. In the E-step however, we do a probability\nlocation, so we split in counts. And we're not going to say\nexactly which distribution has been used to generate a data point. Now next,\nwe're going to adjust the centroid, and this is very similar to M-step where\nwe re-estimate the parameters. That's when we'll have a better\nestimate of the parameter, so here we'll have a better clustering\nresult by adjusting the centroid. And note that centroid is based on\nthe average of the vectors in the cluster. So this is also similar to the M-step\nwhere we do counts,pull together counts and then normalize them. The difference of course is also because\nof the difference in the E-step, and we're not going to consider\nprobabilities when we count the points. In this case, k-Means we're going to all make count of\nthe objects as allocated to this cluster. And this is only a subset of data points,\nbut in the Algorithm, we in principle consider all the data\npoints based on probabilistic allocations. But in nature they are very similar and that's why it's also maximizing\nwell defined object of functions. And it's guaranteed to\nconvert local minimum, so to summarize our discussion\nof clustering methods. We first discussed model based approaches,\nmainly the mixture model. Here we use the implicit similarity\nfunction to define the clustering bias. There is no explicit define similarity\nfunction, the model defines clustering bias and the clustering structure\nis built into a generative model. That's why we can use\npotentially a different model to recover different structure. Complex generative models can be used to\ndiscover complex clustering structures. We do not talk about in full,\nbut we can easily design, generate a model to generate\na hierarchical clusters. We can also use prior to further\ncustomize the clustering algorithm to for example control the topic of one\ncluster or multiple clusters. However one disadvantage of this\napproach is that there is no easy way to directly control the similarity measure. Sometimes we want to that,\nbut it's very hard to inject such a special definition\nof similarity into such a model. We also talked about\nsimilarity-based approaches, these approaches are more flexible to\nactually specify similarity functions. But one major disadvantage is that their objective function\nis not always very clear. The k-Means algorithm has clearly\ndefined the objective function, but it's also very similar to\na model based approach. The hierarchical clustering algorithm on the other hand is harder to\nspecify the objective function. So it's not clear what\nexactly is being optimized, both approaches can\ngenerate term clusters. And document clusters, and\nterm clusters can be in general, generated by representing each\nterm with some text content. For example, take the context of each\nterm as a representation of each term, as we have done in semantic\nrelation learning. And then we can certainly cluster terms,\nbased on actual text [INAUDIBLE]. Of course, term clusters can be generated\nby using generative models as well, as we've seen. [MUSIC]",
 "06_4-6-text-clustering-evaluation.en.txt": "[MUSIC] This lecture is about\nevaluation of text clustering. So far we have talked about multiple\nways of doing text clustering but how do we know which\nmethod works the best? So this has to do with evaluation. Now to talk about evaluation one must go back to the clustering bias that\nwe introduced at the beginning. Because two objects can be similar\ndepending on how you look at them, we must clearly specify\nthe perspective of similarity. Without that, the problem of\nclustering is not well defined. So this perspective is also\nvery important for evaluation. If you look at this slide, and you can see we have two different\nways to cluster these shapes, and if you ask a question, which one is\nthe best, or which one is better? You actually see, there's no way to answer\nthis question without knowing whether we'd like to cluster based on shapes,\nor cluster based on sizes. And that's precisely why\nthe perspective on clustering bias is crucial for evaluation. In general,\nwe can evaluate text clusters in two ways, one is direct evaluation, and\nthe other indirect evaluation. So in direct evaluation, we want to answer the following questions,\nhow close are the system-generated clusters to the ideal clusters\nthat are generated by humans? So the closeness here can be assessed from multiple perspectives and\nthat will help us characterize the quality of cluster result in multiple angles,\nand this is sometimes desirable. Now we also want to quantify\nthe closeness because this would allow us to easily compare different measures\nbased on their performance figures. And finally, you can see, in this case,\nwe essentially inject the clustering bias by using humans, basically humans\nwould bring in the the need or desire to clustering bias. Now, how do we do that exactly? Well, the general procedure\nwould look like this. Given a test set which consists\nof a lot of text objects, we can have humans to create\nthe ideal clustering result, that is, we're going to ask humans to partition\nthe objects to create the gold standard. And they will use their judgments based\non the need of a particular application to generate what they think are the best\nclustering results, and this would be then used to compare with the system generated\nclusters from the same test set. And ideally, we want the system results\nto be the same as the human generated results, but in general,\nthey are not going to be the same. So we would like to then quantify the\nsimilarity between the system-generated clusters and the gold standard clusters. And this similarity can also be measure\nfrom multiple perspectives and this will give us various meshes to quantitatively\nevaluate a cluster, a clustering result. And some of the commonly used measures\ninclude the purity, which measures whether a cluster has a similar object from\nthe same cluster, in the gold standard. And normalized mutual information\nis a commonly used measure which basically measures\nbased on the identity of cluster of object in the system generally. How well can you predict the cluster\nof the object in the gold standard or vice versa? And mutual information captures, the\ncorrelation between these cluster labels and normalized mutual information is often used for quantifying the similarity for this evaluation purpose,\nF measure is another possible measure. Now again a thorough discussion\nof this evaluation and these evaluation issues would be\nbeyond the scope of this course. I've suggested some reading in\nthe end that you can take a look at to know more about that. So here I just want to\ndiscuss some high level ideas that would allow you to think about how\nto do evaluation in your applications. The second way to evaluate text\nclusters is to do indirect evaluation. So in this case the question to answer is,\nhow useful are the clustering results for the intended applications? Now this of course is application\nspecific question, so usefulness is going to depend\non specific applications. In this case, the clustering bias is\nimposed by the independent application as well, so what counts as a best cluster result\nwould be dependent on the application. Now procedure wise we also would create\na test set with text objects for the intended application to quantify\nthe performance of the system. In this case,\nwhat we care about is the contribution of clustering to some application so we often\nhave a baseline system to compare with. This could be the current system for\ndoing something, and then you hope to add\na clustering to improve it, or the baseline system could be using\na different clustering method. And then what you are trying\nto experiment with, and you hope to have better\nidea of word clustering. So in any case you have a baseline system\nwork with, and then you add a clustering algorithm to the baseline system\nto produce a clustering system. And then we have to compare the\nperformance of your clustering system and the baseline system in terms\nof the performance measure for that particular application. So in this case we call it indirect\nevaluation of clusters because there's no explicit assessment of\nthe quality of clusters, but rather it's to assess the contribution\nof clusters to a particular application. So, to summarize text clustering, it's a very useful unsupervised\ngeneral text mining technique, and it's particularly useful for obtaining\nan overall picture of the text content. And this is often needed\nto explore text data, and this is often the first step when\nyou deal with a lot of text data. The second application or\nsecond kind of applications is through discover interesting clustering\nstructures in text data and these structures can be very meaningful. There are many approaches that can\nbe used to form text clustering and we discussed model based approaches and\nsome narrative based approaches. In general, strong clusters tend to\nshow up no matter what method is used. Also the effectiveness of a method\nhighly depends on whether the desired clustering bias is captured appropriately,\nand this can be done either through using the right generating model, the model\ndesign appropriate for the clustering, or the right similarity function\nexpressly define the bias. Deciding the optimal number of customers\nis a very difficult problem for order cluster methods, and that's\nbecause it's unsupervised algorithm, and there's no training there how to guide\nus to select the best number of clusters. Now sometimes you may see some methods\nthat can automatically determine the number of clusters, but\nin general that has some implied application of clustering bias there and\nthat's just not specified. Without clearly defining a clustering\nbias, it's just impossible to say the optimal number of cluster is what,\nso this important to keep in mind. And I should also say sometimes we\ncan also use application to determine the number of clusters, for example,\nif you're clustering search results, then obviously you don't want\nto generate the 100 clusters, so the number can be dictated\nby the interface design. In other situations, we might be\nable to use the fitness to data to assess whether we've got a good number\nof clusters to explain our data well. And to do that,\nyou can vary the number of clusters and watch how well you can fit the data. In general when you add a more components\nto a mixed model you should fit the data better because you, you don't, you can always set the probability\nof using the new component as zero. So you can't in general fit the data\nworse than before, but the question is as you add more components would you be\nable to significantly improve the fitness of the data and that can be used to\ndetermine the right number of clusters. And finally evaluation\nof clustering results, this kind can be done both directly and\nindirectly, and we often would like to do both in order to get a good sense\nabout how well our method works. So here's some suggested reading and\nthis is particularly useful to better understand how the matches\nare calculated and clustering in general [MUSIC]",
 "07_4-7-text-categorization-motivation.en.txt": "[SOUND] This lecture is about text categorization. In this lecture, we're going to\ntalk about text categorization. This is a very important technique for\ntext data mining and analytics. It is relevant to discovery\nof various different kinds of knowledge as shown here. First, it's related to topic mining and\nanalysis. And, that's because it has to do with analyzing text to data based\non some predefined topics. Secondly, it's also related to\nopinion mining and sentiment analysis, which has to do with discovery knowledge\nabout the observer, the human sensor. Because we can categorize the authors,\nfor example, based on the content of the articles\nthat they have written, right? We can, in general,\ncategorize the observer based on the content that they produce. Finally, it's also related\nto text-based prediction. Because, we can often use text\ncategorization techniques to predict some variables in the real world that\nare only remotely related to text data. And so, this is a very important\ntechnique for text to data mining. This is the overall plan for\ncovering the topic. First, we're going to talk about\nwhat is text categorization and why we're interested in\ndoing that in this lecture? And now, we're going to talk about\nhow to do text categorization for how to evaluate\nthe categorization results. So, the problem of text\ncategorization is defined as follows. We're given a set of predefined categories\npossibly forming a hierarchy or so. And often,\nalso a set of training examples or training set of labeled text\nobjects which means the text objects have already been\nenabled with known categories. And then, the task is to classify\nany text object into one or more of these predefined categories. So, the picture on this\nslide shows what happens. When we do text categorization, we have a lot of text objects to be\nprocessed by a categorization system and the system will, in general,\nassign categories through these documents. As shown on the right and\nthe categorization results, and we often assume the availability\nof training examples and these are the documents that\nare tag with known categories. And these examples are very important for helping the system to learn\npatterns in different categories. And, this would further help\nthe system then know how to recognize the categories of new text\nobjects that it has not seen. So, here are some specific\nexamples of text categorization. And in fact, there are many examples,\nhere are just a few. So first, text objects can vary,\nso we can categorize a document, or a passage, or a sentence,\nor collections of text. As in the case of clustering, the units\nto be analyzed can vary a lot, so this creates a lot of possibilities. Secondly, categories can also vary. Allocate in general,\nthere's two major kinds of categories. One is internal categories. These are categories that\ncategorize content of text object. For example, topic categories or\nsentiment categories and they generally have to do with\nthe content of the text objects throughout the categorization\nof the content. The other kind is external categories\nthat can characterize an entity associated with the text object. For example, authors are entities associated\nwith the content that they produce. And so, we can use their content in\ndetermining which author has written, which part, for example, and\nthat's called author attribution. Or, we can have any\nother mininal categories associate with text data\nas long as there is minimal connection between the entity and\ntext data. For example, we might collect a lot\nof reviews about a restaurant or a lot of reviews about a product,\nand then, this text data can help us infer\nproperties of a product or a restaurant. In that case, we can treat this\nas a categorization problem. We can categorize restaurants or categorize products based on\ntheir corresponding reviews. So, this is an example for\nexternal category. Here are some specific\nexamples of the applications. News categorization is very\ncommon as being started a lot. News agencies would like\nto assign predefined categories to categorize\nnews generated everyday. And, these virtual article\ncategorizations are not important aspect. For example, in the biomedical domain,\nthere's MeSH annotations. MeSH stands for Medical Subject Heading,\nand this is ontology of terms, characterize content of\nliterature articles in detail. Another example of application is spam\nemail detection or filtering, right? So, we often have a spam filter to help us distinguish spams\nfrom legitimate emails and this is clearly a binary\nclassification problem. Sentiment categorization of\nproduct reviews or tweets is yet another kind of applications where we\ncan categorize, comparing to positive or negative or positive and\nnegative or neutral. So, you can have send them to categories,\nassign the two text content. Another application is automatic\nemail routing or sorting, so, you might want to automatically sort your\nemails into different folders and that's one application of text categorization\nwhere each folder is a category. The results are another important kind\nof applications of routing emails to the right person to handle,\nso, in helpdesk, email messaging is generally routed\nto a particular person to handle. Different people tend to handle\ndifferent kinds of requests. And in many cases, a person would manually\nassign the messages to the right people. But, if you can imagine,\nyou can't be able to automatically text categorization system\nto help routing request. And, this is a class file, the incoming\nrequest in the one of the categories where each category actually corresponds\nto a person to handle the request. And finally, author attribution, as I just\nmentioned, is yet another application, and it's another example of using text\nto actually infer properties of some other entities. And, there are also many variants\nof the problem formulation. And so, first, we have the simplest case,\nwhich is a binary categorization, where there are only two categories. And, there are many examples like that,\ninformation retrieval or search engine. Applications with one distinguishing\nrelevant documents from non-relevant documents for a particular query. Spam filtering just distinguishing spams\nfrom non-spams, so, also two categories. Sometimes, classifications of\nopinions can be in two categories, positive and a negative. A more general case would be K-category\ncategorization and there are also many applications like that,\nthere could be more than two categories. So, topic categorization is often\nsuch an example where you can have multiple topics. Email routing would be another example\nwhen you may have multiple folders or if you route the email to\nthe right person to handle it, then there are multiple\npeople to classify. So, in all these cases, there are more\nthan two kinds of categories. Another variation is to have\nhierarchical categorization where categories form a hierarchy. Again, topical hierarchy is very common. Yet another variation is\njoint categorization. That's when you have multiple\ncategorization tasks that are related and then you hope to kind of\njoin the categorization. Further leverage the dependency of\nthese tasks to improve accuracy for each individual task. Among all these binary categorizations\nis most fundamental and part of it also is because it's simple and\nprobably it's because it can actually be used to perform\nall the other categorization tasks. For example, a K-category\ncategorization task can be actually performed by using binary categorization. Basically, we can look at\neach category separately and then the binary categorization problem\nis whether object is in this category or not, meaning in other categories. And, the hierarchical categorization\ncan also be done by progressively doing flat categorization at each level. So, we have, first, we categorize\nall the objects into, let's say, a small number of high-level categories, and inside each category, we have further\ncategorized to sub-categories, etc. So, why is text categorization important? Well, I already showed that you,\nseveral applications but, in general, there are several reasons. One is text categorization helps enrich\ntext representation and that's to achieve more understanding of text data that's\nall it was useful for text analysis. So, now with categorization text can\nbe represented in multiple levels. The keyword conditions that's often\nused for a lot text processing tasks. But we can now also add categories and\nthey provide two levels of transition. Semantic categories assigned can also\nbe directly or indirectly useful for application. So, for example, semantic categories\ncould be already very useful or other attribution might\nbe directly useful. Another example is when semantic\ncategories can facilitate aggregation of text content and this is another case\nof applications of text categorization. For example, if we want to know\nthe overall opinions about a product, we could first categorize the opinions\nin each individual view as positive or negative and then, that\nwould allow us to easy to aggregate all the sentiment, and it would tell us about the 70% of the views are positive and\n30% are negative, etc. So, without doing categorization, it will be much harder to aggregate\nsuch opinions to provide a concise way of coding text in some sense\nbased on all of the vocabulary. And, sometimes you may see in some\napplications, text with categorizations called a text coded,\nencoded with some control of vocabulary. The second kind of reasons is to use text categorization to infer\nproperties of entities, and text categories allows\nus to infer the properties of such entities that\nare associate with text data. So, this means we can\nuse text categorization to discover knowledge about the world. In general, as long as we can associate\nthe entity with text of data, we can always the text of data to help\ncategorize the corresponding entities. So, it's used for single information network that will\nconnect the other entities with text data. The obvious entities that can be\ndirectly connected are authors. But, you can also imagine the author's\naffiliations or the author's age and other things can be actually\nconnected to text data indirectly. Once we have made the connection, then we\ncan make a prediction about those values. So, this is a general way to allow\nus to use text mining through, so the text categorization to discover\nknowledge about the world. Very useful, especially in big text\ndata analytics where we are often just using text data as extra sets\nof data extracted from humans to infer certain decision factors\noften together with non-textual data. Specifically with text, for example, we can also think of examples of\ninferring properties of entities. For example, discovery of\nnon-native speakers of a language. And, this can be done by categorizing\nthe content of speakers. Another example is to predict the party\naffiliation of a politician based on the political speech. And, this is again an example\nof using text data to infer some knowledge about the real world. In nature,\nthe problems are all the same, and that's as we defined and\nit's a text categorization problem. [MUSIC]",
 "08_4-8-text-categorization-methods.en.txt": "This lecture is about the methods for\ntext categorization. So in this lecture we're going to discuss\nhow to do text for categorization. First, there're many methods for\ntext categorization. In such a method the idea is\nto determine the category based on some rules that\nwe design carefully to reflect the domain knowledge about\nthe category prediction problem. So for example, if you want to do topic\ncategorization for news articles you can say well, if the news article mentions\nword like a game and sports three times. That we're going to say it's about sports\nthings like that and this would allow us to deterministically decide which category\na document that should be put into. Now such a strategy would work well\nif the following conditions hold. First the categories must be very well\ndefined and this allows the person to clearly decide the category\nbased on some clear rules. A certainly the categories as half to be easy to distinguished at\nthe based on a surface features in text. So that means some official\nfeatures like keywords or punctuations or whatever,\nyou can easily identify in text to data. For example, if there is some\nspecial vocabulary that is known to only occur in a particular category. And that would be most effective because\nwe can easily use such a vocabulary or padding of such a vocabulary\nto recognize this category. Now we also should have\nsufficient knowledge for designing these words, and so if that's\nthe case then such a can be effective. And so it does have a in some domains and\nsometimes. However, in general, there are several\nproblems with this approach. First off, because it's label intensive\nit requires a lot of manual work. Obviously, we can't do this for\nall kinds of categorization problems. We have to do it from scratch for\na different problem. problem because given the rules,\nwhat they need. So it doesn't scale up well. Secondly, it cannot handle\nuncertainty in rules, often the rules Aren't 100% reliable. Take for example looking at\noccurrences of words in texts and trying to decide the topic. It's actually very hard to\nhave 100% correct rule. So for example you can say well,\nif it has game, sports, basketball Then for\nsure it's about sports. But one can also imagine some types of\narticles that mention these cures, but may not be exactly about sports or\nonly marginally touching sports. The main topic could be another topic,\na different topic than sports. So that's one disadvantage\nof this approach. And then finally,\nthe rules maybe inconsistent and this would lead to robustness. More specifically, and sometimes, the\nresults of categorization may be different that depending on which\nrule to be applied. So as in that case that you\nare facing uncertainty. And you will also have to decide\nan order of applying the rules, or combination of results\nthat are contradictory. So all these are problems\nwith this approach. And it turns out that both\nproblems can be solved or alleviated by using machine learning. So these machine learning\nmethods are more automatic. But, I still put automatic\nin quotation marks because they are not really completely automatic\ncause it still require many work. More specifically we have to use\na human experts to help in two ways. First the human experts must annotate\ndata cells was category labels. And would tell the computer which\ndocuments should receive which categories. And this is called training data. And then secondly, the human experts also\nneed to provide a set of features to represent each text object. That can potentially provide\na clue about the category. So, we need to provide some basic\nfeatures for the computers to look into. In the case of tax a natural\nchoice would be the words. So, using each has a feature is\na very common choice to start with, but of course there are other\nsophisticated features like phrases or even parts of ancients tags or\neven syntax to the structures. So once human experts can provide this\nthen we can use machine running to learn soft rules for\ncategorization from the training data. So, soft rules just means, we're going to get decided which category\nwe should be assigned for a document, but it's not going to be use using\na rule that is deterministic. So we might use something similar\nto saying that if it matches games, sports many times,\nit's likely to be sports. But, we're not going to say exactly for\nsure but instead, we're going to use probabilities or\nweights. So that we can combine\nmuch more evidences. So, the learning process,\nbasically is going to figure out which features are most useful for\nseparating different categories. And it's going to also figure out how to\noptimally combine features to minimize errors of the categorization\nof the training data. So the training data,\nas you can see here, is very important. It's the basis for learning. And then, the trained classifier can be\napplied to a new text object to predict the most likely category. And that's to simulate\nthe prediction of what human Would assign to this text object. If the human were to make a judgement. So when we use machine learning for\ntext categorization we can also talk about the problem in the general\nsetting of supervisement. So the set up is to learn\na classifier to map a value of X. Into a map of Y so\nhere X is all the text objects and Y is all the categories,\na set of categories. So the class phi will take\nany value in x as input and would generate a value in y as output. We hope that output y with\nthis right category for x. And here correct, of course,\nis judged based on the training data. So that's a general goal in machine\nlearning problems or supervised learning problems where you are given some examples\nof input and output for a function. And then the computer's\ngoing to figure out the, how the function behaves\nlike based on this examples. And then try to be able\nto compute the values for future x's that when we have not seen. So in general all methods\nwould rely on discriminative features of text objects to\ndistinguish different categories. So that's why these features\nare very important and they have to be provided by humans. And they will also combine multiple\nfeatures in a weight map with weights to be optimized to minimize\nerrors on the training data. So after the learning processes\noptimization problem. An objective function is often tied\ninto the errors on the training data. Different methods tend to vary in\ntheir ways of measuring the errors on the training data. They might optimize\na different objective function, which is often also called a loss\nfunction or cost function. They also tend to vary in their\nways of combining the features. So a linear combination for\nexample is simple, is often used. But they are not as powerful\nas nonlinear combinations. But nonlinear models\nmight be more complex for training, so there are tradeoffs as well. But that would lead to\ndifferent variations of many variations of these learning methods. So in general we can distinguish two\nkinds of classifiers at a high level. One is called generative classifiers. The other is called\ndiscriminative classifiers. The generative classifiers try to learn\nwhat the data looks like in each category. So it attempts to model the joint\ndistribution of the data and the label x and y and\nthis can then be factored out to a product of why\nthe distribution of labels. And the joint probability\nof sorry the conditional probability of X given Y, so it's Y. So we first model\nthe distribution of labels and then we model how the data is\ngenerate a particular label here. And once we can estimate these models, then we can compute this conditional\nprobability of label given data based on the probability\nof data given label. And the label distribution\nhere by using the Bayes Rule. Now this is the most important thing,\nbecause this conditional probability of the label can then be used directly\nto decide which label is most likely. So in such approaches objective\nfunction is actually likelihood. And so,\nwe model how the data are generated. So it only indirectly\ncaptures the training errors. But if we can model the data\nin each category accurately, then we can also classify accurately. One example is Na\u00efve Bayes classifier,\nin this case. The other kind of approaches\nare called discriminative classifies, and these classifies try to learn\nwhat features separate categories. So they direct or attack the problem of\ncategorization for separation of classes. So sorry for the problem. So, these discriminative\nclassifiers attempt to model the conditional probability of the label\ngiven the data point directly. So, the objective function tends\nto directly measure the errors of categorization on the training data. Some examples include\na logistical regression, support vector machines,\nand k-nearest neighbors. We will cover some of these classifiers\nin detail in the next few lectures. [MUSIC]",
 "09_4-9-text-categorization-generative-probabilistic-models.en.txt": "[SOUND]\nThis lecture is about how to use generative probabilistic\nmodels for text categorization. There are in general about two kinds\nof approaches to text categorization by using machine learning. One is by generating probabilistic models. The other is discriminative approaches. In this lecture, we're going to\ntalk about the generative models. In the next lecture, we're going to\ntalk about discriminative approaches. So the problem of text categorization is actually a very similar\nto document clustering. In that, we'll assume that each document\nit belongs to one category or one cluster. The main difference is that in\nclustering we don't really know what are the predefined categories are,\nwhat are the clusters. In fact,\nthat's the goal of text clustering. We want to find such clusters in the data. But in the case of categorization,\nwe are given the categories. So we kind of have\npre-defined categories and then based on these categories and\ntraining data, we would like to allocate a document to one of these categories or\nsometimes multiple categories. But because of the similarity\nof the two problems, we can actually get the document\nclustering models for text categorization. And we understand how we can\nuse generated models to do text categorization from\nthe perspective of clustering. And so, this is a slide that we've talked\nabout before, about text clustering, where we assume there are multiple topics\nrepresented by word distributions. Each topic is one cluster. So once we estimated such a model, we faced a problem of deciding which\ncluster document d should belong to. And this question boils down to decide\nwhich theta i has been used to generate d. Now, suppose d has L words\nrepresented as xi here. Now, how can you compute\nthe probability that a particular topic word distribution zeta i has\nbeen used to generate this document? Well, in general, we use base\nwall to make this influence and you can see this prior information here\nthat we need to consider if a topic or cluster has a higher prior\nthen it's more likely that the document has\nbeen from this cluster. And so, we should favor such a cluster. The other is a likelihood part,\nit's this part. And this has to do with whether\nthe topic word of distribution can explain the content\nof this document well. And we want to pick a topic\nthat's high by both values. So more specifically,\nwe just multiply them together and then choose which topic\nhas the highest product. So more rigorously,\nthis is what we'd be doing. So we're going to choose\nthe topic that would maximize. This posterior probability at the top\nof a given document gets posterior because this one,\np of the i, is the prior. That's our belief about\nwhich topic is more likely, before we observe any document. But this conditional probability here is the posterior probability of the topic\nafter we have observed the document of d. And base wall allows us to update this\nprobability based on the prior and I have shown the details,\nbelow here you can see how the prior here is related to\nthe posterior, on the left-hand side. And this is related to how\nwell this word distribution explains the document here, and\nthe two are related in this way. So to find the topic that has the higher posterior probability here it's\nequivalent to maximize this product as we have seen also,\nmultiple times in this course. And we can then change the probability\nof document in your product of the probability of each word, and\nthat's just because we've made an assumption about independence\nin generating each word. So this is just something that you\nhave seen in document clustering. And we now can see clearly how we\ncan assign a document to a category based on the information\nabout word distributions for these categories and\nthe prior on these categories. So this idea can be directly\nadapted to do categorization. And this is precisely what\na Naive Bayes Classifier is doing. So here it's most really\nthe same information except that we're looking at\nthe categorization problem now. So we assume that if theta i represents category i accurately,\nthat means the word distribution characterizes the content of\ndocuments in category i accurately. Then, what we can do is precisely\nlike what we did for text clustering. Namely we're going to assign\ndocument d to the category that has the highest probability\nof generating this document. In other words, we're going to maximize\nthis posterior probability as well. And this is related to the prior and the [INAUDIBLE] as you have\nseen on the previous slide. And so, naturally we can decompose this [INAUDIBLE] into\na product as you see here. Now, here, I change the notation so\nthat we will write down the product as product of all the words\nin the vocabulary, and even though the document\ndoesn't contain all the words. And the product is still accurately\nrepresenting the product of all the words in the document\nbecause of this count here. When a word,\nit doesn't occur in the document. The count would be 0, so\nthis time will just disappear. So if actively we'll just have the product\nover other words in the document. So basically, with Naive Bayes Classifier, we're going to score each category for\nthe document by this function. Now, you may notice that here it involves\na product of a lot of small probabilities. And this can cause and the four problem. So one way to solve the problem is\nthru take logarithm of this function, which it doesn't changes all\nthe often these categories. But will helps us preserve precision. And so, this is often the function\nthat we actually use to score each category and\nthen we're going to choose the category that has the highest\nscore by this function. So this is called an Naive Bayes\nClassifier, now the keyword base is understandable because we are applying\na base rule here when we go from the posterior probability of the topic to\na product of the likelihood and the prior. Now, it's also called a naive because\nwe've made an assumption that every word in the document is generated\nindependently, and this is indeed a naive assumption because in reality they're\nnot generating independently. Once you see some word,\nthen other words will more likely occur. For example,\nif you have seen a word like a text. Than that mixed category, they see more clustering more likely to\nappear than if you have not the same text. But this assumption allows\nus to simplify the problem. And it's actually quite effective for\nmany text categorization tasks. But you should know that\nthis kind of model doesn't have to make this assumption. We could for example, assume that\nwords may be dependent on each other. So that would make it a bigram analogy\nmodel or a trigram analogy model. And of course you can even use a mixture\nmodel to model what the document looks like in each category. So in nature, they will be all using\nbase rule to do classification. But the actual generating model for\ndocuments in each category can vary. And here, we just talk about very\nsimple case perhaps, the simplest case. So now the question is,\nhow can we make sure theta i actually represents category i accurately? Now in clustering,\nwe learned that this category i or what are the distributions for\ncategory i from the data. But in our case,\nwhat can we do to make sure this theta i represents indeed category i. Well if you think about the question, and you likely come up with the idea\nof using the training data. Indeed in the textbook, we typically assume that there\nis training data available and those are the documents that unknown\nto have generator from which category. In other words, these are the documents\nwith known categories assigned and of course human experts must do that. In here, you see that T1\nrepresents the set of documents that are known to have\nthe generator from category 1. And T2 represents the documents\nthat are known to have been generated from category 2, etc. Now if you look at this picture,\nyou'll see that the model here is really a simplified\nunigram language model. It's no longer mixed modal, why? Because we already know which distribution\nhas been used to generate which documents. There's no uncertainty here, there's\nno mixing of different categories here. So the estimation problem of\ncourse would be simplified. But in general,\nyou can imagine what we want to do is estimate these probabilities\nthat I marked here. And what other probability is that we have\nto estimate it in order to do relation. Well there are two kinds. So one is the prior,\nthe probability of theta i and this indicates how popular\neach category is or how likely will it have observed\nthe document in that category. The other kind is\nthe water distributions and we want to know what words have high\nprobabilities for each category. So the idea then is to just\nuse observe the training data to estimate these two probabilities. And in general, we can do this\nseparately for the different categories. That's just because these documents\nare known to be generated from a specific category. So once we know that, it's in some sense irrelevant of what\nother categories we are also dealing with. So now this is a statistical\nestimation problem. We have observed some\ndata from some model and we want to guess\nthe parameters of this model. We want to take our best\nguess of the parameters. And this is a problem that we have seen\nalso several times in this course. Now, if you haven't thought\nabout this problem, haven't seen life based classifier. It would be very useful for\nyou to pause the video for a moment and to think about how to solve this problem. So let me state the problem again. So let's just think about with category 1, we know there is one word of distribution\nthat has been used to generate documents. And we generate each word in the document\nindependently, and we know that we have observed a set of n sub 1\ndocuments in the set of Q1. These documents have been all\ngenerated from category 1. Namely have been all generated\nusing this same word distribution. Now the question is,\nwhat would be your guess or estimate of the probability of\neach word in this distribution? And what would be your guess of\nthe entire probability of this category? Of course,\nthis singular probability depends on how likely are you to see\ndocuments in other categories? So think for a moment, how do you\nuse all this training data including all these documents that are known\nto be in these k categories, to estimate all these parameters? Now, if you spend some time\nto think about this and it would help you understand\nthe following few slides. So do spend some time to make sure that\nyou can try to solve this problem, or do you best to solve the problem yourself. Now if you have thought about and\nthen you will realize the following to it. First, what's the bases for estimating the\nprior or the probability of each category. Well this has to do with whether you\nhave observed a lot of documents form that category. Intuitively, you have seen a lot\nof documents in sports and very few in medical science. Then you guess is that the probability\nof the sports category is larger or your prior on the category\nwould be larger. And what about the basis for estimating\nthe probability of where each category? Well the same, and you'll be just\nassuming that words that are observed frequently in the documents that are known\nto be generated from a category will likely have a higher probability. And that's just a maximum\nNaive Bayes made of. Indeed, that's what we can do, so this\nmade the probability of which category and to answer the question,\nwhich category is most popular? Then we can simply normalize,\nthe count of documents in each category. So here you see N sub i denotes\nthe number of documents in each category. And we simply just normalize these\ncounts to make this a probability. In other words, we make this\nprobability proportional to the size of training intercept in each category\nthat's a size of the set t sub i. Now what about the word distribution? Well, we do the same. Again this time we can do this for\neach category. So let's say,\nwe're considering category i or theta i. So which word has a higher probability? Well, we simply count the word occurrences in the documents that are known\nto be generated from theta i. And then we put together all\nthe counts of the same word in the set. And then we just normalize these\ncounts to make this distribution of all the words make all\nthe probabilities off these words to 1. So in this case, you're going to see this\nis a proportional through the count of the word in the collection of\ntraining documents T sub i and that's denoted by c of w and T sub i. Now, you may notice that we\noften write down probable estimate in the form of being\nproportional for certain numbers. And this is often sufficient, because we have some constraints\non these distributions. So the normalizer is\ndictated by the constraint. So in this case, it will be useful for\nyou to think about what are the constraints on these\ntwo kinds of probabilities? So once you figure out\nthe answer to this question, and you will know how to\nnormalize these accounts. And so this is a good exercise to\nwork on if it's not obvious to you. There is another issue in\nNaive Bayes which is a smoothing. In fact the smoothing is a general problem\nin older estimate of language morals. And this has to do with, what would happen if you have\nobserved a small amount of data? So smoothing is an important technique\nto address that outsmarts this. In our case, the training data can be\nsmall and when the data set is small when we use maximum likely estimator we often\nface the problem of zero probability. That means if an event is not observed then the estimated\nprobability would be zero. In this case, if we have not seen\na word in the training documents for let's say, category i. Then our estimator would be zero for the\nprobability of this one in this category, and this is generally not accurate. So we have to do smoothing to make\nsure it's not zero probability. The other reason for smoothing is that\nthis is a way to bring prior knowledge, and this is also generally true for\na lot of situations of smoothing. When the data set is small, we tend to rely on some prior\nknowledge to solve the problem. So in this case our [INAUDIBLE] says that\nno word should have zero probability. So smoothing allows us to inject\nthese to prior initial that no order has a real zero probability. There is also a third reason which\nus sometimes not very obvious, but we explain that in a moment. And that is to help achieve\ndiscriminative weighting of terms. And this is also called IDF weighting, inverse document frequency weighting that\nyou have seen in mining word relations. So how do we do smoothing? Well in general we add pseudo\ncounts to these events, we'll make sure that no event has 0 count. So one possible way of smoothing\nthe probability of the category is to simply add a small non active\nconstant delta to the count. Let's pretend that every category\nhas actually some extra number of documents represented by delta. And in the denominator we also add\na k multiplied by delta because we want the probability to some to 1. So in total we've added delta k times\nbecause we have a k categories. Therefore in this sum,\nwe have to also add k multiply by delta as a total pseudocount\nthat we add up to the estimate. Now, it's interesting to think\nabout the influence of that data, obvious data is a smoothing\nparameter here. Meaning that the larger data is and\nthe more we will do smoothing and that means we'll more\nrely on pseudocounts. And we might indeed ignore the actual\ncounts if they are delta is set to infinity. Imagine what would happen if there\nare approaches positively to infinity? Well, we are going to say every category\nhas an infinite amount of documents. And then there's no distinction to them so\nit become just a uniform. What if delta is 0? Well, we just go back to the original\nestimate based on the observed training data to estimate to estimate\nthe probability of each category. Now we can do the same for\nthe word distribution. But in this case,\nsometimes we find it useful to use a nonuniform seudocount for\nthe word. So here you'll see we'll add\na pseudocounts to each word and that's mule multiplied\nby the probability of the word given by a background\nlanguage model, theta sub b. Now that background model in\ngeneral can be estimated by using a logic collection of tests. Or in this case we will use the whole\nset of all the training data to estimate this background language model. But we don't have to use this one, we can use larger test data that\nare available from somewhere else. Now if we use such a background\nlanguage model that has pseudocounts, we'll find that some words will\nreceive more pseudocounts. So what are those words? Well those are the common words\nbecause they get a high probability by the background average model. So the pseudocounts added for\nsuch words will be higher. Real words on the other hand\nwill have smaller pseudocounts. Now this addition of background\nmodel would cause a nonuniform smoothing of these word distributions. We're going to bring the probability of\nthose common words to a higher level, because of the background model. Now this helps make the difference\nof the probability of such words smaller across categories. Because every category has some help\nfrom the background four words, and I get the, a,\nwhich have high probabilities. Therefore, it's not always so\nimportant that each category has documents that contain a lot\nof occurrences of such words or the estimate is more influenced\nby the background model. And the consequence is that\nwhen we do categorization, such words tend not to influence\nthe decision that much as words that have small probabilities\nfrom the background language model. Those words don't get some help\nfrom the background language model. So the difference would be primary because\nof the differences of the occurrences in the training documents\nin different categories. We also see another smoothing parameter\nmu here, which controls the amount of smoothing and just like a delta does for\nthe other probability. And you can easily understand why we\nadd mu to the denominator, because that represents the sum of all the pseudocounts\nthat we add for all the words. So view is also a non\nnegative constant and it's [INAUDIBLE] set to control smoothing. Now there are some interesting\nspecial cases to think about as well. First, let's think about when mu\napproaches infinity what would happen? Well in this case\nthe estimate would approach to the background language model we'll\nattempt to the background language model. So we will bring every word distribution\nto the same background language model and that essentially remove the difference\nbetween these categories. Obviously, we don't want to do that. The other special case is the thing\nabout the background model and suppose, we actually set\nthe two uniform distribution. And let's say,\n1 over the size of the vocabulary. So each one has the same probability,\nthen this smoothing formula is going to be very similar to the one\non the top when we add delta. It's because we're going to add\na constant pseudocounts to every word. So in general, in Naive Bayes categorization we\nhave to do such a small thing. And then once we have these probabilities, then we can compute the score for\neach category. For a document and then choose the category where it was\nthe highest score as we discussed earlier. Now, it's useful to\nfurther understand whether the Naive Bayes scoring\nfunction actually makes sense. So to understand that, and also to\nunderstand why adding a background model will actually achieve the effect of IDF\nweighting and to penalize common words. So suppose we have just two categories and we're going to score based on\ntheir ratio of probability, right? So this is the. Lets say this is our scoring function for two categories, right? So, this is a score of a document for\nthese two categories. And we're going to score based\non this probability ratio. So if the ratio is larger, then it means it's more\nlikely to be in category one. So the larger the score is the more likely the document is in category one. So by using Bayes' rule, we can write down this ratio as follows,\nand you have seen this before. Now, we generally take logarithm of this\nratio, and to avoid small probabilities. And this would then give us this\nformula in the second line. And here we see something\nreally interesting, because this is our scoring function for\ndeciding between the two categories. And if you look at this function,\nwe'll see it has several parts. The first part here is actually\nlog of probability ratio. And so this is a category bias. It doesn't really depend on the document. It just says which category is more\nlikely and then we would then favor this category slightly, right? So, the second part has a sum\nof all the words, right? So, these are the words that\nare observed in the document but in general we can consider all\nthe words in the vocabulary. So here we're going to\ncollect the evidence about which category is more likely,\nright? So inside of the sum you can see\nthere is product of two things. The first, is a count of the word. And this count of the word serves as\na feature to represent the document. And this is what we can\ncollect from document. The second part is\nthe weight of this feature, here it's the weight on which word, right? This weight tells us to\nwhat extent observing this word helps contribute in our decision to put this document in category one. Now remember,\nthe higher the scoring function is, the more likely it's in category one. Now if you look at this ratio, basically,\nsorry this weight it's basically based on the ratio of the probability of the\nword from each of the two distributions. Essentially we're comparing\nthe probability of the word from the two distributions. And if it's a higher according to theta 1, then according to theta 2,\nthen this weight would be positive. And therefore it means when\nwe observe such a word, we will say that it's more\nlikely to be from category one. And the more we observe such a word, the more likely the document\nwill be classified as theta 1. If, on the other hand,\nthe probability of the word from theta 1 is smaller than the probability\nof the word from theta 2, then you can see that\nthis word is negative. Therefore, this is negative evidence for\nsupporting category one. That means the more we\nobserve such a word, the more likely the document\nis actually from theta 2. So this formula now makes a little sense,\nright? So we're going to aggregate all\nthe evidence from the document, we take a sum of all the words. We can call this the features that we collected from the document\nthat would help us make the decision. And then each feature has\na weight that tells us how does this feature support category one or\njust support category two. And this is estimated as the log of\nprobability ratio here in na\u00efve Bayes. And then finally we have\nthis constant of bias here. So that formula actually\nis a formula that can be generalized to accommodate\nmore features and that's why I have introduce\nsome other symbols here. To introduce beta 0 to denote the Bayes\nand fi to denote the each feature and beta sub i to denote\nthe weight on each feature. Now we do this generalisation,\nwhat we see is that in general we can represent\nthe document by feature vector fi, here of course in this case\nfi is the count of a word. But in general, we can put any features\nthat we think are relevant for categorization. For example, document length or font size or\ncount of other patterns in the document. And then our scoring function can be\ndefined as a sum of a constant beta 0 and the sum of the feature\nweights of all the features. So if each f sub i is a feature\nvalue then we multiply the value by the corresponding weight,\nbeta sub i, and we just take the sum. And this is the aggregate of all evidence\nthat we can collect from all these features. And of course there are parameters here. So what are the parameters? Well, these are the betas. These betas are weights. And with a proper setting of the weights,\nthen we can expect such a scoring function to work well to classify documents,\njust like in the case of naive Bayes. We can clearly see naive Bayes\nclassifier as a special case of this general classifier. Actually, this general form is very close\nto a classifier called a logistical regression, and this is actually one\nof those conditional approaches or discriminative approaches\nto classification. And we're going to talk more\nabout such approaches later, but here I want you to note that\nthere is a strong connection, a close connection between\nthe two kinds of approaches. And this slide shows how naive Bayes\nclassifier can be connected to a logistic regression. And you can also see that in\ndiscriminative classifiers that tend to use more\ngeneral form on the bottom, we can accommodate more\nfeatures to solve the problem. [MUSIC]",
 "01_5-1-text-categorization-discriminative-classifier-part-1.en.txt": "[SOUND]\nThis lecture is about the discriminative\nclassifiers for text categorization. In this lecture we're going to\ncontinue talking about how to do text categorization and\ncover discriminative approaches. This is a slide that you have seen from\nthe discussion of Naive Bayes Classifier, where we have shown that although\nNaive Bayes Classifier tries to model the generation of text data, from each\ncategories, we can actually use Bayes' rule to eventually rewrite the scoring\nfunction as you see on this slide. And this scoring function is basically\na weighted combination of a lot of word features, where the feature values\nare word counts, and the feature weights are the log of probability ratios of\nthe word given by two distributions here. Now this kind of scoring function\ncan be actually a general scoring function where we can in general\npresent text data as a feature vector. Of course the features don't\nhave to be all the words. Their features can be other\nsignals that we want to use. And we mentioned that this is precisely\nsimilar to logistic regression. So, in this lecture we're going to\nintroduce some discriminative classifiers. They try to model\nthe conditional distribution of labels given the data directly\nrather than using Bayes' rule to compute that interactively\nas we have seen in naive Bayes. So the general idea of logistic\nregression is to model the dependency of a binary\nresponse variable Y on some predictors that are denoted as X. So here we have also changed the notation to X for future values. You may recall in the previous\nslides we have used FI to represent the future values. And here we use the notation of X factor, which is more common when we introduce such discriminative algorithms. So, X is our input. It's a vector with n features and\neach feature has a value x sub i here. And I will go with a model that dependency\nof this binary response variable of these features. So in our categorization problem when\nI have two categories theta 1 and theta 2, and we can use the Y value to\ndenote the two categories when Y is 1, it means the category of the document,\nthe first class, is theta 1. Now, the goal here is the model, the\nconditional property of Y given X directly as opposed to model of the generation of\nX and Y as in the case of Naive Bayes. And another advantage of this\nkind of approach is that it would allow many other features\nthan words to be used in this vector since we're not modeling\nthe generation of this vector. And we can plug in any\nsignals that we want. So this is potentially advantageous for\ndoing text categorization. So more specifically,\nin logistic regression, assume the functional form of Y\ndepending on X is the following. And this is very closely\nrelated to the log odds that I introduced in the Naive Bayes\nor log of probability ratio of the two categories that you\nhave seen on the previous slide. So this is what I meant. So in the case of Naive Bayes,\nwe compute this by using those words and eventually we have reached\na formula that looks like this. But here we actually\nwould assume explicitly that we with the model our probability of Y given X directly as a function of these features. So, most specifically we assume that the\nratio of the probability of Y equals 1 and the probability of Y equals\n0 is a function of X. All right, so it's a function of x and it's a linear combination of these feature\nvalues controlled by theta values. And it seems we know that\nthe probability of Y equals zero is one minus probability\nof Y equals one and this can be also written in this way. So this is a log out ratio here. And so in logistic regression, we're basically assuming that\nthe probability of Y equals 1. Okay my X is dependent on this linear\ncombination of all these features. So it's just one of the many possible\nways, assuming that the dependency. But this particular form\nhas been quite useful and it also has some nice properties. So if we rewrite this equation to actually\nexpress the probability of Y given X. In terms of X by getting rid of\nthe logarithm we get this functional form, and this is called a logistical function. It's a transformation of X into Y,\nas you see on the right side here, so\nthat the X's will be map into a range of values from 0 to 1.0,\nyou can see. And that's precisely what we want\nsince we have a probability here. And the function form looks like this. So this is the basic idea\nof logistic regression. And it's a very useful classifier that can be used to do a lot of classification\ntasks including text categorization. So as in all cases of model we would be\ninterested in estimating the parameters. And in fact in all of the machine running\nprograms, once you set up with the model, set up object and\nfunction to model the file, then the next step is to\ncompute the parameter values. In general, we're going to adjust\nto these parameter values. Optimize the performance of\nclassify on the training data. So in our case just assume we have\nthe training data here, xi and yi, and each pair is basically a future vector\nof x and a known label for that x. Y is either 1 or 0. So in our case we are interested\nmaximize this conditional likelihood. The conditional likelihood here is basically to model why\ngiven observe the x, so it's not like a moderate x, but rather we're going to model this. Note that this is a conditional\nprobability of Y given X and this is also precisely what we wanted For\nclassification. Now so the likelihood function would be\njust a product of all the training cases. And in each case, this is the model of the probability of\nobserving this particular training case. So given a particular Xi, how likely\nwe are to observe the corresponding Yi? Of course, Yi could be 1 or\n0, and in fact, the function found here would vary\ndepending on whether Yi is 1 or 0. If it's a 1, we'll be taking this form. And that's basically the logistic\nregression function. But what about this, if it's 0? Well, if it's 0, then we have to use\na different form, and that's this one. Now, how do we get this one? Well, that's just a 1 minus\nthe probability of Y=1, right? And you can easily see this. Now the key point in here is that the\nfunction form here depends on the observer Yi, if it's a 1,\nit has a different form than when it's 0. And if you think about when we\nwant to maximize this probability, we're basically going to want this\nprobability to be as high as possible. When the label is 1, that means\nthe document is in probability 1. But if the document is not,\nwe're going to maximize this value, and what's going to happen is\nactually to make this value as small as possible because this sum's 1. When I maximize this one,\nit's equivalent to minimize this one. So you can see basically, if we maximize\nthe conditional likelihood, we're going to basically try to make the prediction on\nthe training data as accurate as possible. So as another occasion, when you\ncompute the maximum likelihood data, basically you'll find a beta value, a set of beta values that would\nmaximize this conditional likelihood. And this, again, then gives us\na standard optimization problem. In this case,\nit can be also solved in many ways. Newton's method is a popular\nway to solve this problem, there are other methods as well. But in the end,\nwe will look at a set of data values. Once we have the beta values,\nthen we have a way to find the scoring function to help us classify a document. So what's the function? Well, it's this one. See, if we have all the beta values,\nare they known? All we need is to compute the Xi for that\ndocument and then plug in those values. That will give us an estimated probability\nthat the document is in category one. Okay so, so much for\nlogistical regression. Let's also introduce another\ndiscriminative classifier called K-Nearest Neighbors. Now in general, I should say there\nare many such approaches, and a thorough introduction to all of them is\nclearly beyond the scope of this course. And you should take\na machine learning course or read more about machine\nlearning to know about them. Here, I just want to include the basic\nintroduction to some of the most commonly used classifiers, since you might\nuse them often for text calculation. So the second classifier is\ncalled K-Nearest Neighbors. In this approach,\nwe're going to also estimate the conditional probability of label\ngiven data, but in a very different way. So the idea is to keep all\nthe training examples and then once we see a text object that we\nwant to classify, we're going to find the K examples in the training set and\nthat are most similar to this text object. Basically, this is to find\nthe neighbors of this text objector in the training data set. So once we found the neighborhood and we found the object that are close to the\nobject we are interested in classifying, and let's say we have found\nthe K-Nearest Neighbors. That's why this method is\ncalled K-Nearest Neighbors. Then we're going to assign the category\nthat's most common in these neighbors. Basically we're going to allow\nthese neighbors to vote for the category of the objective that\nwe're interested in classifying. Now that means if most of them have\na particular category and it's a category one, they're going to say this\ncurrent object will have category one. This approach can also be improved by\nconsidering the distance of a neighbor and of a current object. Basically, we can assume a closed\nneighbor would have more say about the category of the subject. So, we can give such a neighbor\nmore influence on the vote. And we can take away some of\nthe votes based on the distances. But the general idea is look\nat the neighborhood, and then try to assess the category based\non the categories of the neighbors. Intuitively, this makes a lot of sense. But mathematically, this can also be\nregarded as a way to directly estimate there's a conditional probability of\nlabel given data, that is p of Y given X. Now I'm going to explain this intuition in\na moment, but before we proceed, let me emphasize that we do need a similarity\nfunction here in order for this to work. Note that in naive base class five,\nwe did not need a similarity function. And in logistical regression, we did not\ntalk about those similarity function either, but here we explicitly\nrequire a similarity function. Now this similarity function\nactually is a good opportunity for us to inject any of our\ninsights about the features. Basically effective features\nare those that would make the objects that are on the same\ncategory look more similar, but distinguishing objects\nin different categories. So the design of this similarity function\nis closely tied it to the design of the features in logistical\nregression and other classifiers. So let's illustrate how K-NN works. Now suppose we have a lot\nof training instances here. And I've colored them differently and\nto show just different categories. Now suppose we have a new object in\nthe center that we want to classify. So according to this approach,\nyou work on finding the neighbors. Now, let's first think of a special\ncase of finding just one neighbor, the closest neighbor. Now in this case, let's assume the closest\nneighbor is the box filled with diamonds. And so then we're going to say,\nwell, since this is in this object that is in category of diamonds,\nlet's say. Then we're going to say, well, we're going to assign the same\ncategory to our text object. But let's also look at another possibility\nof finding a larger neighborhood, so let's think about the four neighbors. In this case, we're going to include a lot\nof other solid field boxes in red or pink, right? So in this case now, we're going to\nnotice that among the four neighbors, there are three neighbors\nin a different category. So if we take a vote, then we'll conclude the object is\nactually of a different category. So this both illustrates how\ncan nearest neighbor works and also it illustrates some potential\nproblems of this classifier. Basically, the results might\ndepend on the K and indeed, k's an important parameter to optimize. Now, you can intuitively imagine\nif we have a lot of neighbors around this object, and\nthen we'd be okay because we have a lot of neighbors who will\nhelp us decide the categories. But if we have only a few,\nthen the decision may not be reliable. So on the one hand,\nwe want to find more neighbor, right? And then we have more votes. But on the other hand, as we try to find\nmore neighbors we actually could risk on getting neighbors that are not\nreally similar to this instance. They might actually be far away\nas you try to get more neighbors. So although you get more neighbors but\nthose neighbors aren't necessarily so helpful because they are not\nvery similar to the object. So the parameter still has\nto be set empirically. And typically, you can optimize such\na parameter by using cross validation. Basically, you're going to separate\nyour training data into two parts and then you're going to use one\npart to actually help you choose the parameter k here or some other\nparameters in other class files. And then you're going to assume\nthis number that works well on your training that will be actually be\nthe best for your future data. So as I mentioned, K-NN can be actually regarded as estimate\nof conditional problem within y given x an that's why we put this in the category\nof discriminative approaches. So the key assumption that we made in\nthis approach is that the distribution of the label given the document\nprobability a category given for example probability of theta i\ngiven document d is locally smooth. And that just means we're going to assume\nthat this probability is the same for all the documents in these region R here. And suppose we draw a neighborhood and\nwe're going to assume in this neighborhood since the data instances are very\nsimilar we're going to assume that the conditional distribution of the label\ngiven the data will be roughly the same. If these are very different\nthen we're going to assume that the probability of c doc given\nd would be also similar. So that's a very key assumption. And that's actually important assumption that would allow us to\ndo a lot of machinery. But in reality, whether this is true of course,\nwould depend on how we define similarity. Because neighborhood is largely\ndetermined by our similarity function. If our similarity function captures\nobjects that do follow similar distributions then these\nassumptions are okay but if our similarity function could\nnot capture that, obviously these assumption would be a problem and\nthen the classifier would not be accurate. Okay, let's proceed with these assumption. Then what we are saying is that, in order to estimate the probability\nof category given a document. We can try to estimate the probability of\nthe category given that entire region. Now, this has a benefit, of course, of bringing additional data points to\nhelp us estimate this probability. And so this is precisely the idea of K-NN. Basically now we can use\nthe known categories of all the documents in this region\nto estimate this probability. And I have even given a formula here where\nyou can see we just count the topics in this region and then normalize that by the\ntotal number of documents in the region. So the numerator that you see here,\nc of theta i and r, is a counter of the documents in\nregion R was category theta i. Since these are training document and\nwe know they are categories. We can simply count how many\ntimes it was since here. How many times we have the same signs,\netc. And then the denominator is just\nthe total number of training documents in this region. So this gives us a rough estimate of\nwhich categories most popular in this neighborhood. And we are going to assign\nthe popular category to our data object since\nit falls into this region. [MUSIC]",
 "02_5-2-text-categorization-discriminative-classifier-part-2.en.txt": "[SOUND] This lecture is\na continued discussion of Discriminative Classifiers for\nText Categorization. So, in this lecture,\nwe're going to introduce, yet another Discriminative Classifier called\nthe Support Vector Machine or SVM. Which is a very popular\nclassification method and it has been also shown to be effective for\ntext categorization. So to introduce this classifier, let's also think about the simple\ncase of two categories. We have two topic categories,\n01 and 02 here. And we want to classify documents\ninto these two categories and we're going to represent again\na document by a feature factor x here. Now, the idea of this classifier is\nto design also a linear separator here that you'll see and it's very similar to what you have\nseen not just for regression, right? And we're going to do also say\nthat if the sign of this function value is positive then we're going to\nsay the objective is in category one. Otherwise, we're going to\nsay it's in category 2. So that makes 0 that is the decision\nboundary between the few categories. So, in generally hiding\nmarginal space such as, 0. corresponds to a hyper plain. Now I've shown you a simple case of two\ndimensional space it was just X1 and X2 and this case this corresponds\nto a line that you can see here. So, this is a line defined by just three parameters here,\nbeta zero, beta one, and beta two. Now, this line is heading\nin this direction so it shows that as we increase X1,\nX2 will also increase. So we know that beta one and beta two have\ndifferent assigns, one is negative and the other is positive. So let's just assume that beta one is\nnegative and beta two Is positive. Now, it's interesting to examine, then, the data instances on\nthe two sides of the slide. So, here, the data instance are visualized\nas circles for one class and diamonds for the other class. Now, one question is to take a point\nlike this one and to ask the question what's the value of this expression, or\nthis classifier, for this data point? So what do you think? Basically, we're going to evaluate\nits value by using this function. And as we said, if this value's positive\nwe're going to say this is in category one, and if it's negative,\nit's going to be in category two. Intuitively, this line separates these two\ncategories, so we expect the points on one side would be positive and the points\non the other side would be negative. Our question is under the assumption\nthat I just mentioned, let's examine a particular\npoint like this one. So what do you think is\nthe sine of this expression? Well, to examine the sine we can\nsimply look at this expression here. And we can compare this with let's say, value on the line, let's see,\ncompare this with this point. While they have identical X1, but\nthen one has a higher value for X2. Now, let's look at the sin\nof the coefficient for X2. Well, we know this is a positive. So, what that means is\nthat the f value for this point should be higher\nthan the f value for this point on the line that means\nthis will be positive, right? So we know in general of\nall points on this side, the function's value will be positive and you can also verify all the points\non this side will be negative. And so this is how this kind\nof linear classifier or linear separator can then separate\nthe points in the two categories. So, now the natural question is,\nwhich linear separator is the best? Now, I've get you one line here\nthat can separate the two classes. And this line, of course, is determined\nby the vector beta, the coefficients. Different coefficients will\ngive us different lines. So, we could imagine there are other\nlines that can do the same job. Gamma, for example, could give us another line that counts\na separator to these instances. Of course, there are also lines that won't\nseparate to them and those are bad lines. But, the question is,\nwhen we have multiple lines that can separate both clauses,\nwhich align the best? In fact, you can imagine, there are many\ndifferent ways of choosing the line. So, the logistical regression classifier\nthat you have seen earlier actually uses some criteria to determine where this line\nshould be and so linear separate as well. And uses a conditional likelihood\non the training that it determines which line is the best. But in SVM we're going to\nlook at another criteria for determining which line is the best. And this time, the criteria is more tied to\nthe classification arrow as you will see. So, the basic idea is to choose\nthe separator to maximize the margin. So what is a margin? So, I choose some dotted\nlines here to indicate the boundaries of those\ndata points in each class. And the margin is simply\nthe distance between the line, the separator, and\nthe closest point from each class. So you can see the margin of this\nside is as I've shown here and you can also define\nthe margin on the other side. In order for\nthe separator to maximize the margin, it has to be kind of in the middle\nof the two boundaries and you don't want this separator to\nbe very close to one side, and that in intuition makes a lot of sense. So this is basic idea of SVM. We're going to choose a linear\nseparator to maximize the margin. Now on this slide,\nI've also changed the notation so that I'm not going to use beta\nto denote the parameters. But instead, I'm going to use w although\nw was used to denote the words before so don't be confused here. W here is actually a width,\na certain width. So I'm also using lowercase b to\ndenote the beta 0, a biased constant. And there are instances do\nrepresent that as x and I also use the vector form\nof multiplication here. So we see a transpose of w vector\nmultiply by the future vector. So b is a bias constant and w is a set of\nweights with one way for each feature. We have m features and\nso we have m weights and that will represent as a vector. And similarly, the data instance here,\nthe text object, is represented by also a feature\nvector of the same number of elements. Xi is a feature value. For example, word count and\nyou can verify, when we. Multiply these two vectors together,\ntake the dot product, we get the same form of the linear\nseparator as you have seen before. It's just a different way\nof representing this. Now I use this way so that it's\nmore consistent with what notations people usually use when\nthey talk about SVM. This way you can better connect the slides\nwith some other readings you might do. Okay, so when we maximize\nthe margins of a separator, it just means the boundary of\nthe separator is only determined by a few data points, and these are the data\npoints that we call support vectors. So here illustrated are two support\nvectors for one class and two for the other class. And these quotas define\nthe margin basically, and you can imagine once we know which\nare supportive vectors then this center separator line will\nbe determined by them. So the other data points actually\ndon't really matter that much. And you can see if you change the other\ndata points it won't really affect the margin, so\nthe separator will stay the same. Mainly affected by\nthe the support vector machines. Sorry, it's mainly affected\nby the support vectors and that's why it's called\na support vector machine. Okay, so now the next question is,\nof course, how can we set it up to optimize the line? How can we actually find the line or\nthe separator? Now this is equivalent to\nfinding values for w and b, because they will determine\nwhere exactly the separator is. So in the simplest case, the linear SVM\nis just a simple optimization problem. So again, let's recall that our classifier\nis such a linear separator, where we have weights for all the features, and the\nmain goal is remove these weights w and b. And the classifier will say X is in\ncategory theta 1 if it's positive. Otherwise, it's going to say\nit's in the other category. So this is our assumption, our setup. So in the linear SVM,\nwe are going to then seek these parameter values to optimize the margins and\nthen the training error. The training data would be basically\nlike in other classifiers. We have a set of training points\nwhere we know the x vector, and then we also know the corresponding label,\ny i. And here we define y i as two values, but these values are not 0, 1 as you\nhave seen before, but rather -1 and positive 1, and they're corresponding to\nthese two categories, as I've shown here. Now you might wonder why we\ndon't define them as 0 and 1 instead of having -1, 1. And this is purely for mathematical\nconvenience, as you will see in a moment. So the goal of optimization first is to make sure the labeling of\ntraining data is all correct. So that just means if y i,\nthe norm label for instance x i, is 1, we would like this\nclassified value to be large. And here we just choose\na threshold of 1 here. But if you use another threshold,\nyou can easily fit that constant into the parameter values b and\nw to make the right-hand side just 1. Now if, on the other hand, y i is -1,\nthat means it's in a different class, then we want this classifier\nto give us a very small value, in fact a negative value, and we want this\nvalue to be less than or equal to -1. Now these are the two different instances,\ndifferent kinds of cases. How can we combine them together? Now this is where it's convenient\nwhen we have chosen y i as -1 for the other category, because it turns out that we can either\ncombine the two into one constraint. y i multiplied by the classifier value\nmust be larger than or equal to 1. And obviously when y i is just 1, you see this is the same as\nthe constraint on the left-hand side. But when y i is -1, you also see that this\nis equivalent to the other inequality. So this one actually captures both\nconstraints in a unified way, and that's a convenient way of\ncapturing these constraints. What's our second goal? Well, that's to maximize margin, so we want to ensure that separator\ncan do well on the training data. But then, among all the cases\nwhere we can separate the data, we also would like to choose the separator\nthat has the largest margin. Now the margin can be assumed to be\nrelated to the magnitude of the weight. And so\nw transform multiplied by w would give us basically the sum of\nsquares of all those weights. So to have a small value for\nthis expression, it means all the w i's must be small. So we've just assumed that\nwe have a constraint for getting the data on the training\nset to be classified correctly. Now we also have the objective that's\ntied into a maximization of margin, and this is simply to minimize\nw transpose multiplied by w, and we often denote this by phi of w. So now you can see this is\nbasically a optimization problem. We have some variables to optimize,\nand these are the weights and b and we have some constraints. These are linear constraints and the objective function is\na quadratic function of the weights. So this a quadratic program\nwith linear constraints, and there are standard algorithm that\nare variable for solving this problem. And once we solve the problem\nwe obtain the weights w and b. And then this would give us\na well-defined classifier. So we can then use this classifier\nto classify any new text objects. Now the previous formulation did not\nallow any error in the classification, but sometimes the data may not\nbe linear to the separator. That means that they may not\nlook as nice as you have seen on the previous slide where a line\ncan separate all of them. And what would happen if\nwe allowed some errors? Well, the principle can stay. We want to minimize the training error but\ntry to also maximize the margin. But in this case we have a soft margin, because the data points may\nnot be completely separable. So it turns out that we can easily\nmodify SVM to accommodate this. So what you see here is very similar\nto what you have seen before, but we have introduced\nthe extra variable xi i. And we in fact will have one for\neach data instance, and this is going to model the error\nthat we allow for each instance. But the optimization problem\nwould be very similar. So specifically, you will see we have added something\nto the optimization problem. First we have added some\nerror to the constraint so that now we allow a Allow the classifier to make some mistakes here. So, this Xi i is allowed an error. If we set Xi i to 0, then we go\nback to the original constraint. We want every instance to\nbe classified accurately. But, if we allow this to be non-zero,\nthen we allow some errors here. In fact, if the length of the Xi i is very\nlarge, the error can be very, very large. So naturally,\nwe don't want this to happen. So we want to then also\nminimize this Xi i. So, because Xi i needs to be minimized\nin order to control the error. And so, as a result,\nin the objective function, we also add more to the original one,\nwhich is only W, by basically ensuring that we not\nonly minimize the weights, but also minimize the errors, as you see here. Here we simply take a sum\nover all the instances. Each one has a Xi i to model\nthe error allowed for that instance. And when we combine them together, we basically want to minimize\nthe errors on all of them. Now you see there's a parameter C here,\nand that's a constant to control the trade-off between minimizing\nthe errors and maximizing the margin. If C is set to zero, you can see, we go back to the original object function\nwhere we only maximize the margin. We don't really optimize\nthe training errors and then Xi i can be set to a very large value\nto make the constraints easy to satisfy. That's not very good of course, so C should be set to a non-zero value,\na positive value. But when C is set to a very,\nvery large value, we'll see the object of the function will\nbe dominated mostly by the training errors and so the optimization of margin\nwill then play a secondary role. So if that happens, what would happen is then we will try to do our best to\nminimize the training errors, but then we're not going to\ntake care of the margin and that affects the generalization factors\nof the classify for future data. So it's also not good. So in particular, this parameter C\nhas to be actually set carefully. And this is just like in the case of\nk-nearest neighbor where you need to optimize a number of neighbors. Here you need to optimize the C. And this is, in general,\nalso achievable by doing cross-validation. Basically, you look at\nthe empirical data and see what value C should be set to in\norder to optimize the performance. Now with this modification, the problem is still quadratic programming\nwith linear constraints so the optimizing algorithm can be actually applied to solve\nthis different version of the program. Again, once we have obtained\nthe weights and the bias, then we can have classifier that's\nready for classifying new objects. So that's the basic idea of SVM. So to summarize the text\ncategorization methods, where we introduce the many methods,\nand some are generative models. Some are discriminative methods. And these tend to perform\nsimilarly when optimized. So there's still no clear winner,\nalthough each one has its pros and cons. And the performance might also\nvary on different data sets for different problems. And one reason is also because the feature\nrepresentation is very critical and these methods all require\neffective feature representation. And to design an effective feature set, we need domain knowledge and humans\ndefinitely play an important role here, although there are new\nmachine learning methods and algorithm representation learning\nthat can help with learning features. And another common thing\nis that they might be performing similarly on the data set, but with different mistakes. And so,\ntheir performance might be similar, but then the mistakes they\nmake might be different. So that means it's useful to\ncompare different methods for a particular problem and\nthen maybe combine multiple methods because this can improve the robustness\nand they won't make the same mistakes. So assemble approaches that\nwould combine different methods tend to be more robust and\ncan be useful in practice. Most techniques that we introduce\nuse the supervised machine learning, which is a very general method. So that means that these methods can\nbe actually applied to any text or categorization problem. As long as we have humans to help\nannotate some training data sets and design features, then supervising machine\nlearning and all these classifiers can be easily applied to those problems\nto solve the categorization problem to allow us to characterize content\nof text concisely with categories. Or to predict the sum\nproperties of real world variables that are associated\nwith text data. The computers, of course, here are trying\nto optimize the combinations of the features provided by human. And as I said, there are many\ndifferent ways of combining them and they also optimize different object or\nfunctions. But in order to achieve good performance,\nthey all require effective features and also plenty of training data. So as a general rule, and if you can\nimprove the feature representation, and then provide more training data,\nthen you can generally do better. Performance is often much more\naffected by the effectiveness of features than by the choice\nof specific classifiers. So feature design tends to be more\nimportant than the choice of specific classifier. So, how do we design effective features? Well, unfortunately,\nthis is very application-specific. So there's no really much\ngeneral thing to say here. But we can do some analysis of\nthe categorization problem and try to understand what kind of features\nmight help us distinguish categories. And in general, we can use a lot of domain\nknowledge to help us design features. And another way to figure out\nthe effective features is to do error analysis on\nthe categorization results. You could, for example, look at which category tends to be\nconfused with which other categories. And you can use a confusion matrix\nto examine the errors systematically across categories. And then,\nyou can look into specific instances to see why the mistake has been made and\nwhat features can prevent the mistake. And this can allow you to obtain\ninsights for design new features. So error analysis is very\nimportant in general, and that's where you can get the insights\nabout your specific problem. And finally, we can leverage this\non machine learning techniques. So, for example, feature selection is\na technique that we haven't really talked about, but is very important. And it has to do with trying to select the\nmost useful features before you actually train a full classifier. Sometimes training a classifier will also\nhelp you identify which features have high values. There are also other ways\nto ensure this sparsity. Of the model,\nmeaning to recognize the widths. For example, the SVM actually tries\nto minimize the weights on features. But you can further force some features, force to use only a small\nnumber of features. There are also techniques for\ndimension reduction. And that's to reduce a high dimensional\nfeature space into a low dimensional space typically by clustering\nof features in various ways. So metrics factorization\nhas been used to do such a job, and this is some of the\ntechniques are actually very similar to the talking models that we'll discuss. So talking morals like psa or lda can actually help us reduce\nthe dimension of features. Like imagine the words\nour original feature. But the can be matched to the topic\nspace .Let's say we have k topics. So a document can now be represented as a vector of just k values\ncorresponding to the topics. So we can let each topic define one\ndimension, so we have a k dimensional space instead of the original high\ndimensional space corresponding to words. And this is often another way\nto learn effective features. Especially, we could also use the\ncategories to supervise the learning of such low dimensional structures. And so, the original worth features\ncan be also combined with such amazing dimension features or\nlower dimensional space features to provide a multi resolution\nwhich is often very useful. Deep learning is a new technique that\nhas been developed the machine learning. It's particularly useful for\nlearning representations. So deep learning refers to deep neural\nnetwork, it's another kind of classifier, where you can have intermediate\nfeatures embedded in the models. That it's highly non-linear transpire, and some recent events that's allowed us to\ntrain such a complex network effectively. And the technique has been shown to be\nquite effective for speech recognition, computer reasoning, and\nrecently has been applied to text as well. It has shown some promise. And one important advantage\nof this approach in relationship with the featured design,\nis that they can learn intermediate replantations or\ncompound the features automatically. And this is very valuable for\nlearning effective replantation, for text recalibration. Although in text domain, because words are\nexemplary representation of text content, because these are human's imaging for\ncommunication. And they are generally sufficient for\nFor representing content for many tasks. If there's a need for\nsome new representation, people would have invented a new word. So because of this we think\nof value of deep learning for text processing tends to be lower than for\n[INAUDIBLE]. And the speech revenue where\nthey are anchored corresponding where the design that worked as features. But people only still very promising for\nlearning effective features especially for complicated tasks. Like a analysis it has\nbeen shown to be effective because it can provide that\ngoes beyond that of words. Now regarding the training examples. It's generally hard to get a lot of\ntraining examples because it involves human labor. But there are also some\nways to help with this. So one is to assume in some low quality\ntraining examples can also be used. So, those can be called\npseudo training examples. For example, if you take reviews from the\ninternet, they might have overall ratings. So, to train a of categorizer,\nmeaning we want to positive or negative. And categorize these reviews\ninto these two categories. Then we could assume five star reviews\nare all positive training samples. One star are negative. But of course, sometimes even five star reviews will also\nmention negative opinions so the training sample is not all of that high quality,\nbut they can still be useful. Another idea is to exploit\nthe unlabeled data and there are techniques called\nthe semi-supervised machine learning techniques that can allow you to\ncombine labeled data with unlabeled data. So, in other case it's easy to see\nthe next model can be used For both text plus read and\nthe categorization. So you can imagine, if you have a lot of\nunlabeled text data for categorization, then you can actually do clustering\non these text data, learn categories. And then try to somehow\nalign these categories. With the categories defined\nby the training data, where we already know which\ndocuments are in which category. So you can in fact use the Algorithm\nto actually combine both. That would allow you essentially also\npick up useful words and label the data. You can think of this in another way. Basically, we can use let's say a to classify all of the unlabeled text\ndocuments, and then we're going to assume the high confidence Classification\nresults are actually liable. Then you suddenly have more training\ndata because from the enabler that we now know some are labeled as category one,\nsome are labeled as category two. All though the label is not\ncompletely reliable But then they can still be useful. So let's assume they are actually training\nlabel examples, and then we combine them with true training examples through\nimproved categorization method. And so this idea is very powerful. When the enabled data and\nthe training data are very different, and we might need to use other advanced\nmachine learning techniques called domain adaptation or\ntransfer learning. This is when we can Borrow some training examples from\na related problem that may be different. Or, from a categorization password that follow very different distribution\nfrom what we are working on. But basically,\nwhen the two domains are very different, then we need to be careful and\nnot overfit the training domain. But yet, we can still want to use some\nsignals from the related training data. So for example,\ntraining categorization on news might not give you Effective plus y for\nclass vine topics and tweets. But you can still learn something from\nnews to help look at writing tweets. So there are mission learning techniques\nthat can help you do that effectively. Here's a suggested reading where you\ncan find more details about some more of the methods is\nthat we have covered. [MUSIC]",
 "03_5-3-text-categorization-evaluation-part-1.en.txt": "[SOUND] This lecture is about the Evaluation of Text Categorization. So we've talked about many different\nmethods for text categorization. But how do you know which\nmethod works better? And for a particular application, how do you know this is the best\nway of solving your problem? To understand these, we have to how to we have to know how to\nevaluate categorization results. So first some general thoughts\nabout the evaluation. In general, for evaluation of this kind of\nempirical tasks such as categorization, we use methodology that\nwas developed in 1960s by information retrieval researchers. Called a Cranfield Evaluation Methodology. The basic idea is to have\nhumans create test correction, where, we already know, every document\nis tagged with the desired categories. Or, in the case of search, for which\nquery, which documents that should have been retrieved, and\nthis is called, a ground truth. Now, with this ground\ntruth test correction, we can then reuse the collection to\ntest the many different systems and then compare different systems. We can also turn off some components in\nthe system to see what's going to happen. Basically it provides a way to do control\nexperiments to compare different methods. So this methodology has\nbeen virtually used for all the tasks that involve\nempirically defined problems. So in our case, then, we are going to\ncompare our systems categorization results with the categorization,\nground truth, created by humans. And we're going to compare\nour systems decisions, which documents should get\nwhich category with what categories have been assigned\nto those documents by humans. And we want to quantify\nthe similarity of these decisions or equivalently, to measure the difference\nbetween the system output and the desired ideal output\ngenerated by the humans. So obviously, the highest similarity\nis the better results are. The similarity could be\nmeasured in different ways. And that would lead to different measures. And sometimes it's desirable also to match\nthe similarity from different perspectives just to have a better understanding\nof the results in detail. For example, we might be also interested\nin knowing which category performs better and which which category\nis easy to categorize, etc. In general,\ndifferent categorization mistakes however, have different costs for\nspecific applications. So some areas might be\nmore serious than others. So ideally, we would like to model\nsuch differences, but if you read many papers in categorization you will\nsee that they don't generally do that. Instead, they will use a simplified\nmeasure and that's because it's often okay not to consider such a cost\nvariation when we compare methods and when we are interested in knowing\nthe relative difference of these methods. So it's okay to introduce some bias,\nas long as the bias is not already with a particular method and then we should\nexpect the more effective method to perform better than a less effective one,\neven though the measure is not perfect. So the first measure that we'll introduce\nis called classification accuracy and this is a basic into measure\nthe percentage of correct decisions. So here you see that there\nare categories denoted by c1 through ck and there are n documents,\ndenoted by d1 through d N. And for each pair of category and the document,\nwe can then look at the situation. And see if the system has\nsaid yes to this pair, basically has assigned this\ncategory to this document. Or no, so this is denoted by Y or M,\nthat's the systems of the decision. And similarly, we can look at the human's\ndecisions also, if the human has assigned a category to the document of that\nthere will be a plus sign here. That just means that a human. We think of this assignment is correct and\nincorrect then it's a minus. So we'll see all combinations of this Ns,\nyes and nos, minus and pluses. There are four combinations in total. And two of them are correct, and\nthat's when we have y(+) or n(-), and then there are also\ntwo kinds of errors. So the measure of classification\naccuracy is simply to count how many of these decisions are correct. And normalize that by the total\nnumber of decisions we have made. So, we know that the total number\nof decisions is n, multiplied by k. And, the number of correct decisions\nare basically of two kinds. One is y plusses. And the other is n minus this n. We just put together the count. Now, this is a very convenient\nmeasure that will give us one number to characterize performance of a method. And the higher, the better, of course. But the method also has some problems. First it has treated all\nthe decisions equally. But in reality, some decision errors\nare more serious than others. For example, it may be more important to\nget the decisions right on some documents, than others. Or maybe, more important to get\nthe decisions right on some categories, than others, and this would call for some detailed evaluation of this\nresults to understand the strands and of different methods, and to understand\nthe performance of these methods. In detail in a per category or per document basis. One example that shows clearly\nthe decision errors are having different causes is spam filtering that could be\nretrieved as two category categorization problem. Missing a legitimate email result,\nis one type of error. But letting spam to come into your\nfolder is another type of error. The two types of errors\nare clearly very different, because it's very important not\nto miss a legitimate email. It's okay to occasionally let a spam\nemail to come into your inbox. So the error of the first, missing a\nlegitimate email is very, is of high cost. It's a very serious mistake and classification error, classification\naccuracy does not address this issue. There's also another problem\nwith imbalance to test set. Imagine there's a skew to test set where\nmost instances are category one and 98% of instances are category one. Only 2% are in category two. In such a case, we can have a very\nsimple baseline that accurately performs very well and that baseline. Sign with similar,\nI put all instances in the major category. That will get us 98%\naccuracy in this case. It's going to be appearing to be\nvery effective, but in reality, this is obviously not a good result. And so, in general, when we use\nclassification accuracy as a measure, we want to ensure that\nthe causes of balance. And one above equal number of instances,\nfor example in each class the minority\ncategories or causes tend to be overlooked in the evaluation\nof classification accuracy. So, to address these problems,\nwe of course would like to also evaluate the results in other ways and\nin different ways. As I said, it's beneficial to look\nat after multiple perspectives. So for example, we can look at\nthe perspective from each document as a perspective based on each document. So the question here is, how good\nare the decisions on this document? Now, as in the general cases of all\ndecisions, we can think about four combinations of possibilities, depending\non whether the system has said yes and depending on whether the human has said it\ncorrect or incorrect or said yes or no. And so the four combinations are first\nwhen both the human systems said yes, and that's the true positives, when the system\nsays, yes, and it's after the positive. So, when the system says,\nyes, it's a positive. But, when the human confirm\nthat it is indeed correct, that becomes a true positive. When the system says, yes,\nbut the human says, no, that's incorrect,\nthat's a false positive, have FP. And when the system says no, but the human\nsays yes, then it's a false negative. We missed one assignment. When both the system and human says no, then it's also correctly\nto assume that's true negatives. All right, so\nthen we can have some measures to just better characterize the performance\nby using these four numbers and so two popular measures are precision and\nrecall. And these were also proposed by\ninformation retrieval researchers 1960s for evaluating search results, but now they have become standard measures,\nuse it everywhere. So when the system says yes, we can ask\nthe question, how many are correct? What's the percent of correct\ndecisions when the system says yes? That's called precision. It's true positive divided by all\nthe cases when the system says yes, all the positives. The other measure is called recall,\nand this measures whether the document has all\nthe categories it should have. So in this case it's divide the true\npositive by true positives and the false negatives. So these are all the cases where this human Says the document\nshould have this category. So this represents both categories\nthat it should have got, and so recall tells us whether\nthe system has actually indeed assigned all the categories that\nit should have to this document. This gives us a detailed\nview of the document, then we can aggregate them later. And if we're interested in some documents,\nand this will tell us how well we did on\nthose documents, the subsets of them. It might be more interesting than others,\nfor example. And this allows us to analyze\nerrors in more detail as well. We can separate the documents of certain\ncharacteristics from others, and then look at the errors. You might see a pattern A for\nthis kind of document, this long document. It doesn't as well for shock documents. And this gives you some insight for\ninputting the method. Similarly, we can look at\nthe per-category evaluation. In this case, we're going to look at the how good are\nthe decisions on a particular category. As in the previous case we can\ndefine precision and recall. And it would just basically answer the\nquestions from a different perspective. So when the system says yes,\nhow many are correct? That means looking at this category\nto see if all the documents that are assigned with this category\nare indeed in this category, right? And recall, would tell us,\nhas the category been actually assigned to all the documents That\nshould have this category. It's sometimes also useful to combine\nprecision and recall as one measure, and this is often done by using f measure. And this is just a harmonic\nmean of precision. Precision and\nrecall defined on this slide. And it's also controlled\nby a parameter beta to indicate whether precision is\nmore important or recall is more. When beta is set to 1,\nwe have measure called F1, and in this case, we just take equal\nweight upon both procedure and recall. F1 is very often used as a measure for\ncategorization. Now, as in all cases, when we combine\nresults, you always should think about the best way of combining them, so in this\ncase I don't know if you have thought about it and we could have combined\nthem just with arithmetic mean, right. So that would still give us\nthe same range of values, but obviously there's a reason why we didn't\ndo that and why f1 is more popular, and it's actually useful\nto think about difference. And we think about that, you'll see\nthat there is indeed some difference and some undesirable property\nof this arithmatic. Basically, it will be obvious\nto you if you think about a case when the system says yes for\nall the category and document pairs. And then try the compute the precision and\nrecall in that case. And see what would happen. And basically, this kind of measure, the arithmetic mean, is not going to be as reasonable as F1 minus one\n[INAUDIBLE] trade off, so that the two values are equal. There is an extreme case where you have\n0 for one letter and one for the other. Then F1 will be low, but\nthe mean would still be reasonably high. [MUSIC]",
 "04_5-4-text-categorization-evaluation-part-2.en.txt": "[SOUND] This lecture is\na continued discussion of evaluation of text categorization. Earlier we have introduced measures that\ncan be used with computer provision and recall. For each category and each document\nnow in this lecture we're going to further examine how to combine the\nperformance of the different categories of different documents how to aggregate them,\nhow do we take average? You see on the title here I indicated\nit's called a macro average and this is in contrast to micro average\nthat we'll talk more about later. So, again, for each category we're going\nto compute the precision require an f1 so for example category c1 we have\nprecision p1, recall r1 and F value f1. And similarly we can do that for category\n2 and and all the other categories. Now once we compute that and\nwe can aggregate them, so for example we can aggregate\nall the precision values. For all the categories, for\ncomputing overall precision. And this is often very useful to summarize\nwhat we have seen in the whole data set. And aggregation can be\ndone many different ways. Again as I said, in a case when you\nneed to aggregate different values, it's always good to think about what's\nthe best way of doing the aggregation. For example, we can consider arithmetic\nmean, which is very commonly used, or you can use geometric mean,\nwhich would have different behavior. Depending on the way you aggregate,\nyou might have got different conclusions. in terms of which method works better,\nso it's important to consider these differences and choosing the right one or\na more suitable one for your task. So the difference fore example\nbetween arithmetically and geometrically is that the arithmetically\nwould be dominated by high values whereas geometrically would\nbe more affected by low values. Base and so whether you are want\nto emphasis low values or high values would be a question\nrelate with all you And similar we can do that for\nrecal and F score. So that's how we can generate the overall\nprecision, recall and F score. Now we can do the same for aggregation\nof other all the document All right. So it's exactly the same situation for\neach document on our computer. Precision, recall, and F. And then after we have completed\nthe computation for all these documents, we're going to aggregate them to generate\nthe overall precision, overall recall, and overall F score. These are, again, examining\nthe results from different angles. Which one's more useful will\ndepend on your application. In general, it's beneficial to look at\nthe results from all these perspectives. And especially if you compare different\nmethods in different dimensions, it might reveal which method\nIs better in which measure or in what situations and\nthis provides insightful. Understanding the strands of a method or\na weakness and this provides further insight for\nimproving them. So as I mentioned,\nthere is also micro-average in contrast to the macro average\nthat we talked about earlier. In this case, what we do is you\npool together all the decisions, and then compute the precision and recall. So we can compute the overall\nprecision and recall by just counting how many cases are in true positive,\nhow many cases in false positive, etc, it's computing the values\nin the contingency table, and then we can compute the precision and\nrecall just once. In contrast, in macro-averaging, we're\ngoing to do that for each category first. And then aggregate over these categories\nor we do that for each document and then aggregate all the documents but\nhere we pooled them together. Now this would be very similar to\nthe classification accuracy that we used earlier, and one problem here of course to treat all\nthe instances, all the decisions equally. And this may not be desirable. But it may be a property for\nsome applications, especially if we associate the, for\nexample, the cost for each combination. Then we can actually compute for example,\nweighted classification accuracy. Where you associate the different cost or\nutility for each specific decision, so there could be variations of these\nmethods that would be more useful. But in general macro average tends to\nbe more information than micro average, just because it might reflect the need for\nunderstanding performance on each category or performance on each\ndocument which are needed in applications. But macro averaging and micro averaging,\nthey are both very common, and you might see both reported in\nresearch papers on Categorization. Also sometimes categorization\nresults might actually be evaluated from ranking prospective. And this is because categorization\nresults are sometimes or often indeed passed it to a human for\nvarious purposes. For example, it might be passed\nto humans for further editing. For example, news articles can be tempted\nto be categorized by using a system and then human editors would\nthen correct them. And all the email messages might be\nthroughout to the right person for handling in the help desk. And in such a case the categorizations\nwill help prioritizing the task for\nparticular customer service person. So, in this case the results\nhave to be prioritized and if the system can't give a score\nto the categorization decision for confidence then we can use the scores\nto rank these decisions and then evaluate the results as a rank list,\njust as in a search engine. Evaluation where you rank\nthe documents in responsible query. So for example a discovery of\nspam emails can be evaluated based on ranking emails for\nthe spam category. And this is useful if you want people\nto to verify whether this is really spam, right? The person would then take\nthe rank To check one by one and then verify whether this is indeed a spam. So to reflect the utility for\nhumans in such a task, it's better to evaluate Ranking Chris and this\nis basically similar to a search again. And in such a case often\nthe problem can be better formulated as a ranking problem\ninstead of a categorization problem. So for example, ranking documents in\na search engine can also be framed as a binary categorization problem,\ndistinguish the relevant documents that are useful to users from those that\nare not useful, but typically we frame this as a ranking problem,\nand we evaluate it as a rank list. That's because people tend\nto examine the results so ranking evaluation more reflects\nutility from user's perspective. So to summarize categorization evaluation, first evaluation is always very\nimportant for all these tasks. So get it right. If you don't get it right,\nyou might get misleading results. And you might be misled to believe\none method is better than the other, which is in fact not true. So it's very important to get it right. Measures must also reflect\nthe intended use of the results for a particular application. For example, in spam filtering and news categorization the results\nare used in maybe different ways. So then we would need to\nconsider the difference and design measures appropriately. We generally need to consider how will the\nresults be further processed by the user and think from a user's perspective. What quality is important? What aspect of quality is important? Sometimes there are trade offs between\nmultiple aspects like precision and recall and so we need to know for this\napplication is high recall more important, or high precision is more important. Ideally we associate the different cost\nwith each different decision arrow. And this of course has to be designed\nin an application specific way. Some commonly used measures for relative\ncomparison methods are the following. Classification accuracy, it's very\ncommonly used for especially balance. [INAUDIBLE] preceding [INAUDIBLE]\nScores are common and report characterizing performances,\ngiven angles and give us some [INAUDIBLE] like a [INAUDIBLE] Per\ndocument basis [INAUDIBLE] And then take a average of all of them, different\nways micro versus macro [INAUDIBLE]. In general, you want to look at the\nresults from multiple perspectives and for particular applications some perspectives\nwould be more important than others but diagnoses and\nanalysis of categorization methods. It's generally useful to look at\nas many perspectives as possible to see subtle differences between methods\nor tow see where a method might be weak from which you can obtain sight for\nimproving a method. Finally sometimes ranking\nmay be more appropriate so be careful sometimes categorization has\ngot may be better frame as a ranking tasks and there're machine running methods for\noptimizing ranking measures as well. So here are two suggested readings. One is some chapters of this book where\nyou can find more discussion about evaluation measures. The second is a paper about\ncomparison of different approaches to text categorization and it also has an excellent discussion of\nhow to evaluate textual categorization. [MUSIC]",
 "05_5-5-opinion-mining-and-sentiment-analysis-motivation.en.txt": "[SOUND] This lecture is about, Opinion Mining and Sentiment Analysis, covering, Motivation. In this lecture,\nwe're going to start, talking about, mining a different kind of knowledge. Namely, knowledge about the observer or\nhumans that have generated the text data. In particular, we're going to talk about\nthe opinion mining and sentiment analysis. As we discussed earlier, text data\ncan be regarded as data generated from humans as subjective sensors. In contrast, we have other devices such\nas video recorder that can report what's happening in the real world objective to\ngenerate the viewer data for example. Now the main difference between test\ndata and other data, like video data, is that it has rich opinions, and the content tends to be subjective\nbecause it's generated from humans. Now, this is actually a unique advantaged\nof text data, as compared with other data, because the office is a great\nopportunity to understand the observers. We can mine text data to\nunderstand their opinions. Understand people's preferences,\nhow people think about something. So this lecture and the following lectures\nwill be mainly about how we can mine and analyze opinions buried\nin a lot of text data. So let's start with\nthe concept of opinion. It's not that easy to\nformally define opinion, but mostly we would define\nopinion as a subjective statement describing what a person\nbelieves or thinks about something. Now, I highlighted quite a few words here. And that's because it's worth thinking\na little bit more about these words. And that will help us better\nunderstand what's in an opinion. And this further helps us to\ndefine opinion more formally. Which is always needed to computation to\nresolve the problem of opinion mining. So let's first look at the key\nword of subjective here. This is in contrast with objective\nstatement or factual statement. Those statements can be proved right or\nwrong. And this is a key differentiating\nfactor from opinions which tends to be not\neasy to prove wrong or right, because it reflects what\nthe person thinks about something. So in contrast, objective statement can\nusually be proved wrong or correct. For example, you might say this\ncomputer has a screen and a battery. Now that's something you can check. It's either having a battery or not. But in contrast with this, think about\nthe sentence such as, this laptop has the best battery or\nthis laptop has a nice screen. Now these statements\nare more subjective and it's very hard to prove\nwhether it's wrong or correct. So opinion, is a subjective statement. And next lets look at\nthe keyword person here. And that indicates that\nis an opinion holder. Because when we talk about opinion,\nit's about an opinion held by someone. And then we notice that\nthere is something here. So that is the target of the opinion. The opinion is expressed\non this something. And now, of course, believes or\nthinks implies that an opinion will depend on the culture or\nbackground and the context in general. Because a person might think\ndifferent in a different context. People from different background\nmay also think in different ways. So this analysis shows that there are\nmultiple elements that we need to include in order to characterize opinion. So, what's a basic opinion\nrepresentation like? Well, it should include at\nleast three elements, right? Firstly, it has to specify\nwhat's the opinion holder. So whose opinion is this? Second, it must also specify the target,\nwhat's this opinion about? And third, of course,\nwe want opinion content. And so what exactly is opinion? If you can identify these, we get a basic understanding of opinion\nand can already be useful sometimes. You want to understand further,\nwe want enriched opinion representation. And that means we also want to\nunderstand that, for example, the context of the opinion and\nwhat situation was the opinion expressed. For example, what time was it expressed? We, also, would like to, people understand\nthe opinion sentiment, and this is to understand that what the opinion tells\nus about the opinion holder's feeling. For example, is this opinion positive,\nor negative? Or perhaps the opinion holder was happy or\nwas sad, and so such understanding obvious\nto those beyond just Extracting the opinion content,\nit needs some analysis. So let's take a simple\nexample of a product review. In this case, this actually expressed the\nopinion holder, and expressed the target. So its obviously whats opinion holder and that's just reviewer and its also often\nvery clear whats the opinion target and that's the product review for\nexample iPhone 6. When the review is posted usually\nyou can't such information easier. Now the content, of course,\nis a review text that's, in general, also easy to obtain. So you can see product reviews are fairly easy to analyze in terms of obtaining\na basic opinion of representation. But of course, if you want to get more\ninformation, you might know the Context, for example. The review was written in 2015. Or, we want to know that the sentiment\nof this review is positive. So, this additional understanding of\ncourse adds value to mining the opinions. Now, you can see in this case the task\nis relatively easy and that's because the opinion holder and the opinion\ntarget have already been identified. Now let's take a look at\nthe sentence in the news. In this case, we have a implicit\nholder and a implicit target. And the tasker is in general harder. So, we can identify opinion holder here,\nand that's the governor of Connecticut. We can also identify the target. So one target is Hurricane Sandy, but there is also another target\nmentioned which is hurricane of 1938. So what's the opinion? Well, there's a negative sentiment here that's indicated by words like bad and\nworst. And we can also, then, identify context,\nNew England in this case. Now, unlike in the playoff review, all these elements must be extracted by\nusing natural RAM processing techniques. So, the task Is much harder. And we need a deeper natural\nlanguage processing. And these examples also suggest that a lot of work can be\neasy to done for product reviews. That's indeed what has happened. Analyzing and\nassembling news is still quite difficult, it's more difficult than the analysis\nof opinions in product reviews. Now there are also some other\ninteresting variations. In fact, here we're going to\nexamine the variations of opinions, more systematically. First, let's think about\nthe opinion holder. The holder could be an individual or\nit could be group of people. Sometimes, the opinion\nwas from a committee. Or from a whole country of people. Opinion target accounts will vary a lot. It can be about one entity,\na particular person, a particular product, a particular policy, ect. But it could be about a group of products. Could be about the products\nfrom a company in general. Could also be very specific\nabout one attribute, though. An attribute of the entity. For example,\nit's just about the battery of iPhone. It could be someone else's opinion. And one person might comment on\nanother person's Opinion, etc. So, you can see there is a lot of\nvariation here that will cause the problem to vary a lot. Now, opinion content, of course,\ncan also vary a lot on the surface, you can identify one-sentence opinion or\none-phrase opinion. But you can also have longer\ntext to express an opinion, like the whole article. And furthermore we identify\nthe variation in the sentiment or emotion damage that's above\nthe feeding of the opinion holder. So, we can distinguish a positive\nversus negative or mutual or happy versus sad, separate. Finally, the opinion\ncontext can also vary. We can have a simple context, like\ndifferent time or different locations. But there could be also complex contexts, such as some background\nof topic being discussed. So when opinion is expressed in\nparticular discourse context, it has to be interpreted in different ways than\nwhen it's expressed in another context. So the context can be very [INAUDIBLE] to\nentire discourse context of the opinion. From computational perspective, we're mostly interested in what opinions\ncan be extracted from text data. So, it turns out that we can\nalso differentiate, distinguish, different kinds of opinions in text\ndata from computation perspective. First, the observer might make\na comment about opinion targeting, observe the word So\nin case we have the author's opinion. For example,\nI don't like this phone at all. And that's an opinion of this author. In contrast, the text might also\nreport opinions about others. So the person could also Make observation\nabout another person's opinion and reported this opinion. So for example,\nI believe he loves the painting. And that opinion is really about the It is\nreally expressed by another person here. So, it doesn't mean this\nauthor loves that painting. So clearly, the two kinds of opinions\nneed to be analyzed in different ways, and sometimes in product reviews, you can see, although mostly the opinions\nare false from this reviewer. Sometimes, a reviewer might mention\nopinions of his friend or her friend. Another complication is that\nthere may be indirect opinions or inferred opinions that can be obtained. By making inferences on what's expressed in the text that might\nnot necessarily look like opinion. For example, one statement that might be, this phone ran out of\nbattery in just one hour. Now, this is in a way a factual statement\nbecause It's either true or false, right? You can even verify that,\nbut from this statement, one can also infer some negative opinions\nabout the quality of the battery of this phone, or the feeling of\nthe opinion holder about the battery. The opinion holder clearly wished\nthat the battery do last longer. So these are interesting variations\nthat we need to pay attention to when we extract opinions. Also, for\nthis reason about indirect opinions, it's often also very useful to extract\nwhatever the person has said about the product, and sometimes factual\nsentences like these are also very useful. So, from a practical viewpoint, sometimes we don't necessarily\nextract the subject of sentences. Instead, again, all the sentences that\nare about the opinions are useful for understanding the person or\nunderstanding the product that we commend. So the task of opinion mining can be\ndefined as taking textualized input to generate a set of\nopinion representations. Each representation we should\nidentify opinion holder, target, content, and the context. Ideally we can also infer opinion\nsentiment from the comment and the context to better understand. The opinion. Now often, some elements of\nthe representation are already known. I just gave a good example in\nthe case of product we'd use where the opinion holder and the opinion\ntarget are often expressly identified. And that's not why this turns out to be\none of the simplest opinion mining tasks. Now, it's interesting to think about\nthe other tasks that might be also simple. Because those are the cases\nwhere you can easily build applications by using\nopinion mining techniques. So now that we have talked about what is\nopinion mining, we have defined the task. Let's also just talk a little bit about\nwhy opinion mining is very important and why it's very useful. So here, I identify three major reasons,\nthree broad reasons. The first is it can help decision support. It can help us optimize our decisions. We often look at other people's opinions,\nlook at read the reviews in order to make a decisions like\nbuying a product or using a service. We also would be interested\nin others opinions when we decide whom to vote for example. And policy makers, may also want to know people's\nopinions when designing a new policy. So that's one general,\nkind of, applications. And it's very broad, of course. The second application is to understand\npeople, and this is also very important. For example, it could help\nunderstand people's preferences. And this could help us\nbetter serve people. For example, we optimize a product search\nengine or optimize a recommender system if we know what people are interested in,\nwhat people think about product. It can also help with advertising,\nof course, and we can have targeted advertising if we know what kind of\npeople tend to like what kind of plot. Now the third kind of application\ncan be called voluntary survey. Now this is most important research\nthat used to be done by doing surveys, doing manual surveys. Question, answer it. People need to feel informs\nto answer their questions. Now this is directly related to humans\nas sensors, and we can usually aggregate opinions from a lot of humans through\nkind of assess the general opinion. Now this would be very useful for\nbusiness intelligence where manufacturers want to know where their products\nhave advantages over others. What are the winning\nfeatures of their products, winning features of competitive products. Market research has to do with\nunderstanding consumers oppinions. And this create very useful directive for\nthat. Data-driven social science research\ncan benefit from this because they can do text mining to understand\nthe people's opinions. And if you can aggregate a lot of opinions\nfrom social media, from a lot of, popular information then you can actually\ndo some study of some questions. For example, we can study the behavior of\npeople on social media on social networks. And these can be regarded as voluntary\nsurvey done by those people. In general, we can gain a lot of advantage\nin any prediction task because we can leverage the text data as\nextra data above any problem. And so we can use text based\nprediction techniques to help you make predictions or\nimprove the accuracy of prediction. [MUSIC]",
 "06_5-6-opinion-mining-and-sentiment-analysis-sentiment-classification.en.txt": "[NOISE]\nThis lecture is about\nthe sentiment classification. If we assume that most of the elements in the opinion\nrepresentation are all ready known, then our only task may be just a sentiment\nclassification, as shown in this case. So suppose we know who's the opinion\nholder and what's the opinion target, and also know the content and the context\nof the opinion, then we mainly need to decide the opinion\nsentiment of the review. So this is a case of just using sentiment\nclassification for understanding opinion. Sentiment classification can be\ndefined more specifically as follows. The input is opinionated text object,\nthe output is typically a sentiment label, or a sentiment tag, and\nthat can be designed in two ways. One is polarity analysis, where we have\ncategories such as positive, negative, or neutral. The other is emotion\nanalysis that can go beyond a polarity to characterize\nthe feeling of the opinion holder. In the case of polarity analysis,\nwe sometimes also have numerical ratings as you\noften see in some reviews on the web. Five might denote the most positive, and\none maybe the most negative, for example. In general, you have just disk holder\ncategories to characterize the sentiment. In emotion analysis, of course,\nthere are also different ways for design the categories. The six most frequently\nused categories are happy, sad, fearful, angry,\nsurprised, and disgusted. So as you can see, the task is essentially\na classification task, or categorization task, as we've seen before, so it's\na special case of text categorization. This also means any textual categorization\nmethod can be used to do sentiment classification. Now of course if you just do that,\nthe accuracy may not be good because sentiment classification\ndoes requires some improvement over regular text categorization technique,\nor simple text categorization technique. In particular,\nit needs two kind of improvements. One is to use more sophisticated features\nthat may be more appropriate for sentiment tagging as I\nwill discuss in a moment. The other is to consider\nthe order of these categories, and especially in polarity analysis,\nit's very clear there's an order here, and so these categories\nare not all that independent. There's order among them, and so\nit's useful to consider the order. For example, we could use\nordinal regression to do that, and that's something that\nwe'll talk more about later. So now, let's talk about some features\nthat are often very useful for text categorization and\ntext mining in general, but some of them are especially also\nneeded for sentiment analysis. So let's start from the simplest one,\nwhich is character n-grams. You can just have a sequence\nof characters as a unit, and they can be mixed with different n's,\ndifferent lengths. All right, and\nthis is a very general way and very robust way to\nrepresent the text data. And you could do that for\nany language, pretty much. And this is also robust to spelling\nerrors or recognition errors, right? So if you misspell a word by one character\nand this representation actually would allow you to match this word when\nit occurs in the text correctly. Right, so misspell the word and\nthe correct form can be matched because they contain some common\nn-grams of characters. But of course such a recommendation\nwould not be as discriminating as words. So next, we have word n-grams,\na sequence of words and again, we can mix them with different n's. Unigram's are actually often very\neffective for a lot of text processing tasks, and it's mostly because words\nare word designed features by humans for communication, and so\nthey are often good enough for many tasks. But it's not good, or not sufficient for\nsentiment analysis clearly. For example, we might see a sentence like, it's not good or\nit's not as good as something else, right? So in such a case if you\njust take a good and that would suggest positive that's not\ngood, all right so it's not accurate. But if you take a bigram, not good\ntogether, and then it's more accurate. So longer n-grams are generally more\ndiscriminative, and they're more specific. If you match it, and it says a lot, and it's accurate it's unlikely,\nvery ambiguous. But it may cause overfitting because with\nsuch very unique features that machine oriented program can easily pick up\nsuch features from the training set and to rely on such unique features\nto distinguish the categories. And obviously, that kind of classify, one\nwould generalize word to future there when such discriminative features\nwill not necessarily occur. So that's a problem of\noverfitting that's not desirable. We can also consider part of speech tag,\nn-grams if we can do part of speech tagging an, for example,\nadjective noun could form a pair. We can also mix n-grams of words and\nn-grams of part of speech tags. For example, the word great might be\nfollowed by a noun, and this could become a feature, a hybrid feature, that could\nbe useful for sentiment analysis. So next we can also have word classes. So these classes can be syntactic like a\npart of speech tags, or could be semantic, and they might represent concepts in\nthe thesaurus or ontology, like WordNet. Or they can be recognized the name\nentities, like people or place, and these categories can be used to enrich\nthe presentation as additional features. We can also learn word clusters and\nparodically, for example, we've talked about the mining\nassociations of words. And so we can have cluster of\nparadigmatically related words or syntaxmatically related words, and these clusters can be features to\nsupplement the word base representation. Furthermore, we can also have\nfrequent pattern syntax, and these could be frequent word set,\nthe words that form the pattern do not necessarily\noccur together or next to each other. But we'll also have locations where the words my occur more closely together,\nand such patterns provide a more discriminative\nfeatures than words obviously. And they may also generalize better\nthan just regular n-grams because they are frequent. So you expected them to\noccur also in tested data. So they have a lot of advantages, but\nthey might still face the problem of overfeeding as the features\nbecome more complex. This is a problem in general, and the same\nis true for parse tree-based features, when you can use a parse tree to derive\nfeatures such as frequent subtrees, or paths, and\nthose are even more discriminating, but they're also are more likely\nto cause over fitting. And in general, pattern discovery\nalgorithm's are very useful for feature construction because they allow\nus to search in a large space of possible features that are more complex than\nwords that are sometimes useful. So in general, natural language\nprocessing is very important that they derive complex features, and\nthey can enrich text representation. So for example, this is a simple sentence that I showed\nyou a long time ago in another lecture. So from these words we can only\nderive simple word n-grams, representations or character n-grams. But with NLP,\nwe can enrich the representation with a lot of other information such\nas part of speech tags, parse trees or entities, or even speech act. Now with such enriching information\nof course, then we can generate a lot of other features, more complex features\nlike a mixed grams of a word and the part of speech tags, or\neven a part of a parse tree. So in general, feature design actually\naffects categorization accuracy significantly, and it's a very important\npart of any machine learning application. In general, I think it would be\nmost effective if you can combine machine learning, error analysis, and\ndomain knowledge in design features. So first you want to\nuse the main knowledge, your understanding of the problem,\nthe design seed features, and you can also define a basic feature space\nwith a lot of possible features for the machine learning program to work on,\nand machine can be applied to select the most effective features or\nconstruct the new features. That's feature learning, and these features can then be further\nanalyzed by humans through error analysis. And you can look at\nthe categorization errors, and then further analyze what features can\nhelp you recover from those errors, or what features cause overfitting and\ncause those errors. And so this can lead into\nfeature validation that will revised the feature set,\nand then you can iterate. And we might consider using\na different features space. So NLP enriches text\nrecognition as I just said, and because it enriches the feature space, it allows much larger such a space\nof features and there are also many, many more features that can be\nvery useful for a lot of tasks. But be careful not to use a lot\nof category features because it can cause overfitting,\nor otherwise you would have to training careful\nnot to let overflow happen. So a main challenge in design features, a common challenge is to optimize\na trade off between exhaustivity and the specificity, and this trade off\nturns out to be very difficult. Now exhaustivity means we want\nthe features to actually have high coverage of a lot of documents. And so in that sense,\nyou want the features to be frequent. Specifity requires the feature\nto be discriminative, so naturally infrequent the features\ntend to be more discriminative. So this really cause a trade off between frequent versus infrequent features. And that's why a featured\ndesign is usually odd. And that's probably the most important\npart in machine learning any problem in particularly in our case,\nfor text categoration or more specifically\nthe senitment classification. [MUSIC]",
 "07_5-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression.en.txt": "[NOISE] This lecture is about the ordinal logistic regression for\nsentiment analysis. So, this is our problem set up for a\ntypical sentiment classification problem. Or more specifically a rating prediction. We have an opinionated text document d as\ninput, and we want to generate as output, a rating in the range of 1 through k so it's a discrete rating, and\nthis is a categorization problem. We have k categories here. Now we could use a regular text for categorization technique\nto solve this problem. But such a solution would not consider the\norder and dependency of the categories. Intuitively, the features that can\ndistinguish category 2 from 1, or rather rating 2 from 1,\nmay be similar to those that can distinguish k from k-1. For example, positive words\ngenerally suggest a higher rating. When we train categorization problem by treating these categories as\nindependent we would not capture this. So what's the solution? Well in general we can order to classify\nand there are many different approaches. And here we're going to\ntalk about one of them that called ordinal logistic regression. Now, let's first think about how\nwe use logistical regression for a binary sentiment. A categorization problem. So suppose we just wanted to distinguish\na positive from a negative and that is just a two category\ncategorization problem. So the predictors are represented as X and\nthese are the features. And there are M features all together. The feature value is a real number. And this can be representation\nof a text document. And why it has two values,\nbinary response variable 0 or 1. 1 means X is positive,\n0 means X is negative. And then of course this is a standard\ntwo category categorization problem. We can apply logistical regression. You may recall that in logistical\nregression, we assume the log of probability that the Y is equal to one,\nis assumed to be a linear function\nof these features, as shown here. So this would allow us to also write\nthe probability of Y equals one, given X in this equation that you\nare seeing on the bottom. So that's a logistical function and you can see it relates\nthis probability to, probability that y=1\nto the feature values. And of course beta i's\nare parameters here, so this is just a direct application of logistical\nregression for binary categorization. What if we have multiple categories,\nmultiple levels? Well we have to use such a binary\nlogistical regression problem to solve this multi\nlevel rating prediction. And the idea is we can introduce\nmultiple binary class files. In each case we asked\nthe class file to predict the, whether the rating is j or above,\nor the rating's lower than j. So when Yj is equal to 1,\nit means rating is j or above. When it's 0,\nthat means the rating is Lower than j. So basically if we want to predict\na rating in the range of 1-k, we first have one classifier to\ndistinguish a k versus others. And that's our classifier one. And then we're going to have another\nclassifier to distinguish it. At k-1 from the rest. That's Classifier 2. And in the end, we need a Classifier\nto distinguish between 2 and 1. So altogether we'll have k-1 classifiers. Now if we do that of course then\nwe can also solve this problem and the logistical regression program\nwill be also very straight forward as you have just seen\non the previous slide. Only that here we have more parameters. Because for each classifier,\nwe need a different set of parameters. So now the logistical regression\nclassifies index by J, which corresponds to a rating level. And I have also used of\nJ to replace beta 0. And this is to. Make the notation more consistent, than was what we can show in\nthe ordinal logistical regression. So here we now have basically k minus one\nregular logistic regression classifiers. Each has it's own set of parameters. So now with this approach,\nwe can now do ratings as follows. After we have trained these k-1\nlogistic regression classifiers, separately of course,\nthen we can take a new instance and then invoke a classifier\nsequentially to make the decision. So first let look at the classifier\nthat corresponds to level of rating K. So this classifier will tell\nus whether this object should have a rating of K or about. If probability according to this\nlogistical regression classifier is larger than point five,\nwe're going to say yes. The rating is K. Now, what if it's not as\nlarge as twenty-five? Well, that means the rating's below K,\nright? So now,\nwe need to invoke the next classifier, which tells us whether\nit's above K minus one. It's at least K minus one. And if the probability is\nlarger than twenty-five, then we'll say, well, then it's k-1. What if it says no? Well, that means the rating\nwould be even below k-1. And so we're going to just keep\ninvoking these classifiers. And here we hit the end when we need\nto decide whether it's two or one. So this would help us solve the problem. Right? So we can have a classifier that would\nactually give us a prediction of a rating in the range of 1 through k. Now unfortunately such a strategy is not\nan optimal way of solving this problem. And specifically there are two\nproblems with this approach. So these equations are the same as. You have seen before. Now the first problem is that there\nare just too many parameters. There are many parameters. Now, can you count how many\nparameters do we have exactly here? Now this may be a interesting exercise. To do.\nSo you might want to just pause the video and\ntry to figure out the solution. How many parameters do I have for\neach classifier? And how many classifiers do we have? Well you can see the, and so\nit is that for each classifier we have n plus one parameters, and we have k\nminus one classifiers all together, so the total number of parameters is\nk minus one multiplied by n plus one. That's a lot. A lot of parameters, so when\nthe classifier has a lot of parameters, we would in general need a lot of data\nout to actually help us, training data, to help us decide the optimal\nparameters of such a complex model. So that's not ideal. Now the second problems\nis that these problems, these k minus 1 plus fives,\nare not really independent. These problems are actually dependent. In general, words that are positive\nwould make the rating higher for any of these classifiers. For all these classifiers. So we should be able to take\nadvantage of this fact. Now the idea of ordinal logistical\nregression is precisely that. The key idea is just\nthe improvement over the k-1 independent logistical\nregression classifiers. And that idea is to tie\nthese beta parameters. And that means we are going to\nassume the beta parameters. These are the parameters that indicated\nthe inference of those weights. And we're going to assume these\nbeta values are the same for all the K- 1 parameters. And this just encodes our intuition that, positive words in general would\nmake a higher rating more likely. So this is intuitively assumptions,\nso reasonable for our problem setup. And we have this order\nin these categories. Now in fact, this would allow us\nto have two positive benefits. One is it's going to reduce\nthe number of families significantly. And the other is to allow us\nto share the training data. Because all these parameters\nare similar to be equal. So these training data, for\ndifferent classifiers can then be shared to help us set\nthe optimal value for beta. So we have more data to help\nus choose a good beta value. So what's the consequence, well the formula would look very similar\nto what you have seen before only that, now the beta parameter has just one\nindex that corresponds to the feature. It no longer has the other index that\ncorresponds to the level of rating. So that means we tie them together. And there's only one set of better\nvalues for all the classifiers. However, each classifier still\nhas the distinct R for value. The R for parameter. Except it's different. And this is of course needed to predict\nthe different levels of ratings. So R for sub j is different it\ndepends on j, different than j, has a different R value. But the rest of the parameters,\nthe beta i's are the same. So now you can also ask the question,\nhow many parameters do we have now? Again, that's an interesting\nquestion to think about. So if you think about it for a moment, and you will see now, the param,\nwe have far fewer parameters. Specifically we have M plus K minus one. Because we have M, beta values, and\nplus K minus one of our values. So let's just look basically, that's basically the main idea of\nordinal logistical regression. So, now, let's see how we can use such\na method to actually assign ratings. It turns out that with this, this idea of\ntying all the parameters, the beta values. We also end up by having\na similar way to make decisions. And more specifically now, the criteria\nwhether the predictor probabilities are at least 0.5 above,\nand now is equivalent to whether the score of\nthe object is larger than or equal to negative authors of j,\nas shown here. Now, the scoring function is just\ntaking the linear combination of all the features with\nthe divided beta values. So, this means now we can simply make\na decision of rating, by looking at the value of this scoring function,\nand see which bracket it falls into. Now you can see the general\ndecision rule is thus, when the score is in the particular\nrange of all of our values, then we will assign the corresponding\nrating to that text object. So in this approach,\nwe're going to score the object by using the features and\ntrained parameter values. This score will then be\ncompared with a set of trained alpha values to see which\nrange the score is in. And then, using the range, we can then decide which\nrating the object should be getting. Because, these ranges of alpha\nvalues correspond to the different levels of ratings, and that's from\nthe way we train these alpha values. Each is tied to some level of rating. [MUSIC]",
 "01_6-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1.en.txt": "[MUSIC] This lecture is about the Latent Aspect\nRating Analysis for Opinion Mining and Sentiment Analysis. In this lecture, we're going to continue discussing\nOpinion Mining and Sentiment Analysis. In particular, we're going to introduce\nLatent Aspect Rating Analysis which allows us to perform detailed\nanalysis of reviews with overall ratings. So, first is motivation. Here are two reviews that you often\nsee in the net about the hotel. And you see some overall ratings. In this case,\nboth reviewers have given five stars. And, of course,\nthere are also reviews that are in text. Now, if you just look at these reviews, it's not very clear whether the hotel is\ngood for its location or for its service. It's also unclear why\na reviewer liked this hotel. What we want to do is to\ndecompose this overall rating into ratings on different aspects such as\nvalue, rooms, location, and service. So, if we can decompose\nthe overall ratings, the ratings on these different aspects,\nthen, we can obtain a more detailed understanding\nof the reviewer's opinionsabout the hotel. And this would also allow us to rank\nhotels along different dimensions such as value or rooms. But, in general, such detailed\nunderstanding will reveal more information about the user's preferences,\nreviewer's preferences. And also, we can understand better\nhow the reviewers view this hotel from different perspectives. Now, not only do we want to\ninfer these aspect ratings, we also want to infer the aspect weights. So, some reviewers may care more about\nvalues as opposed to the service. And that would be a case. like what's shown on the left for\nthe weight distribution, where you can see a lot of\nweight is places on value. But others care more for service. And therefore, they might place\nmore weight on service than value. The reason why this is\nalso important is because, do you think about a five star on value, it might still be very expensive if the\nreviewer cares a lot about service, right? For this kind of service,\nthis price is good, so the reviewer might give it a five star. But if a reviewer really cares\nabout the value of the hotel, then the five star, most likely,\nwould mean really cheap prices. So, in order to interpret the ratings\non different aspects accurately, we also need to know these aspect weights. When they're combined together, we can have a more detailed\nunderstanding of the opinion. So the task here is to get these reviews\nand their overall ratings as input, and then,\ngenerate both the aspect ratings, the compose aspect ratings, and\nthe aspect rates as output. And this is a problem called\nLatent Aspect Rating Analysis. So the task, in general,\nis given a set of review articles about the topic with overall ratings, and\nwe hope to generate three things. One is the major aspects\ncommented on in the reviews. Second is ratings on each aspect,\nsuch as value and room service. And third is the relative weights placed\non different aspects by the reviewers. And this task has a lot of applications,\nand if you can do this, and it will enable a lot of applications. I just listed some here. And later, I will show you some results. And, for example,\nwe can do opinion based entity ranking. We can generate an aspect-level\nopinion summary. We can also analyze reviewers preferences,\ncompare them or compare their preferences\non different hotels. And we can do personalized\nrecommendations of products. So, of course, the question is\nhow can we solve this problem? Now, as in other cases of\nthese advanced topics, we won\u2019t have time to really\ncover the technique in detail. But I\u2019m going to give you a brisk, basic introduction to the technique\ndevelopment for this problem. So, first step, we\u2019re going to talk about\nhow to solve the problem in two stages. Later, we\u2019re going to also mention that\nwe can do this in the unified model. Now, take this review with\nthe overall rating as input. What we want to do is, first,\nwe're going to segment the aspects. So we're going to pick out what words\nare talking about location, and what words are talking\nabout room condition, etc. So with this, we would be able\nto obtain aspect segments. In particular, we're going to\nobtain the counts of all the words in each segment, and\nthis is denoted by C sub I of W and D. Now this can be done by using seed\nwords like location and room or price to retrieve\nthe [INAUDIBLE] in the segments. And then, from those segments,\nwe can further mine correlated words with these seed words and\nthat would allow us to segmented the text into segments,\ndiscussing different aspects. But, of course, later, as we will see, we can also use\n[INAUDIBLE] models to do the segmentation. But anyway, that's the first stage, where the obtain the council\nof words in each segment. In the second stage, which is called Latent Rating Regression,\nwe're going to use these words and their frequencies in different\naspects to predict the overall rate. And this predicting happens in two stages. In the first stage,\nwe're going to use the [INAUDIBLE] and the weights of these words in each\naspect to predict the aspect rating. So, for example, if in your discussion\nof location, you see a word like, amazing, mentioned many times,\nand it has a high weight. For example, here, 3.9. Then, it will increase\nthe Aspect Rating for location. But, another word like, far,\nwhich is an acted weight, if it's mentioned many times,\nand it will decrease the rating. So the aspect ratings, assume that it\nwill be a weighted combination of these word frequencies where the weights\nare the sentiment weights of the words. Of course, these sentimental weights\nmight be different for different aspects. So we have, for each aspect, a set of\nterm sentiment weights as shown here. And that's in order by beta sub I and W. In the second stage or second step,\nwe're going to assume that the overall rating is simply a weighted\ncombination of these aspect ratings. So we're going to assume we have aspect\nweights to the [INAUDIBLE] sub i of d, and this will be used to take a weighted\naverage of the aspect ratings, which are denoted by r sub i of d. And we're going to assume the overall\nrating is simply a weighted average of these aspect ratings. So this set up allows us to predict\nthe overall rating based on the observable frequencies. So on the left side, you will see all these observed\ninformation, the r sub d and the count. But on the right side, you see all the information in\nthat range is actually latent. So, we hope to discover that. Now, this is a typical case of\na generating model where would embed the interesting variables\nin the generated model. And then, we're going to set up\na generation probability for the overall rating given\nthe observed words. And then, of course, we can adjust these\nparameter values including betas Rs and alpha Is in order to maximize\nthe probability of the data. In this case, the conditional probability\nof the observed rating given the document. So we have seen such cases before in, for example, PISA,\nwhere we predict a text data. But here, we're predicting the rating,\nand the parameters, of course, are very different. But we can see, if we can uncover\nthese parameters, it would be nice, because r sub i of d is precise as\nthe ratings that we want to get. And these are the composer\nratings on different aspects. [INAUDIBLE] sub I D is precisely\nthe aspect weights that we hope to get as a byproduct,\nthat we also get the beta factor, and these are the [INAUDIBLE] factor,\nthe sentiment weights of words. So more formally, the data we are modeling here is a set of\nreview documents with overall ratings. And each review document denote by a d,\nand the overall ratings denote by r sub d. And d pre-segments turn\ninto k aspect segments. And we're going to use ci(w,d) to denote\nthe count of word w in aspect segment i. Of course, it's zero if the word\ndoesn't occur in the segment. Now, the model is going to\npredict the rating based on d. So, we're interested in the provisional\nproblem of r sub-d given d. And this model is set up as follows. So r sub-d is assumed the two\nfollow a normal distribution doesn't mean that denotes\nactually await the average of the aspect of ratings r\nSub I of d as shown here. This normal distribution is\na variance of data squared. Now, of course,\nthis is just our assumption. The actual rating is not necessarily\nanything thing this way. But as always, when we make this\nassumption, we have a formal way to model the problem and that allows us\nto compute the interest in quantities. In this case, the aspect ratings and\nthe aspect weights. Now, the aspect rating as\nyou see on the [INAUDIBLE] is assuming that will be\na weight of sum of these weights. Where the weight is just\nthe [INAUDIBLE] of the weight. So as I said, the overall rating is assumed to be\na weighted average of aspect ratings. Now, these other values, r for\nsub I of D, or denoted together by other vector that depends on D is\nthat the token of specific weights. And we\u2019re going to assume that\nthis vector itself is drawn from another Multivariate Gaussian\ndistribution, with mean denoted by a Mu factor,\nand covariance metrics sigma here. Now, so this means, when we generate our\noverall rating, we're going to first draw a set of other values from this\nMultivariate Gaussian Prior distribution. And once we get these other values,\nwe're going to use then the weighted average of aspect ratings as\nthe mean here to use the normal distribution to generate\nthe overall rating. Now, the aspect rating, as I just said,\nis the sum of the sentiment weights of words in aspect, note that here the\nsentiment weights are specific to aspect. So, beta is indexed by i,\nand that's for aspect. And that gives us a way to model\ndifferent segment of a word. This is neither because\nthe same word might have positive sentiment for another aspect. It's also used for see what parameters\nwe have here beta sub i and w gives us the aspect-specific\nsentiment of w. So, obviously,\nthat's one of the important parameters. But, in general, we can see we have these\nparameters, beta values, the delta, and the Mu, and sigma. So, next, the question is, how can\nwe estimate these parameters and, so we collectively denote all\nthe parameters by lambda here. Now, we can, as usual,\nuse the maximum likelihood estimate, and this will give us the settings\nof these parameters, that with a maximized observed ratings\ncondition of their respective reviews. And of, course,\nthis would then give us all the useful variables that we\nare interested in computing. So, more specifically, we can now,\nonce we estimate the parameters, we can easily compute the aspect rating,\nfor aspect the i or sub i of d. And that's simply to take all of the words\nthat occurred in the segment, i, and then take their counts and then multiply that by the center of\nthe weight of each word and take a sum. So, of course, this time would be zero for\nwords that are not occurring in and that's why were going to take the sum\nof all the words in the vocabulary. Now what about the s factor weights? Alpha sub i of d, well,\nit's not part of our parameter. Right? So we have to use that to compute it. And in this case, we can use the Maximum a Posteriori to compute this alpha value. Basically, we're going to maximize the\nproduct of the prior of alpha according to our assumed Multivariate Gaussian\nDistribution and the likelihood. In this case,\nthe likelihood rate is the probability of generating this observed overall rating\ngiven this particular alpha value and some other parameters, as you see here. So for more details about this model,\nyou can read this paper cited here. [MUSIC]",
 "02_6-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2.en.txt": "[SOUND] This lecture is a continued discussion of\nLatent Aspect Rating Analysis. Earlier, we talked about how to solve\nthe problem of LARA in two stages. But we first do segmentation\nof different aspects. And then we use a latent regression\nmodel to learn the aspect ratings and then later the weight. Now it's also possible to develop\na unified generative model for solving this problem, and that is we not only model the generational\nover-rating based on text. We also model the generation of text,\nand so a natural solution would\nbe to use topic model. So given the entity, we can assume there are aspects that\nare described by word distributions. Topics. And then we an use a topic model to model\nthe generation of the reviewed text. I will assume words in the review text\nare drawn from these distributions. In the same way as we assumed for\ngenerating model like PRSA. And then we can then plug in\nthe latent regression model to use the text to further\npredict the overrating. And that means when we first\npredict the aspect rating and then combine them with aspect weights\nto predict the overall rating. So this would give us\na unified generated model, where we model both the generation of text\nand the overall ready condition on text. So we don't have time to discuss\nthis model in detail as in many other cases in this part of the cause\nwhere we discuss the cutting edge topics, but there's a reference site here\nwhere you can find more details. So now I'm going to show you some\nsimple results that you can get by using these kind of generated models. First, it's about rating decomposition. So here, what you see\nare the decomposed ratings for three hotels that have\nthe same overall rating. So if you just look at the overall rating, you can't really tell much\ndifference between these hotels. But by decomposing these\nratings into aspect ratings we can see some hotels have higher\nratings for some dimensions, like value, but others might score better\nin other dimensions, like location. And so this can give you detailed\nopinions at the aspect level. Now here, the ground-truth is\nshown in the parenthesis, so it also allows you to see whether\nthe prediction is accurate. It's not always accurate but It's mostly\nstill reflecting some of the trends. The second result you compare\ndifferent reviewers on the same hotel. So the table shows the decomposed ratings\nfor two reviewers about same hotel. Again their high level\noverall ratings are the same. So if you just look at the overall\nratings, you don't really get that much information about the difference\nbetween the two reviewers. But after you decompose the ratings, you can see clearly that they have\nhigh scores on different dimensions. So this shows that model can review\ndifferences in opinions of different reviewers and such a detailed\nunderstanding can help us understand better about reviewers and also better\nabout their feedback on the hotel. This is something very interesting, because this is in some\nsense some byproduct. In our problem formulation,\nwe did not really have to do this. But the design of the generating\nmodel has this component. And these are sentimental weights for\nwords in different aspects. And you can see the highly weighted words\nversus the negatively loaded weighted words here for\neach of the four dimensions. Value, rooms, location, and cleanliness. The top words clearly make sense, and\nthe bottom words also make sense. So this shows that with this approach, we can also learn sentiment\ninformation directly from the data. Now, this kind of lexicon is very useful\nbecause in general, a word like long, let's say, may have different sentiment\npolarities for different context. So if I say the battery life of this\nlaptop is long, then that's positive. But if I say the rebooting time for\nthe laptop is long, that's bad, right? So even for\nreviews about the same product, laptop, the word long is ambiguous, it could\nmean positive or it could mean negative. But this kind of lexicon, that we can\nlearn by using this kind of generated models, can show whether a word is\npositive for a particular aspect. So this is clearly very useful, and in\nfact such a lexicon can be directly used to tag other reviews about hotels or tag comments about hotels in\nsocial media like Tweets. And what's also interesting is that since\nthis is almost completely unsupervised, well assuming the reviews whose\noverall rating are available And then this can allow us to learn form\npotentially larger amount of data on the internet to reach sentiment lexicon. And here are some results to\nvalidate the preference words. Remember the model can infer wether\na reviewer cares more about service or the price. Now how do we know whether\nthe inferred weights are correct? And this poses a very difficult\nchallenge for evaluation. Now here we show some\ninteresting way of evaluating. What you see here are the prices\nof hotels in different cities, and these are the prices of hotels that are\nfavored by different groups of reviewers. The top ten are the reviewers\nwas the highest inferred value to other aspect ratio. So for example value versus location,\nvalue versus room, etcetera. Now the top ten of the reviewers that\nhave the highest ratios by this measure. And that means these reviewers\ntend to put a lot of weight on value as compared\nwith other dimensions. So that means they really\nemphasize on value. The bottom ten on the other\nhand of the reviewers. The lowest ratio, what does that mean? Well it means these reviewers have\nput higher weights on other aspects than value. So those are people that cared about\nanother dimension and they didn't care so much the value in some sense, at least\nas compared with the top ten group. Now these ratios are computer based on\nthe inferred weights from the model. So now you can see the average prices\nof hotels favored by top ten reviewers are indeed much cheaper than those\nthat are favored by the bottom ten. And this provides some indirect way\nof validating the inferred weights. It just means the weights are not random. They are actually meaningful here. In comparison,\nthe average price in these three cities, you can actually see the top ten\ntend to have below average in price, whereas the bottom half, where they care\na lot about other things like a service or room condition tend to have hotels\nthat have higher prices than average. So with these results we can build\na lot of interesting applications. For example, a direct application would be\nto generate the rated aspect, the summary, and because of the decomposition we\nhave now generated the summaries for each aspect. The positive sentences the negative\nsentences about each aspect. It's more informative than original review\nthat just has an overall rating and review text. Here are some other results about the aspects that's covered\nfrom reviews with no ratings. These are mp3 reviews, and these results show that the model\ncan discover some interesting aspects. Commented on low overall ratings versus\nthose higher overall per ratings. And they care more about\nthe different aspects. Or they comment more on\nthe different aspects. So that can help us discover for\nexample, consumers' trend in appreciating different\nfeatures of products. For example, one might have discovered\nthe trend that people tend to like larger screens of cell phones or\nlight weight of laptop, etcetera. Such knowledge can be useful for manufacturers to design their\nnext generation of products. Here are some interesting results\non analyzing users rating behavior. So what you see is average weights along different dimensions by\ndifferent groups of reviewers. And on the left side you see the weights\nof viewers that like the expensive hotels. They gave the expensive hotels 5 Stars,\nand you can see their average rates\ntend to be more for some service. And that suggests that people like\nexpensive hotels because of good service, and that's not surprising. That's also another way to\nvalidate it by inferred weights. If you look at the right side where,\nlook at the column of 5 Stars. These are the reviewers that\nlike the cheaper hotels, and they gave cheaper hotels five stars. As we expected and\nthey put more weight on value, and that's why they like\nthe cheaper hotels. But if you look at the, when they didn't\nlike expensive hotels, or cheaper hotels, then you'll see that they tended to\nhave more weights on the condition of the room cleanness. So this shows that by using this model,\nwe can infer some information that's very hard to obtain\neven if you read all the reviews. Even if you read all the reviews it's\nvery hard to infer such preferences or such emphasis. So this is a case where text mining\nalgorithms can go beyond what humans can do, to review\ninteresting patterns in the data. And this of course can be very useful. You can compare different hotels, compare the opinions from different\nconsumer groups, in different locations. And of course, the model is general. It can be applied to any\nreviews with overall ratings. So this is a very useful\ntechnique that can support a lot of text mining applications. Finally the results of applying this\nmodel for personalized ranking or recommendation of entities. So because we can infer the reviewers\nweights on different dimensions, we can allow a user to actually\nsay what do you care about. So for example, I have a query\nhere that shows 90% of the weight should be on value and 10% on others. So that just means I don't\ncare about other aspect. I just care about getting a cheaper hotel. My emphasis is on the value dimension. Now what we can do with such query\nis we can use reviewers that we believe have a similar preference\nto recommend a hotels for you. How can we know that? Well, we can infer the weights of\nthose reviewers on different aspects. We can find the reviewers whose\nweights are more precise, of course inferred rates\nare similar to yours. And then use those reviewers to\nrecommend hotels for you and this is what we call personalized or\nrather query specific recommendations. Now the non-personalized\nrecommendations now shown on the top, and you can see the top results generally\nhave much higher price, than the lower group and that's because when the\nreviewer's cared more about the value as dictated by this query they tended\nto really favor low price hotels. So this is yet\nanother application of this technique. It shows that by doing text mining\nwe can understand the users better. And once we can handle users better\nwe can solve these users better. So to summarize our discussion\nof opinion mining in general, this is a very important topic and\nwith a lot of applications. And as a text sentiment\nanalysis can be readily done by using just text categorization. But standard technique\ntends to not be enough. And so we need to have enriched\nfeature implementation. And we also need to consider\nthe order of those categories. And we'll talk about ordinal\nregression for some of these problem. We have also assume that\nthe generating models are powerful for mining latent user preferences. This in particular in the generative\nmodel for mining latent regression. And we embed some interesting\npreference information and send the weights of words in the model\nas a result we can learn most useful information when\nfitting the model to the data. Now most approaches have been proposed and\nevaluated. For product reviews, and that was because\nin such a context, the opinion holder and the opinion target are clear. And they are easy to analyze. And there, of course,\nalso have a lot of practical applications. But opinion mining from news and\nsocial media is also important, but that's more difficult than analyzing review data,\nmainly because the opinion holders and opinion targets are all interested. So that calls for natural management processing\ntechniques to uncover them accurately. Here are some suggested readings. The first two are small books that\nare of some use of this topic, where you can find a lot of discussion\nabout other variations of the problem and techniques proposed for\nsolving the problem. The next two papers about\ngenerating models for rating the aspect rating analysis. The first one is about solving\nthe problem using two stages, and the second one is about a unified model\nwhere the topic model is integrated with the regression model to solve\nthe problem using a unified model. [MUSIC]",
 "03_6-3-text-based-prediction.en.txt": "[SOUND] This lecture is about the Text-Based Prediction. In this lecture, we're going to\nstart talking about the mining a different kind of knowledge,\nas you can see here on this slide. Namely we're going to use text\ndata to infer values of some other variables in the real world that may\nnot be directly related to the text. Or only remotely related to text data. So this is very different\nfrom content analysis or topic mining where we directly\ncharacterize the content of text. It's also different from opinion mining or\nsentiment analysis, which still have to do is\ncharacterizing mostly the content. Only that we focus more\non the subject of content which reflects what we know\nabout the opinion holder. But this only provides limited\nreview of what we can predict. In this lecture and the following\nlectures, we're going to talk more about how we can predict more\nInformation about the world. How can we get the sophisticated patterns\nof text together with other kind of data? It would be useful first to take a look\nat the big picture of prediction, and data mining in general, and\nI call this data mining loop. So the picture that you are seeing right\nnow is that there are multiple sensors, including human sensors, to report what we have seen in\nthe real world in the form of data. Of course the data in the form\nof non-text data, and text data. And our goal is to see if we\ncan predict some values of important real world\nvariables that matter to us. For example, someone's house condition,\nor the weather, or etc. And so these variables would be important\nbecause we might want to act on that. We might want to make\ndecisions based on that. So how can we get from the data\nto these predicted values? Well in general we'll first have to do\ndata mining and analysis of the data. Because we, in general, should treat\nall the data that we collected in such a prediction problem set up. We are very much interested in\njoint mining of non-text and text data, which should\ncombine all the data together. And then, through analysis,\ngenerally there are multiple predictors of this\ninteresting variable to us. And we call these features. And these features can then be\nput into a predictive model, to actually predict the value\nof any interesting variable. So this then allows us\nto change the world. And so\nthis basically is the general process for making a prediction based on data,\nincluding the test data. Now it's important to emphasize\nthat a human actually plays a very important\nrole in this process. Especially because of\nthe involvement of text data. So human first would be involved\nin the mining of the data. It would control the generation\nof these features. And it would also help us\nunderstand the text data, because text data are created\nto be consumed by humans. Humans are the best in consuming or\ninterpreting text data. But when there are, of course, a lot of\ntext data then machines have to help and that's why we need to do text data mining. Sometimes machines can see patterns in\na lot of data that humans may not see. But in general human would\nplay an important role in analyzing some text data, or applications. Next, human also must be involved\nin predictive model building and adjusting or testing. So in particular, we will have a lot\nof domain knowledge about the problem of prediction that we can build\ninto this predictive model. And then next, of course, when we have\npredictive values for the variables, then humans would be involved in\ntaking actions to change a word or make decisions based on\nthese particular values. And finally it's interesting\nthat a human could be involved in controlling the sensors. And this is so that we can\nadjust to the sensors to collect the most useful data for prediction. So that's why I call\nthis data mining loop. Because as we perturb the sensors,\nit'll collect the new data and more useful data then we will\nobtain more data for prediction. And this data generally will help\nus improve the predicting accuracy. And in this loop, humans will recognize what additional\ndata will need to be collected. And machines, of course, help humans identify what data\nshould be collected next. In general, we want to collect data\nthat is most useful for learning. And there was actually a subarea in\nmachine learning called active learning that has to do with this. How do you identify data points that would be most helpful\nin machine learning programs? If you can label them, right? So, in general, you can see there is a loop here from\ndata acquisition to data analysis. Or data mining to prediction of values. And to take actions to change the word,\nand then observe what happens. And then you can then\ndecide what additional data have to be collected by\nadjusting the sensors. Or from the prediction arrows,\nyou can also note what additional data we need to acquire in order to\nimprove the accuracy of prediction. And this big picture is\nactually very general and it's reflecting a lot of important\napplications of big data. So, it's useful to keep that in mind\nwhile we are looking at some text mining techniques. So from text mining perspective and\nwe're interested in text based prediction. Of course, sometimes texts\nalone can make predictions. And this is most useful for prediction about human behavior or\nhuman preferences or opinions. But in general text data will be\nput together as non-text data. So the interesting questions\nhere would be, first, how can we design effective predictors? And how do we generate such\neffective predictors from text? And this question has been addressed to\nsome extent in some previous lectures where we talked about what kind of\nfeatures we can design for text data. And it has also been\naddressed to some extent by talking about the other knowledge\nthat we can mine from text. So, for example, topic mining can be very\nuseful to generate the patterns or topic based indicators or predictors that can\nbe further fed into a predictive model. So topics can be intermediate\nrecognition of text. That would allow us to do\ndesign high level features or predictors that are useful for\nprediction of some other variable. It may be also generated from original\ntext data, it provides a much better implementation of the problem and\nit serves as more effective predictors. And similarly similar analysis can\nlead to such predictors, as well. So, those other data mining or text mining algorithms can be\nused to generate predictors. The other question is, how can we join\nthe mine text and non-text data together? Now, this is a question that\nwe have not addressed yet. So, in this lecture, and in the following lectures,\nwe're going to address this problem. Because this is where we can generate much\nmore enriched features for prediction. And allows us to review a lot of\ninteresting knowledge about the world. These patterns that\nare generated from text and non-text data themselves can sometimes,\nalready be useful for prediction. But, when they are put together\nwith many other predictors they can really help\nimproving the prediction. Basically, you can see text-based\nprediction can actually serve as a unified framework to combine many text mining and\nanalysis techniques. Including topic mining and any content\nmining techniques or segment analysis. The goal here is mainly to evoke\nvalues of real-world variables. But in order to achieve the goal\nwe can do some other preparations. And these are subtasks. So one subtask could mine the content\nof text data, like topic mining. And the other could be to mine\nknowledge about the observer. So sentiment analysis, opinion. And both can help provide predictors for\nthe prediction problem. And of course we can also add non-text\ndata directly to the predicted model, but then non-text data also helps\nprovide a context for text analyst. And that further improves the topic\nmining and the opinion analysis. And such improvement often leads to more\neffective predictors for our problems. It would enlarge the space of patterns\nof opinions of topics that we can mine from text and\nthat we'll discuss more later. So the joint analysis of text and non-text data can be actually\nunderstood from two perspectives. One perspective,\nwe have non-text can help with testimony. Because non-text data can\nprovide a context for mining text data provide a way to\npartition data in different ways. And this leads to a number of type of\ntechniques for contextual types of mining. And that's the mine text in\nthe context defined by non-text data. And you see this reference here, for\na large body of work, in this direction. And I will need to highlight some of them,\nin the next lectures. Now, the other perspective is text data can help with non-text\ndata mining as well. And this is because text\ndata can help interpret patterns discovered from non-text data. Let's say you discover some frequent\npatterns from non-text data. Now we can use the text data\nassociated with instances where the pattern occurs as well as\ntext data that is associated with instances where the pattern\ndoesn't look up. And this gives us two sets of text data. And then we can see what's the difference. And this difference in text data is\ninterpretable because text content is easy to digest. And that difference might\nsuggest some meaning for this pattern that we\nfound from non-text data. So, it helps interpret such patterns. And this technique is\ncalled pattern annotation. And you can see this reference\nlisted here for more detail. So here are the references\nthat I just mentioned. The first is reference for\npattern annotation. The second is, Qiaozhu Mei's\ndissertation on contextual text mining. It contains a large body of work on\ncontextual text mining techniques. [MUSIC]",
 "04_6-4-contextual-text-mining-motivation.en.txt": "[SOUND]\nThis lecture is about\nthe contextual text mining. Contextual text mining\nis related to multiple kinds of knowledge that we mine from\ntext data, as I'm showing here. It's related to topic mining because you\ncan make topics associated with context, like time or location. And similarly, we can make opinion\nmining more contextualized, making opinions connected to context. It's related to text based prediction\nbecause it allows us to combine non-text data with text data to derive\nsophisticated predictors for the prediction problem. So more specifically, why are we\ninterested in contextual text mining? Well, that's first because text\noften has rich context information. And this can include direct context such\nas meta-data, and also indirect context. So, the direct context can grow\nthe meta-data such as time, location, authors, and\nsource of the text data. And they're almost always available to us. Indirect context refers to additional\ndata related to the meta-data. So for example, from office,\nwe can further obtain additional context such as social network of\nthe author, or the author's age. Such information is not in general\ndirectly related to the text, yet through the process, we can connect them. There could be other text\ndata from the same source, as this one through the other text can\nbe connected with this text as well. So in general, any related data\ncan be regarded as context. So there could be removed or\nrated for context. And so what's the use? What is text context used for? Well, context can be used to partition\ntext data in many interesting ways. It can almost allow us to partition\ntext data in other ways as we need. And this is very important\nbecause this allows us to do interesting comparative analyses. It also in general,\nprovides meaning to the discovered topics, if we associate the text with context. So here's illustration of how context can be regarded as interesting\nways of partitioning of text data. So here I just showed some research\npapers published in different years. On different venues, different conference names here listed on\nthe bottom like the SIGIR or ACL, etc. Now such text data can be partitioned in many interesting ways\nbecause we have context. So the context here just includes time and\nthe conference venues. But perhaps we can include\nsome other variables as well. But let's see how we can partition\nthis interesting of ways. First, we can treat each\npaper as a separate unit. So in this case, a paper ID and the,\neach paper has its own context. It's independent. But we can also treat all the papers\nwithin 1998 as one group and this is only possible because\nof the availability of time. And we can partition data in this way. This would allow us to compare topics for\nexample, in different years. Similarly, we can partition\nthe data based on the menus. We can get all the SIGIR papers and\ncompare those papers with the rest. Or compare SIGIR papers with KDD papers,\nwith ACL papers. We can also partition the data to obtain\nthe papers written by authors in the U.S., and that of course,\nuses additional context of the authors. And this would allow us to then\ncompare such a subset with another set of papers written\nby also seen in other countries. Or we can obtain a set of\npapers about text mining, and this can be compared with\npapers about another topic. And note that these\npartitionings can be also intersected with each other to generate\neven more complicated partitions. And so in general, this enables\ndiscovery of knowledge associated with different context as needed. And in particular,\nwe can compare different contexts. And this often gives us\na lot of useful knowledge. For example, comparing topics over time,\nwe can see trends of topics. Comparing topics in different\ncontexts can also reveal differences about the two contexts. So there are many interesting questions\nthat require contextual text mining. Here I list some very specific ones. For example, what topics have\nbeen getting increasing attention recently in data mining research? Now to answer this question, obviously we need to analyze\ntext in the context of time. So time is context in this case. Is there any difference in the responses\nof people in different regions to the event, to any event? So this is a very broad\nan answer to this question. In this case of course,\nlocation is the context. What are the common research\ninterests of two researchers? In this case, authors can be the context. Is there any difference in the research\ntopics published by authors in the USA and those outside? Now in this case,\nthe context would include the authors and their affiliation and location. So this goes beyond just\nthe author himself or herself. We need to look at the additional\ninformation connected to the author. Is there any difference in the opinions\nof all the topics expressed on one social network and another? In this case, the social network of\nauthors and the topic can be a context. Other topics in news data that\nare correlated with sudden changes in stock prices. In this case, we can use a time series\nsuch as stock prices as context. What issues mattered in the 2012\npresidential campaign, or presidential election? Now in this case,\ntime serves again as context. So, as you can see,\nthe list can go on and on. Basically, contextual text mining\ncan have many applications. [MUSIC]",
 "05_6-5-contextual-text-mining-contextual-probabilistic-latent-semantic-analysis.en.txt": "[MUSIC] This lecture is about\na specific technique for Contextual Text Mining called Contextual\nProbabilistic Latent Semantic Analysis. In this lecture, we're going to continue\ndiscussing Contextual Text Mining. And we're going to introduce Contextual\nProbablitistic Latent Semantic Analysis as exchanging of POS for\ndoing contextual text mining. Recall that in contextual text mining\nwe hope to analyze topics in text, in consideration of the context so that we can associate the topics with a\nproperty of the context were interesting. So in this approach, contextual\nprobabilistic latent semantic analysis, or CPLSA, the main idea is to\nexpress to the add interesting context variables into a generating model. Recall that before when we generate\nthe text we generally assume we'll start wIth some topics, and\nthen assemble words from some topics. But here, we're going to add context\nvariables, so that the coverage of topics, and also the content of topics\nwould be tied in context. Or in other words, we're going to let\nthe context Influence both coverage and the content of a topic. The consequences that this will enable\nus to discover contextualized topics. Make the topics more interesting,\nmore meaningful. Because we can then have topics\nthat can be interpreted as specifically to a particular\ncontext that we are interested in. For example, a particular time period. As an extension of PLSA model, CPLSA does the following changes. Firstly it would model the conditional\nlikelihood of text given context. That clearly suggests that the generation\nof text would then depend on context, and that allows us to bring\ncontext into the generative model. Secondly, it makes two specific\nassumptions about the dependency of topics on context. One is to assume that depending on\nthe context, depending on different time periods or different locations, we assume\nthat there are different views of a topic or different versions of word\ndescriptions that characterize a topic. And this assumption allows\nus to discover different variations of the same topic\nin different contexts. The other is that we assume the topic\ncoverage also depends on the context. That means depending on the time or location, we might cover\ntopics differently. Again, this dependency\nwould then allow us to capture the association of\ntopics with specific contexts. We can still use the EM algorithm to solve\nthe problem of parameter estimation. And in this case, the estimated parameters\nwould naturally contain context variables. And in particular, a lot of conditional probabilities\nof topics given certain context. And this is what allows you\nto do contextual text mining. So this is the basic idea. Now, we don't have time to\nintroduce this model in detail, but there are references here that you\ncan look into to know more detail. Here I just want to explain the high\nlevel ideas in more detail. Particularly I want to explain\nthe generation process. Of text data that has context\nassociated in such a model. So as you see here, we can assume\nthere are still multiple topics. For example, some topics might represent\na themes like a government response, donation Or the city of New Orleans. Now this example is in the context\nof Hurricane Katrina and that hit New Orleans. Now as you can see we\nassume there are different views associated with each of the topics. And these are shown as View 1,\nView 2, View 3. Each view is a different\nversion of word distributions. And these views are tied\nto some context variables. For example, tied to the location Texas,\nor the time July 2005, or the occupation of the author\nbeing a sociologist. Now, on the right side, now we assume\nthe document has context information. So the time is known to be July 2005. The location is Texas, etc. And such context information is\nwhat we hope to model as well. So we're not going to just model the text. And so one idea here is to model\nthe variations of top content and various content. And this gives us different views\nof the water distributions. Now on the bottom you will see the theme\ncoverage of top Coverage might also vary according to these context\nbecause in the case of a location like Texas, people might\nwant to cover the red topics more. That's New Orleans. That's visualized here. But in a certain time period, maybe Particular topic and\nwill be covered more. So this variation is\nalso considered in CPLSA. So to generate the searcher document With\ncontext, with first also choose a view. And this view of course now could\nbe from any of these contexts. Let's say, we have taken this\nview that depends on the time. In the middle. So now, we will have a specific\nversion of word distributions. Now, you can see some probabilities\nof words for each topic. Now, once we have chosen a view, now the situation will be very similar\nto what happened in standard ((PRSA)) We assume we have got word distribution\nassociated with each topic, right? And then next, we will also choose\na coverage from the bottom, so we're going to choose a particular\ncoverage, and that coverage, before is fixed in PLSA, and\nassigned to a particular document. Each document has just one\ncoverage distribution. Now here, because we consider context, so\nthe distribution of topics or the coverage of Topics can vary depending on the\ncontext that has influenced the coverage. So, for example,\nwe might pick a particular coverage. Let's say in this case we picked\na document specific coverage. Now with the coverage and\nthese word distributions we can generate a document in\nexactly the same way as in PLSA. So what it means, we're going to\nuse the coverage to choose a topic, to choose one of these three topics. Let's say we have picked the yellow topic. Then we'll draw a word from this\nparticular topic on the top. Okay, so\nwe might get a word like government. And then next time we might\nchoose a different topic, and we'll get donate, etc. Until we generate all the words. And this is basically\nthe same process as in PLSA. So the main difference is\nwhen we obtain the coverage. And the word distribution,\nwe let the context influence our choice So in other words we have extra switches\nthat are tied to these contacts that will control the choices of different views\nof topics and the choices of coverage. And naturally the model we have\nmore parameters to estimate. But once we can estimate those\nparameters that involve the context, then we will be able to understand\nthe context specific views of topics, or context specific coverages of topics. And this is precisely what we\nwant in contextual text mining. So here are some simple results. From using such a model. Not necessary exactly the same model,\nbut similar models. So on this slide you see\nsome sample results of comparing news articles about Iraq War and\nAfghanistan War. Now we have about 30 articles on Iraq\nwa,r and 26 articles on Afghanistan war. And in this case,\nthe goal is to review the common topic. It's covered in both sets of articles and the differences of variations of\nthe topic in each of the two collections. So in this case the context is explicitly\nspecified by the topic or collection. And we see the results here\nshow that there is a common theme that's corresponding to\nCluster 1 here in this column. And there is a common theme indicting that\nUnited Nations is involved in both Wars. It's a common topic covered\nin both sets of articles. And that's indicated by the high\nprobability words shown here, united and nations. Now if you know the background,\nof course this is not surprising and this topic is indeed very\nrelevant to both wars. If you look at the column further and\nthen what's interesting's that the next two cells of word\ndistributions actually tell us collection specific variations\nof the topic of United Nations. So it indicates that the Iraq War, United Nations was more involved\nin weapons factions, whereas in the Afghanistan War it was more involved\nin maybe aid to Northern Alliance. It's a different variation of\nthe topic of United Nations. So this shows that by\nbringing the context. In this case different the walls or\ndifferent the collection of texts. We can have topical variations\ntied to these contexts, to review the differences of coverage\nof the United Nations in the two wars. Now similarly if you look at\nthe second cluster Class two, it has to do with the killing of people,\nand, again, it's not surprising if you know\nthe background about wars. All the wars involve killing of people,\nbut imagine if you are not familiar\nwith the text collections. We have a lot of text articles, and such a technique can reveal the common\ntopics covered in both sets of articles. It can be used to review common topics\nin multiple sets of articles as well. If you look at of course in\nthat column of cluster two, you see variations of killing of people\nand that corresponds to different contexts And here is another example of results obtained from blog articles\nabout Hurricane Katrina. In this case,\nwhat you see here is visualization of the trends of topics over time. And the top one shows just\nthe temporal trends of two topics. One is oil price, and one is about\nthe flooding of the city of New Orleans. Now these topics are obtained from\nblog articles about Hurricane Katrina. And people talk about these topics. And end up teaching to some other topics. But the visualisation shows\nthat with this technique, we can have conditional\ndistribution of time. Given a topic. So this allows us to plot\nthis conditional probability the curve is like what you're seeing here. We see that, initially, the two\ncurves tracked each other very well. But later we see the topic of New Orleans\nwas mentioned again but oil price was not. And this turns out to be the time period when another hurricane,\nhurricane Rita hit the region. And that apparently triggered more\ndiscussion about the flooding of the city. The bottom curve shows\nthe coverage of this topic about flooding of the city by block\narticles in different locations. And it also shows some shift of\ncoverage that might be related to people's migrating from the state\nof Louisiana to Texas for example. So in this case we can see the time can\nbe used as context to review trends of topics. These are some additional\nresults on spacial patterns. In this case it was about\nthe topic of government response. And there was some criticism about\nthe slow response of government in the case of Hurricane Katrina. And the discussion now is\ncovered in different locations. And these visualizations show the coverage\nin different weeks of the event. And initially it's covered\nmostly in the victim states, in the South, but then gradually\nspread into other locations. But in week four,\nwhich is shown on the bottom left, we see a pattern that's very similar\nto the first week on the top left. And that's when again\nHurricane Rita hit in the region. So such a technique would allow\nus to use location as context to examine their issues of topics. And of course the moral\nis completely general so you can apply this to any\nother connections of text. To review spatial temporal patterns. His view found another application\nof this kind of model, where we look at the use of the model for\nevent impact analysis. So here we're looking at the research\narticles information retrieval. IR, particularly SIGIR papers. And the topic we are focusing on\nis about the retrieval models. And you can see the top words with high\nprobability about this model on the left. And then we hope to examine\nthe impact of two events. One is a start of TREC, for\nText and Retrieval Conference. This is a major evaluation\nsponsored by U.S. government, and was launched in 1992 or\naround that time. And that is known to have made a impact on the topics of research\ninformation retrieval. The other is the publication of\na seminal paper, by Croft and Porte. This is about a language model\napproach to information retrieval. It's also known to have made a high\nimpact on information retrieval research. So we hope to use this kind of\nmodel to understand impact. The idea here is simply to\nuse the time as context. And use these events to divide\nthe time periods into a period before. For the event and\nanother after this event. And then we can compare\nthe differences of the topics. The and the variations, etc. So in this case,\nthe results show before track the study of retrieval models was mostly a vector\nspace model, Boolean model etc. But the after Trec, apparently the study of retrieval models\nhave involved a lot of other words. That seems to suggest some\ndifferent retrieval tasks, so for example, email was used in\nthe enterprise search tasks and subtopical retrieval was another\ntask later introduced by Trec. On the bottom,\nwe see the variations that are correlated with the propagation of\nthe language model paper. Before, we have those classic\nprobability risk model, logic model, Boolean etc., but after 1998, we see clear dominance of language\nmodel as probabilistic models. And we see words like language model,\nestimation of parameters, etc. So this technique here can use events as\ncontext to understand the impact of event. Again the technique is generals so you can use this to analyze\nthe impact of any event. Here are some suggested readings. The first is paper about simple staging of\npsi to label cross-collection comparison. It's to perform comparative\ntext mining to allow us to extract common topics shared\nby multiple collections. And there are variations\nin each collection. The second one is the main\npaper about the CPLSA model. Was a discussion of a lot of applications. The third one has a lot of details\nabout the special temporal patterns for the Hurricane Katrina example. [MUSIC]",
 "06_6-6-contextual-text-mining-mining-topics-with-social-network-context.en.txt": "[SOUND] This lecture is about how to mine text data with social network as context. In this lecture we're going to continue\ndiscussing contextual text mining. In particular, we're going to look at\nthe social network of others as context. So first, what's our motivation for using\nnetwork context for analysis of text? The context of a text\narticle can form a network. For example the authors\nof research articles might form collaboration networks. But authors of social media content\nmight form social networks. For example,\nin Twitter people might follow each other. Or in Facebook as people might\nclaim friends of others, etc. So such context connects\nthe content of the others. Similarly, locations associated with\ntext can also be connected to form geographical network. But in general you can can imagine\nthe metadata of the text data can form some kind of network\nif they have some relations. Now there is some benefit in\njointly analyzing text and its social network context or\nnetwork context in general. And that's because we can use network to\nimpose some constraints on topics of text. So for example it's reasonable\nto assume that authors connected in collaboration networks\ntend to write about the similar topics. So such heuristics can be used\nto guide us in analyzing topics. Text also can help characterize the\ncontent associated with each subnetwork. And this is to say that both kinds of data, the network and\ntext, can help each other. So for example the difference in\nopinions expressed that are in two subnetworks can be reviewed by\ndoing this type of joint analysis. So here briefly you could use a model\ncalled a network supervised topic model. In this slide we're going to\ngive some general ideas. And then in the next slide we're\ngoing to give some more details. But in general in this part of the course\nwe don't have enough time to cover these frontier topics in detail. But we provide references\nthat would allow you to read more about the topic\nto know the details. But it should still be useful\nto know the general ideas. And to know what they can do to know\nwhen you might be able to use them. So the general idea of network\nsupervised topic model is the following. Let's start with viewing\nthe regular topic models. Like if you had an LDA as\nsorting optimization problem. Of course, in this case, the optimization objective\nfunction is a likelihood function. So we often use maximum likelihood\nestimator to obtain the parameters. And these parameters will give us\nuseful information that we want to obtain from text data. For example, topics. So we want to maximize the probability\nof tests that are given the parameters generally denoted by number. The main idea of incorporating network is to think about the constraints that\ncan be imposed based on the network. In general,\nthe idea is to use the network to impose some constraints on\nthe model parameters, lambda here. For example, the text at adjacent nodes of the network\ncan be similar to cover similar topics. Indeed, in many cases,\nthey tend to cover similar topics. So we may be able to smooth\nthe topic distributions on the graph on the network so\nthat adjacent nodes will have very similar topic distributions. So they will share a common\ndistribution on the topics. Or have just a slight variations of the\ntopic of distributions, of the coverage. So, technically, what we can do\nis simply to add a network and use the regularizers to the likelihood\nof objective function as shown here. So instead of just optimize\nthe probability of test data given parameters lambda, we're\ngoing to optimize another function F. This function combines the likelihood with\na regularizer function called R here. And the regularizer defines\nthe the parameters lambda and the Network. It tells us basically what kind of parameters are preferred\nfrom a network constraint perspective. So you can easily see this is in effect implementing the idea of imposing\nsome prior on the model parameters. Only that we're not necessary\nhaving a probabilistic model, but the idea is the same. We're going to combine the two in\none single objective function. So, the advantage of this idea\nis that it's quite general. Here the top model can be any\ngenerative model for text. It doesn't have to be PLSA or\nLEA, or the current topic models. And similarly,\nthe network can be also in a network. Any graph that connects\nthese text objects. This regularizer can\nalso be any regularizer. We can be flexible in capturing different\nheuristics that we want to capture. And finally,\nthe function F can also vary, so there can be many different\nways to combine them. So, this general idea is actually quite,\nquite powerful. It offers a general approach\nto combining these different types of data in single\noptimization framework. And this general idea can really\nbe applied for any problem. But here in this paper reference here, a particular instantiation\ncalled a NetPLSA was started. In this case, it's just for instantiating of PLSA to incorporate this\nsimple constraint imposed by network. And the prior here is the neighbors on the network must have\nsimilar topic distribution. They must cover similar\ntopics in similar ways. And that's basically\nwhat it says in English. So technically we just have\na modified objective function here. Let's define both the texts you can\nactually see in the network graph G here. And if you look at this formula, you can actually recognize\nsome part fairly familiarly. Because they are, they should be\nfairly familiar to you by now. So can you recognize which\npart is the likelihood for the test given the topic model? Well if you look at it, you will see this\npart is precisely the PLSA log-likelihood that we want to maximize when we\nestimate parameters for PLSA alone. But the second equation shows some\nadditional constraints on the parameters. And in particular,\nwe'll see here it's to measure the difference between the topic\ncoverage at node u and node v. The two adjacent nodes on the network. We want their distributions to be similar. So here we are computing the square\nof their differences and we want to minimize this difference. And note that there's a negative sign in\nfront of this sum, this whole sum here. So this makes it possible to find the parameters that are both to maximize the PLSA log-likelihood. That means the parameters\nwill fit the data well and, also to respect that this\nconstraint from the network. And this is the negative\nsign that I just mentioned. Because this is an negative sign,\nwhen we maximize this object in function we'll actually\nminimize this statement term here. So if we look further in\nthis picture we'll see the results will weight of\nedge between u and v here. And that space from out network. If you have a weight that says well, these two nodes are strong\ncollaborators of researchers. These two are strong connections\nbetween two people in a social network. And they would have weight. Then that means it would be more important\nthat they're topic coverages are similar. And that's basically what it says here. And finally you see\na parameter lambda here. This is a new parameter to control\nthe influence of network constraint. We can see easily, if lambda is set to 0,\nwe just go back to the standard PLSA. But when lambda is set to a larger value, then we will let the network\ninfluence the estimated models more. So as you can see, the effect here is\nthat we're going to do basically PLSA. But we're going to also try\nto make the topic coverages on the two nodes that are strongly\nconnected to be similar. And we ensure their coverages are similar. So here are some of the several results,\nfrom that paper. This is slide shows the record\nresults of using PLSA. And the data here is DBLP data,\nbibliographic data, about research articles. And the experiments have to do with\nusing four communities of applications. IR information retrieval. DM stands for data mining. ML for machinery and web. There are four communities of articles,\nand we were hoping to see that the topic mining can help\nus uncover these four communities. But from these assembled topics that you\nhave seen here that are generated by PLSA. And PLSA is unable to generate\nthe four communities that correspond to our intuition. The reason was because they\nare all mixed together and there are many words that\nare shared by these communities. So it's not that easy to use\nfour topics to separate them. If we use more topics,\nperhaps we will have more coherent topics. But what's interesting is that if we\nuse the NetPLSA where the network, the collaboration network in this case of\nauthors is used to impose constraints. And in this case we also use four topics. But Ned Pierre said we gave\nmuch more meaningful topics. So here we'll see that these topics\ncorrespond well to the four communities. The first is information retrieval. The second is data mining. Third is machine learning. And the fourth is web. So that separation was mostly\nbecause of the influence of network where with leverage is\na collaboration network information. Essentially the people that\nform a collaborating network would then be kind of assumed\nto write about similar topics. And that's why we're going to\nhave more coherent topics. And if you just listen to text data\nalone based on the occurrences, you won't get such coherent topics. Even though a topic model, like PLSA or LDA also should be able to\npick up co-occurring words. So in general the topics\nthat they generate represent words that co-occur each other. But still they cannot generate such\na coherent results as NetPLSA, showing that the network\ncontest is very useful here. Now a similar model could have been also\nuseful to to characterize the content associated with each\nsubnetwork of collaborations. So a more general view of text\nmining in context of network is you treat text as living in a rich\ninformation network environment. And that means we can connect all the\nrelated data together as a big network. And text data can be associated with\na lot of structures in the network. For example, text data can be associated\nwith the nodes of the network, and that's basically what we just\ndiscussed in the NetPLSA. But text data can be associated with age\nas well, or paths or even subnetworks. And such a way to represent texts\nthat are in the big environment of all the context information\nis very powerful. Because it allows to analyze all the data,\nall the information together. And so in general, analysis of text\nshould be using the entire network information that's\nrelated to the text data. So here's one suggested reading. And this is the paper about NetPLSA where\nyou can find more details about the model and how to make such a model. [MUSIC]",
 "07_6-7-contextual-text-mining-mining-casual-topics-with-time-series-supervision.en.txt": "[SOUND] This lecture is about using a time series as context to potentially\ndiscover causal topics in text. In this lecture, we're going to continue\ndiscussing Contextual Text Mining. In particular, we're going to look\nat the time series as a context for analyzing text,\nto potentially discover causal topics. As usual, it started with the motivation. In this case, we hope to use text\nmining to understand a time series. Here, what you are seeing is Dow Jones\nIndustrial Average stock price curves. And you'll see a sudden drop here. Right. So one would be interested knowing\nwhat might have caused the stock market to crash. Well, if you know the background, and\nyou might be able to figure it out if you look at the time stamp, or there are other\ndata that can help us think about. But the question here is can\nwe get some clues about this from the companion news stream? And we have a lot of news data\nthat generated during that period. So if you do that we might\nactually discover the crash. After it happened,\nat the time of the September 11 attack. And that's the time when there\nis a sudden rise of the topic about September 11\nhappened in news articles. Here's another scenario where we want\nto analyze the Presidential Election. And this is the time series that are from\nthe Presidential Prediction Market. For example, I write a trunk of market\nwould have stocks for each candidate. And if you believe one candidate that will\nwin then you tend to buy the stock for that candidate, causing the price\nof that candidate to increase. So, that's a nice way to actual do\nsurvey of people's opinions about these candidates. Now, suppose you see something\ndrop of price for one candidate. And you might also want to know what\nmight have caused the sudden drop. Or in a social science study, you might\nbe interested in knowing what method in this election,\nwhat issues really matter to people. Now again in this case, we can look at the companion news\nstream and ask for the question. Are there any clues in the news stream\nthat might provide insight about this? So for example,\nwe might discover the mention of tax cut has been increasing since that point. So maybe,\nthat's related to the drop of the price. So all these cases are special\ncases of a general problem of joint analysis of text and a time series\ndata to discover causal topics. The input in this case is time series plus text data that are produced in the same\ntime period, the companion text stream. And this is different from\nthe standard topic models, where we have just to text collection. That's why we see time series here,\nit serves as context. Now, the output that we\nwant to generate is the topics whose coverage in the text stream has\nstrong correlations with the time series. For example, whenever the topic is\nmanaging the price tends to go down, etc. Now we call these topics Causal Topics. Of course, they're not,\nstrictly speaking, causal topics. We are never going to be able to\nverify whether they are causal, or there's a true causal relationship here. That's why we put causal\nin quotation marks. But at least they are correlating\ntopics that might potentially explain the cause and humans can certainly further analyze such\ntopics to understand the issue better. And the output would contain topics\njust like in topic modeling. But we hope that these topics are not\njust the regular topics with. These topics certainly don't have to\nexplain the data of the best in text, but rather they have to explain\nthe data in the text. Meaning that they have to reprehend\nthe meaningful topics in text. Cement but also more importantly, they should be correlated with external\nhand series that's given as a context. So to understand how we solve this\nproblem, let's first adjust to solve the problem with reactive\ntopic model, for example PRSA. And we can apply this to text stream and with some extension like a CPRSA or\nContextual PRSA. Then we can discover these\ntopics in the correlation and also discover their coverage over time. So, one simple solution is,\nto choose the topics from this set that have the strongest\ncorrelation with the external time series. But this approach is not\ngoing to be very good. Why?\nBecause awareness pictured to the topics is\nthat they will discover by PRSA or LDA. And that means the choice of\ntopics will be very limited. And we know these models try to maximize\nthe likelihood of the text data. So those topics tend to be the major\ntopics that explain the text data well. aAnd they are not necessarily\ncorrelated with time series. Even if we get the best one, the most\ncorrelated topics might still not be so interesting from causal perspective. So here in this work site here,\na better approach was proposed. And this approach is called\nIterative Causal Topic Modeling. The idea is to do an iterative\nadjustment of topic, discovered by topic models using\ntime series to induce a product. So here's an illustration on\nhow this work, how this works. Take the text stream as input and then apply regular topic modeling\nto generate a number of topics. Let's say four topics. Shown here. And then we're going to use\nexternal time series to assess which topic is more causally related or\ncorrelated with the external time series. So we have something that rank them. And we might think that topic one and\ntopic four are more correlated and topic two and topic three are not. Now we could have stopped here and that would be just like what the simple\napproached that I talked about earlier then we can get to these topics and\ncall them causal topics. But as I also explained that these\ntopics are unlikely very good because they are general topics that\nexplain the whole text connection. They are not necessary. The best topics are correlated\nwith our time series. So what we can do in this approach\nis to first zoom into word level and we can look into each word and\nthe top ranked word listed for each topic. Let's say we take Topic 1\nas the target examined. We know Topic 1 is correlated\nwith the time series. Or is at least the best that we could\nget from this set of topics so far. And we're going to look at the words\nin this topic, the top words. And if the topic is correlated\nwith the Time Series, there must be some words that are highly\ncorrelated with the Time Series. So here, for example,\nwe might discover W1 and W3 are positively correlated with Time Series, but\nW2 and W4 are negatively correlated. So, as a topic, and it's not good to mix\nthese words with different correlations. So we can then for\nthe separate of these words. We are going to get all the red words\nthat indicate positive correlations. W1 and W3.\nAnd we're going to also get another sub topic. If you want. That represents a negatively\ncorrelated words, W2 and W4. Now, these subtopics, or these variations\nof topics, based on the correlation analysis, are topics that are still quite\nrelated to the original topic, Topic 1. But they are already deviating, because of the use of time series\ninformation for bias selection of words. So then in some sense,\nwell we should expect so, some sense more correlated with the time\nseries than the original Topic 1. Because the Topic 1 has mixed words,\nhere we separate them. So each of these two subtopics can be expected to be better\ncoherent in this time series. However, they may not be so\ncoherent as it mention. So the idea here is to go back\nto topic model by using these each as a prior to further\nguide the topic modeling. And that's to say we ask our topic\nmodels now discover topics that are very similar to each\nof these two subtopics. And this will cause a bias toward more\ncorrelate to the topics was a time series. Of course then we can apply topic models\nto get another generation of topics. And that can be further ran to the base of\nthe time series to set after the highly correlated topics. And then we can further analyze\nthe components at work in the topic and then try to analyze.word\nlevel correlation. And then get the even more\ncorrelated subtopics that can be further fed into the process as prior\nto drive the topic of model discovery. So this whole process is just a heuristic\nway of optimizing causality and coherence, and that's our ultimate goal. Right? So here you see the pure topic\nmodels will be very good at maximizing topic coherence,\nthe topics will be all meaningful. If we only use causality test,\nor correlation measure, then we might get a set words that\nare strongly correlate with time series, but they may not\nnecessarily mean anything. It might not be cementric connected. So, that would be at the other extreme,\non the top. Now, the ideal is to get the causal\ntopic that's scored high, both in topic coherence and\nalso causal relation. In this approach, it can be regarded as an alternate\nway to maximize both sine engines. So when we apply the topic models\nwe're maximizing the coherence. But when we decompose the topic\nmodel words into sets of words that are very strong\ncorrelated with the time series. We select the most strongly correlated\nwords with the time series. We are pushing the model\nback to the causal dimension to make it\nbetter in causal scoring. And then, when we apply\nthe selected words as a prior to guide a topic modeling, we again\ngo back to optimize the coherence. Because topic models, we ensure the next\ngeneration of topics to be coherent and we can iterate when they're optimized\nin this way as shown on this picture. So the only I think a component that you\nhaven't seen such a framework is how to measure the causality. Because the rest is just talking more on. So let's have a little bit\nof discussion of that. So here we show that. And let's say we have a topic\nabout government response here. And then we just talking more of we can\nget coverage of the topic over time. So, we have a time series, X sub t. Now, we also have, are give a time series\nthat represents external information. It's a non text time series, Y sub t. It's the stock prices. Now the the question\nhere is does Xt cause Yt? Well in other words, we want to match\nthe causality relation between the two. Or maybe just measure\nthe correlation of the two. There are many measures that\nwe can use in this framework. For example, pairs in correlation\nis a common use measure. And we got to consider time lag here so that we can try to\ncapture causal relation. Using somewhat past data and\nusing the data in the past to try to correlate with the data on points of y that represents the future,\nfor example. And by introducing such lag, we can\nhopefully capture some causal relation by even using correlation measures\nlike person correlation. But a common use, the measure for\ncausality here is Granger Causality Test. And the idea of this test\nis actually quite simple. Basically you're going to have\nall the regressive model to use the history information\nof Y to predict itself. And this is the best we could\nwithout any other information. So we're going to build such a model. And then we're going to add some history\ninformation of X into such model. To see if we can improve\nthe prediction of Y. If we can do that with a statistically\nsignificant difference. Then we just say X has some\ncausal inference on Y, or otherwise it wouldn't have causal\nimprovement of prediction of Y. If, on the other hand,\nthe difference is insignificant and that would mean X does not really\nhave a cause or relation why. So that's the basic idea. Now, we don't have time to explain\nthis in detail so you could read, but you would read at this cited reference\nhere to know more about this measure. It's a very convenient used measure. Has many applications. So next, let's look at some simple\nresults generated by this approach. And here the data is\nthe New York Times and in the time period of June\n2000 through December of 2011. And here the time series we used\nis stock prices of two companies. American Airlines and Apple and the goal is to see if we inject\nthe sum time series contest, whether we can actually get topics\nthat are wise for the time series. Imagine if we don't use any input,\nwe don't use any context. Then the topics from New York\ntimes discovered by PRSA would be just general topics that\npeople talk about in news. All right.\nThose major topics in the news event. But here you see these topics are indeed\nbiased toward each time series. And particularly if you look\nat the underlined words here in the American Airlines result,\nand you see airlines, airport, air, united trade,\nor terrorism, etc. So it clearly has topics that are more\ncorrelated with the external time series. On the right side, you see that some of the topics\nare clearly related to Apple, right. So you can see computer, technology,\nsoftware, internet, com, web, etc. So that just means the time series has effectively served as a context\nto bias the discovery of topics. From another perspective, these results help us on what people\nhave talked about in each case. So not just the people,\nwhat people have talked about, but what are some topics that might be\ncorrelated with their stock prices. And so these topics can serve\nas a starting point for people to further look into issues and\nyou'll find the true causal relations. Here are some other results from analyzing Presidential Election time series. The time series data here is\nfrom Iowa Electronic market. And that's a prediction market. And the data is the same. New York Times from May\n2000 to October 2000. That's for\n2000 presidential campaign election. Now, what you see here are the top three words in significant\ntopics from New York Times. And if you look at these topics, and they\nare indeed quite related to the campaign. Actually the issues\nare very much related to the important issues of\nthis presidential election. Now here I should mention that the text\ndata has been filtered by using only the articles that mention\nthese candidate names. It's a subset of these news articles. Very different from\nthe previous experiment. But the results here clearly show\nthat the approach can uncover some important issues in that\npresidential election. So tax cut, oil energy, abortion and\ngun control are all known to be important issues in\nthat presidential election. And that was supported by some\nliterature in political science. And also I was discussing Wikipedia,\nright. So basically the results show\nthat the approach can effectively discover possibly causal topics\nbased on the time series data. So there are two suggested readings here. One is the paper about this iterative\ntopic modeling with time series feedback. Where you can find more details\nabout how this approach works. And the second one is reading\nabout Granger Casuality text. So in the end, let's summarize\nthe discussion of Text-based Prediction. Now, Text-based prediction\nis generally very useful for big data applications that involve text. Because they can help us inform\nnew knowledge about the world. And the knowledge can go beyond\nwhat's discussed in the text. As a result can also support\noptimizing of our decision making. And this has a wider spread application. Text data is often combined with\nnon-text data for prediction. because, for this purpose,\nthe prediction purpose, we generally would like to combine\nnon-text data and text data together, as much cruel as possible for prediction. And so as a result during\nthe analysis of text and non-text is very necessary and\nit's also very useful. Now when we analyze text data\ntogether with non-text data, we can see they can help each other. So non-text data, provide a context for\nmining text data, and we discussed a number of techniques for\ncontextual text mining. And on the other hand,\na text data can also help interpret patterns discovered from non-text data,\nand this is called a pattern annotation. In general,\nthis is a very active research topic, and there are new papers being published. And there are also many open\nchallenges that have to be solved. [MUSIC]",
 "08_6-8-course-summary.en.txt": "This lecture is a summary\nof this whole course. First, let's revisit the topics\nthat we covered in this course. In the beginning, we talked about\nthe natural language processing and how it can enrich text representation. We then talked about how to mine\nknowledge about the language, natural language used to express the, what's observing the world in text and\ndata. In particular, we talked about\nhow to mine word associations. We then talked about how\nto analyze topics in text. How to discover topics and analyze them. This can be regarded as\nknowledge about observed world, and then we talked about how to mine\nknowledge about the observer and particularly talk about the, how to\nmine opinions and do sentiment analysis. And finally, we will talk about\nthe text-based prediction, which has to do with predicting values of other real\nworld variables based on text data. And in discussing this, we will also\ndiscuss the role of non-text data, which can contribute additional\npredictors for the prediction problem, and also can provide context for\nanalyzing text data, and in particular we talked about how\nto use context to analyze topics. So here are the key high-level\ntake away messages from this cost. I going to go over these major topics and point out what are the key take-away\nmessages that you should remember. First the NLP and text representation. You should realize that NLP\nis always very important for any text replication because it\nenriches text representation. The more NLP the better text\nrepresentation we can have. And this further enables more\naccurate knowledge discovery, to discover deeper knowledge,\nburied in text. However, the current estate of art\nof natural energy processing is, still not robust enough. So, as an result,\nthe robust text mining technologies today, tend to be based on world [INAUDIBLE]. And tend to rely a lot\non statistical analysis, as we've discussed in this course. And you may recall we've mostly\nused word based representations. And we've relied a lot on\nstatistical techniques, statistical learning\ntechniques particularly. In word-association mining and\nanalysis the important points first, we are introduced the two concepts for\ntwo basic and complementary relations of words,\nparadigmatic and syntagmatic relations. These are actually very general\nrelations between elements sequences. If you take it as meaning\nelements that occur in similar context in the sequence and elements\nthat tend to co-occur with each other. And these relations might be also\nmeaningful for other sequences of data. We also talked a lot about\ntest the similarity then we discuss how to discover\nparadynamic similarities compare the context of words discover\nwords that share similar context. At that point level, we talked about representing text\ndata with a vector space model. And we talked about some retrieval\ntechniques such as BM25 for measuring similarity of text and\nfor assigning weights to terms, tf-idf weighting, et cetera. And this part is well-connected\nto text retrieval. There are other techniques that\ncan be relevant here also. The next point is about\nco-occurrence analysis of text, and we introduce some information\ntheory concepts such as entropy, conditional entropy,\nand mutual information. These are not only very useful for measuring the co-occurrences of words,\nthey are also very useful for analyzing other kind of data, and\nthey are useful for, for example, for feature selection in text\ncategorization as well. So this is another important concept,\ngood to know. And then we talked about\nthe topic mining and analysis, and that's where we introduce in\nthe probabilistic topic model. We spent a lot of time to\nexplain the basic topic model, PLSA in detail and this is, those are the\nbasics for understanding LDA which is. Theoretically, a more opinion model, but we did not have enough time to really\ngo in depth in introducing LDA. But in practice,\nPLSA seems as effective as LDA and it's simpler to implement and\nit's also more efficient. In this part of Wilson videos is some\ngeneral concepts that would be useful to know, one is generative model,\nand this is a general method for modeling text data and\nmodeling other kinds of data as well. And we talked about the maximum life\nerase data, the EM algorithm for solving the problem of\ncomputing maximum estimator. So, these are all general techniques\nthat tend to be very useful in other scenarios as well. Then we talked about the text\nclustering and the text categorization. Those are two important building blocks\nin any text mining application systems. In text with clustering we talked\nabout how we can solve the problem by using a slightly different mixture module\nthan the probabilistic topic model. and we then also prefer to\nview the similarity based approaches to test for cuss word. In categorization we also talk\nabout the two kinds of approaches. One is generative classifies\nthat rely on to base word to infer the condition of or\nprobability of a category given text data, in deeper we'll introduce you should\nuse [INAUDIBLE] base in detail. This is the practical use for technique,\nfor a lot of text, capitalization tasks. We also introduce the some\ndiscriminative classifiers, particularly logistical regression,\ncan nearest labor and SBN. They also very important, they are very\npopular, they are very useful for text capitalization as well. In both parts, we'll also discuss\nhow to evaluate the results. Evaluation is quite important because if\nthe matches that you use don't really reflect the volatility of the method then\nit would give you misleading results so its very important to\nget the variation right. And we talked about variation of\ncategorization in detail was a lot of specific measures. Then we talked about the sentiment\nanalysis and the paradigm and that's where we introduced\nsentiment classification problem. And although it's a special\ncase of text recalculation, but we talked about how to extend or\nimprove the text recalculation method by using more sophisticated features that\nwould be needed for sentiment analysis. We did a review of some common use for\ncomplex features for text analysis, and then we also talked about how to\ncapture the order of these categories, in sentiment classification, and\nin particular we introduced ordinal logistical regression then we also talked\nabout Latent Aspect Rating Analysis. This is an unsupervised way of using\na generative model to understand and review data in more detail. In particular, it allows us to\nunderstand the composed ratings of a reviewer on different\naspects of a topic. So given text reviews\nwith overall ratings, the method allows even further\nratings on different aspects. And it also allows us to infer, the viewers laying their\nweights on these aspects or which aspects are more important to\na viewer can be revealed as well. And this enables a lot of\ninteresting applications. Finally, in the discussion of prediction,\nwe mainly talk about the joint mining of text and non text data, as they\nare both very important for prediction. We particularly talked about how text data\ncan help non-text data and vice versa. In the case of using non-text\ndata to help text data analysis, we talked about\nthe contextual text mining. We introduced the contextual PLSA as a\ngeneralizing or generalized model of PLSA to allows us to incorporate the context\nof variables, such as time and location. And this is a general way to allow us\nto reveal a lot of interesting topic of patterns in text data. We also introduced the net PLSA,\nin this case we used social network or network in general of text\ndata to help analyze puppets. And finally we talk about how\ncan be used as context to mine potentially causal\nTopics in text layer. Now, in the other way of using text to help interpret patterns\ndiscovered from LAM text data, we did not really discuss anything in\ndetail but just provide a reference but I should stress that that's after a very\nimportant direction to know about, if you want to build a practical\ntext mining systems, because understanding and\ninterpreting patterns is quite important. So this is a summary of the key\ntake away messages, and I hope these will be very\nuseful to you for building any text mining applications or to you for\nthe starting of these algorithms. And this should provide a good basis for\nyou to read from your research papers, to know more about more of allowance for other organisms or\nto invent new hours in yourself. So to know more about this topic, I would suggest you to look\ninto other areas in more depth. And during this short period\nof time of this course, we could only touch the basic concepts,\nbasic principles, of text mining and we emphasize the coverage\nof practical algorithms. And this is after the cost\nof covering algorithms and in many cases we omit the discussion\nof a lot of algorithms. So to learn more about the subject\nyou should definitely learn more about the natural language process\nbecause this is foundation for all text based applications. The more NLP you can do, the better\nthe additional text that you can get, and then the deeper knowledge\nyou can discover. So this is very important. The second area you should look into\nis the Statistical Machine Learning. And these techniques are now\nthe backbone techniques for not just text analysis applications but\nalso for NLP. A lot of NLP techniques are nowadays\nactually based on supervised machinery. So, they are very important\nbecause they are a key to also understanding some\nadvancing NLP techniques and naturally they will provide more tools for\ndoing text analysis in general. Now, a particularly interesting area, called deep learning has attracted\na lot of attention recently. It has also shown promise\nin many application areas, especially in speech and vision, and\nhas been applied to text data as well. So, for example, recently there has\nwork on using deep learning to do segment analysis to\nachieve better accuracy. So that's one example of [INAUDIBLE]\ntechniques that we weren't able to cover, but that's also very important. And the other area that has emerged\nin status learning is the water and baring technique, where they can\nlearn better recognition of words. And then these better recognitions will\nallow you confuse similarity of words. As you can see, this provides directly a way to discover\nthe paradigmatic relations of words. And results that people have got,\nso far, are very impressive. That's another promising technique\nthat we did not have time to touch, but, of course,\nwhether these new techniques would lead to practical useful techniques\nthat work much better than the current technologies is still an open\nquestion that has to be examined. And no serious evaluation\nhas been done yet. In, for example, examining\nthe practical value of word embedding, other than word similarity and\nbasic evaluation. But nevertheless,\nthese are advanced techniques that surely will make impact\nin text mining in the future. So its very important to\nknow more about these. Statistical learning is also the key to\npredictive modeling which is very crucial for many big data applications and we did\nnot talk about that predictive modeling component but this is mostly about\nthe regression or categorization techniques and this is another reason\nwhy statistical learning is important. We also suggest that you learn more about\ndata mining, and that's simply because general data mining algorithms can always\nbe applied to text data, which can be regarded as as special\ncase of general data. So there are many applications\nof data mining techniques. In particular for example, pattern\ndiscovery would be very useful to generate the interesting features for test analysis\nand the reason that an information network that mining techniques can also be used\nto analyze text information at work. So these are all good to know. In order to develop effective\ntext analysis techniques. And finally, we also recommend you to\nlearn more about the text retrieval, information retrieval, of search engines. This is especially important if you\nare interested in building practical text application systems. And a search ending would\nbe an essential system component in any text-based applications. And that's because texts data\nare created for humans to consume. So humans are at the best position\nto understand text data and it's important to have human in the loop\nin big text data applications, so it can in particular help text\nmining systems in two ways. One is through effectively reduce\nthe data size from a large collection to a small collection with the most\nrelevant text data that only matter for the particular interpretation. So the other is to provide a way to\nannotate it, to explain parents, and this has to do with\nknowledge providence. Once we discover some knowledge,\nwe have to figure out whether or not the discovery is really reliable. So we need to go back to\nthe original text to verify that. And that is why the search\nengine is very important. Moreover, some techniques\nof information retrieval, for example BM25, vector space and\nare also very useful for text data mining. We only mention some of them,\nbut if you know more about text retrieval you'll see that there\nare many techniques that are used for it. Another technique that it's used for\nis indexing technique that enables quick response of search engine to a user's\nquery, and such techniques can be very useful for building efficient\ntext mining systems as well. So, finally, I want to remind\nyou of this big picture for harnessing big text data that I showed\nyou at your beginning of the semester. So in general, to deal with\na big text application system, we need two kinds text,\ntext retrieval and text mining. And text retrieval, as I explained,\nis to help convert big text data into a small amount of most relevant data for\na particular problem, and can also help providing knowledge provenance,\nhelp interpreting patterns later. Text mining has to do with further\nanalyzing the relevant data to discover the actionable knowledge that can be\ndirectly useful for decision making or many other tasks. So this course covers text mining. And there's a companion course\ncalled Text Retrieval and Search Engines that covers text retrieval. If you haven't taken that course,\nit would be useful for you to take it, especially if you are interested\nin building a text caching system. And taking both courses will give you\na complete set of practical skills for building such a system. So in [INAUDIBLE]\nI just would like to thank you for taking this course. I hope you have learned useful knowledge\nand skills in test mining and [INAUDIBLE]. As you see from our discussions\nthere are a lot of opportunities for this kind of techniques and\nthere are also a lot of open channels. So I hope you can use what you have\nlearned to build a lot of use for applications will benefit society and\nto also join the research community to discover new\ntechniques for text mining and benefits. Thank you. [MUSIC]"
}